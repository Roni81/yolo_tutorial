{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718f9f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9a18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '/home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b5979f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/roni/dev_ws/yolo_tut/src/runs/detect/train/weights/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "class_names = model.names\n",
    "\n",
    "colors = {cls_id: [random.randint(0, 255) for _ in range(3)] for cls_id in class_names}\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182fa664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 (no detections), 10.4ms\n",
      "Speed: 3.4ms preprocess, 10.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 2.8ms preprocess, 6.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.0ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.8ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 3.1ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.0ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.2ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.4ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.7ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.7ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 4.1ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.6ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.2ms preprocess, 6.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.2ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.4ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.6ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.8ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 3.2ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 3.0ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.5ms preprocess, 4.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 3.3ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.9ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 3.4ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.0ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.9ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.8ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.4ms preprocess, 6.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.3ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.1ms preprocess, 5.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.2ms preprocess, 6.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.7ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.9ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.0ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 2.0ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 3.0ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.2ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.9ms\n",
      "Speed: 3.1ms preprocess, 9.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.2ms\n",
      "Speed: 3.3ms preprocess, 9.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.6ms preprocess, 7.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.9ms preprocess, 6.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 3.3ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 3.0ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.4ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.4ms preprocess, 5.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.0ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.8ms preprocess, 6.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.8ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.6ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.0ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.6ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.3ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.4ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.2ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.2ms preprocess, 7.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 3.8ms preprocess, 6.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.3ms preprocess, 5.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.6ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.7ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.6ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.4ms preprocess, 5.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.3ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.9ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.4ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.5ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.5ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.0ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.8ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.3ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.1ms\n",
      "Speed: 3.8ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.7ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.7ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 3.0ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 3.0ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.8ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.0ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.9ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 3.5ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 3.1ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.9ms preprocess, 6.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.1ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 3.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.7ms preprocess, 5.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.1ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.4ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.2ms\n",
      "Speed: 4.7ms preprocess, 9.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.8ms\n",
      "Speed: 3.7ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.2ms\n",
      "Speed: 4.3ms preprocess, 9.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.7ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 3.5ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.2ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.5ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.5ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.4ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.3ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.8ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.2ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.4ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.2ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 3.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.4ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.1ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.1ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.9ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.9ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.1ms\n",
      "Speed: 2.7ms preprocess, 10.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 3.1ms preprocess, 6.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 4.5ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.2ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.9ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 3.4ms preprocess, 6.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.0ms preprocess, 6.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.2ms preprocess, 7.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.8ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.3ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.0ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.4ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.2ms preprocess, 7.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.6ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.2ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 3.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.1ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 3.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.9ms preprocess, 5.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.1ms preprocess, 6.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.9ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.3ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.4ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.0ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.6ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.3ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.5ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.4ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.7ms\n",
      "Speed: 2.7ms preprocess, 8.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 4.2ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.7ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 3.3ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.7ms\n",
      "Speed: 2.9ms preprocess, 9.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.7ms preprocess, 6.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.7ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.2ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.6ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.3ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.6ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 3.8ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.6ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.8ms preprocess, 6.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.6ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.2ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.6ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.6ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.2ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 2.4ms preprocess, 6.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.7ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.9ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.6ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.2ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.7ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 2.7ms preprocess, 8.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 4.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 3.0ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.1ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.7ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.7ms preprocess, 8.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.9ms\n",
      "Speed: 2.8ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.9ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.9ms preprocess, 7.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 3.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.7ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 2.1ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.9ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 2.0ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 2.5ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 3.0ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.1ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.8ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.0ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 4.1ms preprocess, 8.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 3.5ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.9ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.8ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.0ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 3.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.5ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.6ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.8ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.9ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.4ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.0ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 4.1ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.4ms preprocess, 6.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.5ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.2ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.5ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.8ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.4ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.3ms\n",
      "Speed: 3.1ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.7ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 2.8ms preprocess, 8.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.1ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.7ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.6ms preprocess, 6.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 2.6ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.4ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 3.0ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.1ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.7ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 3.2ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.5ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 3.0ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.1ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 3.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.7ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 3.7ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 3.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.7ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 3.8ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.4ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.6ms preprocess, 5.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 3.7ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 3.3ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 3.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.4ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.7ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 3.1ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.5ms\n",
      "Speed: 4.5ms preprocess, 9.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.9ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 4.0ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.2ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.1ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.7ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 3.7ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.6ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.2ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.9ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.3ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 3.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.5ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 4.6ms preprocess, 7.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 4.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.3ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.9ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.8ms preprocess, 5.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.9ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.4ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.4ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 3.5ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.5ms preprocess, 6.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.1ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.3ms preprocess, 7.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.0ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.2ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.7ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.3ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.9ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.9ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 3.0ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.5ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.0ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.5ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.7ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.9ms preprocess, 4.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.2ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.9ms preprocess, 6.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.3ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.5ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.2ms\n",
      "Speed: 2.3ms preprocess, 10.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 1.9ms preprocess, 8.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 3.6ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 3.1ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.4ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.6ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 3.5ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.4ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 3.6ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.5ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.7ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.0ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.6ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.5ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.8ms preprocess, 6.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 2.8ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.4ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.3ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.8ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 3.0ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.4ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.6ms\n",
      "Speed: 4.9ms preprocess, 10.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 25.6ms\n",
      "Speed: 5.1ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 12.6ms\n",
      "Speed: 2.8ms preprocess, 12.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.2ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 2.2ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.1ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.9ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.8ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.7ms\n",
      "Speed: 2.0ms preprocess, 10.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.3ms preprocess, 7.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.8ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.0ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.4ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 2.2ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.7ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 3.6ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.8ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 3.0ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.8ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 2.5ms preprocess, 8.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.9ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.2ms preprocess, 4.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.6ms preprocess, 4.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.5ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.0ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 4.1ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 3.1ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 3.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.4ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.8ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 2.1ms preprocess, 7.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 2.7ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.7ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.3ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.9ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 3.6ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 3.8ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.5ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 3.1ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.2ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 3.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.0ms preprocess, 5.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.4ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.8ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 3.0ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 2.2ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.5ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.7ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.3ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.3ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.7ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.4ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 4.3ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.9ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 3.2ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 4.5ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.9ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.7ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 3.2ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 3.4ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.0ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.8ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.3ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 3.0ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.2ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.9ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.4ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.8ms\n",
      "Speed: 1.9ms preprocess, 8.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 3.4ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.4ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.7ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.9ms preprocess, 6.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 2.7ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.5ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 3.0ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 1.8ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.9ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.4ms preprocess, 4.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.5ms\n",
      "Speed: 3.4ms preprocess, 8.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.9ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.9ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.4ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.9ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.4ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.8ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.4ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.9ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.7ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 3.0ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 11.8ms\n",
      "Speed: 1.9ms preprocess, 11.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.2ms\n",
      "Speed: 2.8ms preprocess, 8.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.1ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.6ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.8ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.7ms preprocess, 8.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.9ms preprocess, 6.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.5ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.7ms\n",
      "Speed: 2.6ms preprocess, 9.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.4ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 3.1ms preprocess, 8.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.5ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.1ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.0ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 2.3ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.9ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.7ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.8ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.5ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.9ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.9ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.3ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.7ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.7ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 3.4ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.0ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 3.4ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.9ms preprocess, 6.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 3.0ms preprocess, 8.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.2ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.1ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.9ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.4ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.3ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.3ms preprocess, 6.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 3.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.9ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.4ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.3ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.8ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 2.4ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 3.4ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.1ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.4ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.9ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.1ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.6ms preprocess, 6.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 3.8ms preprocess, 8.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 3.8ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.2ms preprocess, 6.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.4ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 3.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.2ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 3.0ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.4ms preprocess, 6.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 4.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.5ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.2ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.5ms preprocess, 5.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.5ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.4ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.4ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.0ms\n",
      "Speed: 1.8ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.4ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.6ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.0ms preprocess, 6.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.5ms preprocess, 5.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.1ms preprocess, 6.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.9ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.4ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 4.2ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 3.1ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.9ms\n",
      "Speed: 1.8ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.9ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.2ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.4ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.8ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.4ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.5ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 1.9ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 3.4ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.0ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 3.3ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 2.1ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.4ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.8ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.2ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.9ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.3ms preprocess, 6.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.3ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 3.0ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 2.3ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.5ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.5ms preprocess, 6.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.9ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.4ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.6ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.2ms preprocess, 5.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 3.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.9ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.1ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.9ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.2ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.2ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 3.2ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.2ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.0ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.4ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 3.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 11.5ms\n",
      "Speed: 3.1ms preprocess, 11.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 3.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.9ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.5ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.2ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 3.4ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.9ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.5ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 3.1ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.9ms\n",
      "Speed: 1.8ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.7ms preprocess, 4.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.4ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.5ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.6ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 4.6ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.0ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 3.3ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.9ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.8ms preprocess, 8.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.0ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.5ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.6ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.9ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 3.1ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 2.1ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.4ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 3.1ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.2ms\n",
      "Speed: 2.4ms preprocess, 8.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.9ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 4.1ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.4ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 2.2ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.9ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.8ms preprocess, 4.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.5ms\n",
      "Speed: 3.0ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.6ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.7ms preprocess, 6.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.5ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.2ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.8ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 3.2ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.1ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.2ms preprocess, 6.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.5ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.0ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 11.0ms\n",
      "Speed: 3.8ms preprocess, 11.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 2.3ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 3.3ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.6ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 3.1ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 2.3ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 2.0ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.9ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 4.3ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 3.8ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.2ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.9ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 2.0ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.3ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.3ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.4ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.9ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.5ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.5ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.5ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 3.5ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.6ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.7ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.6ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.4ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.9ms\n",
      "Speed: 3.5ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 3.4ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 2.4ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 3.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.7ms\n",
      "Speed: 2.4ms preprocess, 10.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.2ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 3.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.8ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.9ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 3.1ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 1.7ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.6ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 2.5ms preprocess, 4.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 4.2ms preprocess, 8.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.0ms preprocess, 6.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.7ms\n",
      "Speed: 1.4ms preprocess, 8.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 2.4ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 2.8ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.2ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 2.3ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.1ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 3.1ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 3.1ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 4.7ms preprocess, 6.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.1ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 2.0ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.6ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.0ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 3.6ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 3.2ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.1ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.8ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.5ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.9ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.5ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 3.7ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.0ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 3.1ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 3.0ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 2.3ms preprocess, 6.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.2ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.8ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 2.4ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 4.1ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.5ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.8ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.5ms\n",
      "Speed: 3.8ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.8ms preprocess, 5.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.0ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.2ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.7ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.0ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.1ms preprocess, 7.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 3.8ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.9ms preprocess, 6.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 3.0ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 2.7ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.0ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.3ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.7ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.2ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.0ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.0ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.1ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 20.0ms\n",
      "Speed: 2.7ms preprocess, 20.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.1ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.0ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.6ms\n",
      "Speed: 2.7ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.5ms\n",
      "Speed: 1.7ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 3.0ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.7ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 4.0ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.0ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 2.3ms preprocess, 8.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.4ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.6ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.3ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.6ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.6ms preprocess, 6.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.5ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.2ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 3.8ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.6ms\n",
      "Speed: 2.0ms preprocess, 10.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 4.0ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 3.7ms preprocess, 8.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.3ms preprocess, 6.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.2ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.4ms\n",
      "Speed: 4.1ms preprocess, 9.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.8ms\n",
      "Speed: 3.0ms preprocess, 8.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.9ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 2.5ms preprocess, 8.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 3.1ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.3ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.5ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 3.2ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.4ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 3.1ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.9ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 4.1ms preprocess, 7.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.7ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.1ms preprocess, 5.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.4ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.0ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.3ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.4ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.8ms preprocess, 7.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.9ms preprocess, 7.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.1ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.1ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 3.0ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.9ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.5ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 10.7ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.3ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.7ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.1ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.1ms\n",
      "Speed: 3.0ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.7ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.1ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.8ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.5ms preprocess, 5.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 2.0ms preprocess, 4.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 3.1ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 1.8ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.1ms\n",
      "Speed: 5.3ms preprocess, 10.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.7ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.4ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.9ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.4ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.2ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.1ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.1ms preprocess, 6.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.2ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.0ms preprocess, 6.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.3ms preprocess, 7.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.9ms preprocess, 7.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.7ms\n",
      "Speed: 3.5ms preprocess, 9.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.2ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 2.7ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.4ms\n",
      "Speed: 3.0ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 3.0ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.1ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.9ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.6ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.9ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.3ms preprocess, 5.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 12.2ms\n",
      "Speed: 3.0ms preprocess, 12.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.9ms\n",
      "Speed: 3.6ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.3ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 3.1ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.6ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 3.2ms preprocess, 7.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.5ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.9ms preprocess, 6.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.7ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.7ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.4ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.0ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.0ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.7ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.0ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 10.5ms\n",
      "Speed: 2.2ms preprocess, 10.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 3.0ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 5.1ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.8ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.4ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.9ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.1ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.2ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.0ms preprocess, 6.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.4ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.7ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.5ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.5ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.8ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.4ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.1ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.6ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.9ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.9ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 3.0ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 1.8ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 2.6ms preprocess, 6.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.7ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.9ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.2ms preprocess, 6.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.2ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.0ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.9ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.9ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.9ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.9ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.9ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 1.9ms preprocess, 8.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 9.1ms\n",
      "Speed: 5.0ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.4ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.0ms\n",
      "Speed: 2.2ms preprocess, 7.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.0ms preprocess, 5.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.5ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.4ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.4ms preprocess, 7.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.7ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.4ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.5ms\n",
      "Speed: 2.3ms preprocess, 8.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 2.4ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.2ms preprocess, 7.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.5ms\n",
      "Speed: 1.9ms preprocess, 8.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.4ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.6ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.7ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.5ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.6ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.8ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.9ms\n",
      "Speed: 2.3ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 3.3ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.7ms\n",
      "Speed: 2.7ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.3ms preprocess, 4.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.9ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.9ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.9ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.1ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 1.8ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.1ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.9ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.7ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.1ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 2.3ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 3.1ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 3.1ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.1ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.8ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.8ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.2ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.7ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.2ms\n",
      "Speed: 2.7ms preprocess, 8.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.8ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.6ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.9ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 2.0ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.7ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.8ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.4ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.4ms preprocess, 4.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 2.7ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.6ms\n",
      "Speed: 1.5ms preprocess, 8.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 3.1ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.1ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.6ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.3ms\n",
      "Speed: 2.5ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.3ms preprocess, 5.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.5ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.9ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.2ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.2ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.9ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 2.2ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.4ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.7ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 2.8ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 5.7ms preprocess, 8.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 2.8ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 3.0ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.9ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.7ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 3.3ms preprocess, 6.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.1ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.0ms preprocess, 6.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 3.2ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 2.2ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 3.4ms preprocess, 6.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.3ms preprocess, 6.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.7ms\n",
      "Speed: 2.8ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.6ms preprocess, 5.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.6ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.3ms preprocess, 6.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.2ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 2.3ms preprocess, 6.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 2.3ms preprocess, 6.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.8ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 1.2ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.9ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.4ms preprocess, 5.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 3.0ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 3.3ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 3.0ms preprocess, 7.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.8ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.4ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.8ms preprocess, 5.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.7ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.7ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.6ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 3.0ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.7ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.4ms\n",
      "Speed: 1.5ms preprocess, 8.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.2ms\n",
      "Speed: 3.8ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 2.2ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.5ms\n",
      "Speed: 2.6ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 2.9ms preprocess, 5.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 3.0ms preprocess, 5.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.7ms\n",
      "Speed: 1.2ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 1.2ms preprocess, 5.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.9ms\n",
      "Speed: 1.2ms preprocess, 5.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.8ms\n",
      "Speed: 2.0ms preprocess, 5.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 2.5ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.5ms\n",
      "Speed: 2.4ms preprocess, 6.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.6ms preprocess, 5.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.3ms preprocess, 6.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.0ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.5ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.1ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.5ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 2.4ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 7.8ms\n",
      "Speed: 2.6ms preprocess, 7.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.4ms preprocess, 5.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 2.0ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.8ms preprocess, 5.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 2.2ms preprocess, 5.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.9ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.8ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.8ms preprocess, 6.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.0ms\n",
      "Speed: 2.0ms preprocess, 5.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.7ms\n",
      "Speed: 1.3ms preprocess, 5.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.1ms\n",
      "Speed: 2.3ms preprocess, 6.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.5ms\n",
      "Speed: 2.9ms preprocess, 5.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.6ms\n",
      "Speed: 1.4ms preprocess, 5.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 2.3ms preprocess, 5.1ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 1.5ms preprocess, 6.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 6.0ms\n",
      "Speed: 3.1ms preprocess, 6.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.2ms\n",
      "Speed: 1.5ms preprocess, 5.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.12.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m      2\u001b[39m     ret, frame = cap.read()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     frame = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m720\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\\\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.12.0) /io/opencv/modules/imgproc/src/resize.cpp:4208: error: (-215:Assertion failed) !ssize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    \n",
    "    if not ret:\n",
    "        break\\\n",
    "\n",
    "    results = model(frame)\n",
    "\n",
    "    boxes = results[0].boxes\n",
    "\n",
    "    for box in boxes:\n",
    "        \n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        conf = float(box.conf[0])\n",
    "        cls_id = int(box.cls[0])\n",
    "\n",
    "        label = f\"{class_names[cls_id]} {conf:.2f}\"\n",
    "\n",
    "        color = colors(cls_id)\n",
    "\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "    cv2.imshow(\"Inference\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4153b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found: ['eye', 'man', 'woman']\n",
      "Processing train set...\n",
      "Processing val set...\n",
      "Dataset preparation complete. YAML saved to /home/roni/dev_ws/yolo_tut/datasets/lesserafim/dataset.yaml\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100%  6.2MB 55.8MB/s 0.1s.1s<0.0s\n",
      "Ultralytics 8.3.235  Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 2060, 5739MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/roni/dev_ws/yolo_tut/datasets/lesserafim/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/roni/dev_ws/yolo_tut/src/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100%  5.4MB 44.3MB/s 0.1s.1s<0.0s\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 3560.31070.2 MB/s, size: 101.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/roni/dev_ws/yolo_tut/datasets/lesserafim/labels/train... 11 images, 0 backgrounds, 0 corrupt: 100%  11/11 2.0Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/roni/dev_ws/yolo_tut/datasets/lesserafim/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 744.4532.9 MB/s, size: 175.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/roni/dev_ws/yolo_tut/datasets/lesserafim/labels/val... 3 images, 0 backgrounds, 0 corrupt: 100%  3/3 382.0it/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/roni/dev_ws/yolo_tut/datasets/lesserafim/labels/val.cache\n",
      "Plotting labels to /home/roni/dev_ws/yolo_tut/src/runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/roni/dev_ws/yolo_tut/src/runs/detect/train\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      1.38G      1.388      3.312      1.484         37        640: 100%  1/1 1.5s/it 1.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 4.9it/s 0.2s\n",
      "                   all          3         14       0.02        0.9      0.262      0.207\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      1.38G      1.484      3.384      1.535         37        640: 100%  1/1 6.6it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 15.2it/s 0.1s\n",
      "                   all          3         14       0.02        0.9      0.266      0.205\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      1.39G      1.298      3.301      1.473         37        640: 100%  1/1 8.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 17.2it/s 0.1s\n",
      "                   all          3         14       0.02        0.9        0.3       0.23\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      1.39G      1.253      3.287      1.383         36        640: 100%  1/1 8.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 32.6it/s 0.0s\n",
      "                   all          3         14       0.02        0.9      0.261      0.207\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      1.42G       1.25      3.358      1.462         36        640: 100%  1/1 5.6it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 29.6it/s 0.0s\n",
      "                   all          3         14     0.0205        0.9      0.273      0.215\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      1.44G      1.337      3.254      1.426         37        640: 100%  1/1 8.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 30.3it/s 0.0s\n",
      "                   all          3         14     0.0207        0.9      0.297      0.236\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      1.45G      1.476      3.334       1.46         38        640: 100%  1/1 7.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 28.8it/s 0.0s\n",
      "                   all          3         14     0.0187        0.8      0.294      0.237\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      1.47G      1.261      3.281      1.363         37        640: 100%  1/1 7.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 33.8it/s 0.0s\n",
      "                   all          3         14     0.0187        0.8      0.324      0.264\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      1.49G      1.202      3.254      1.347         37        640: 100%  1/1 7.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 28.1it/s 0.0s\n",
      "                   all          3         14     0.0208        0.9      0.332      0.267\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      1.51G      1.216        3.2      1.282         36        640: 100%  1/1 7.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 30.1it/s 0.0s\n",
      "                   all          3         14     0.0209        0.9      0.352      0.276\n",
      "\n",
      "10 epochs completed in 0.002 hours.\n",
      "Optimizer stripped from /home/roni/dev_ws/yolo_tut/src/runs/detect/train/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /home/roni/dev_ws/yolo_tut/src/runs/detect/train/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /home/roni/dev_ws/yolo_tut/src/runs/detect/train/weights/best.pt...\n",
      "Ultralytics 8.3.235  Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 2060, 5739MiB)\n",
      "Model summary (fused): 72 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%  1/1 39.5it/s 0.0s\n",
      "                   all          3         14     0.0208        0.9      0.343      0.281\n",
      "                   man          2          5      0.014        0.8      0.164      0.142\n",
      "                 woman          3          9     0.0275          1      0.522      0.419\n",
      "Speed: 0.2ms preprocess, 2.8ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/roni/dev_ws/yolo_tut/src/runs/detect/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. Configuration\n",
    "SOURCE_DIR = '/home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original'\n",
    "DATASET_NAME = 'lesserafim'\n",
    "BASE_DIR = f'/home/roni/dev_ws/yolo_tut/datasets/{DATASET_NAME}'\n",
    "IMAGES_DIR = os.path.join(BASE_DIR, 'images')\n",
    "LABELS_DIR = os.path.join(BASE_DIR, 'labels')\n",
    "\n",
    "# Create directory structure\n",
    "for split in ['train', 'val']:\n",
    "    os.makedirs(os.path.join(IMAGES_DIR, split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(LABELS_DIR, split), exist_ok=True)\n",
    "\n",
    "# 2. Convert LabelMe JSON to YOLO txt\n",
    "classes = set()\n",
    "data_pairs = []\n",
    "\n",
    "# First pass to find all pairs and classes\n",
    "for filename in os.listdir(SOURCE_DIR):\n",
    "    if filename.endswith('.json'):\n",
    "        json_path = os.path.join(SOURCE_DIR, filename)\n",
    "        image_name = filename.replace('.json', '.jpg')\n",
    "        image_path = os.path.join(SOURCE_DIR, image_name)\n",
    "        \n",
    "        if os.path.exists(image_path):\n",
    "            data_pairs.append((json_path, image_path))\n",
    "            # Extract classes\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                for shape in data['shapes']:\n",
    "                    classes.add(shape['label'])\n",
    "\n",
    "class_list = sorted(list(classes))\n",
    "class_to_id = {cls: i for i, cls in enumerate(class_list)}\n",
    "print(f\"Classes found: {class_list}\")\n",
    "\n",
    "# Shuffle and split\n",
    "random.shuffle(data_pairs)\n",
    "split_idx = int(len(data_pairs) * 0.8)\n",
    "train_pairs = data_pairs[:split_idx]\n",
    "val_pairs = data_pairs[split_idx:]\n",
    "\n",
    "def process_pair(pair, split):\n",
    "    json_file, img_file = pair\n",
    "    filename = os.path.basename(img_file)\n",
    "    basename = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Copy image\n",
    "    dst_img_path = os.path.join(IMAGES_DIR, split, filename)\n",
    "    shutil.copy(img_file, dst_img_path)\n",
    "    \n",
    "    # Process label\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    img_h = data['imageHeight']\n",
    "    img_w = data['imageWidth']\n",
    "    \n",
    "    yolo_lines = []\n",
    "    for shape in data['shapes']:\n",
    "        label = shape['label']\n",
    "        cls_id = class_to_id[label]\n",
    "        points = shape['points']\n",
    "        \n",
    "        # Calculate bounding box from points\n",
    "        x_coords = [p[0] for p in points]\n",
    "        y_coords = [p[1] for p in points]\n",
    "        \n",
    "        x_min = min(x_coords)\n",
    "        x_max = max(x_coords)\n",
    "        y_min = min(y_coords)\n",
    "        y_max = max(y_coords)\n",
    "        \n",
    "        # Convert to center x, center y, width, height (normalized)\n",
    "        dw = 1.0 / img_w\n",
    "        dh = 1.0 / img_h\n",
    "        \n",
    "        w = x_max - x_min\n",
    "        h = y_max - y_min\n",
    "        x_center = x_min + w / 2.0\n",
    "        y_center = y_min + h / 2.0\n",
    "        \n",
    "        w *= dw\n",
    "        h *= dh\n",
    "        x_center *= dw\n",
    "        y_center *= dh\n",
    "        \n",
    "        yolo_lines.append(f\"{cls_id} {x_center} {y_center} {w} {h}\")\n",
    "        \n",
    "    # Write label file\n",
    "    dst_label_path = os.path.join(LABELS_DIR, split, f\"{basename}.txt\")\n",
    "    with open(dst_label_path, 'w') as f:\n",
    "        f.write('\\n'.join(yolo_lines))\n",
    "\n",
    "print(\"Processing train set...\")\n",
    "for pair in train_pairs:\n",
    "    process_pair(pair, 'train')\n",
    "\n",
    "print(\"Processing val set...\")\n",
    "for pair in val_pairs:\n",
    "    process_pair(pair, 'val')\n",
    "\n",
    "# 3. Create dataset.yaml\n",
    "yaml_content = {\n",
    "    'path': BASE_DIR,\n",
    "    'train': 'images/train',\n",
    "    'val': 'images/val',\n",
    "    'names': {i: name for i, name in enumerate(class_list)}\n",
    "}\n",
    "\n",
    "yaml_path = os.path.join(BASE_DIR, 'dataset.yaml')\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(yaml_content, f)\n",
    "\n",
    "print(f\"Dataset preparation complete. YAML saved to {yaml_path}\")\n",
    "\n",
    "# 4. Train YOLO\n",
    "model = YOLO(\"yolov8n.pt\") \n",
    "results = model.train(data=yaml_path, epochs=10, imgsz=640)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ea67b",
   "metadata": {},
   "source": [
    "### segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b6f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ec8bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-seg.pt to 'yolov8n-seg.pt': 100%  6.7MB 27.0MB/s 0.3s.2s<0.1s\n",
      "\n",
      "\u001b[KDownloading https://ultralytics.com/images/bus.jpg to 'bus.jpg': 100%  134.2KB 7.5MB/s 0.0s\n",
      "image 1/1 /home/roni/dev_ws/yolo_tut/src/bus.jpg: 640x480 4 persons, 1 bus, 1 skateboard, 45.4ms\n",
      "Speed: 1.7ms preprocess, 45.4ms inference, 6.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1m/home/roni/dev_ws/yolo_tut/src/runs/segment/predict\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " obb: None\n",
       " orig_img: array([[[119, 146, 172],\n",
       "         [121, 148, 174],\n",
       "         [122, 152, 177],\n",
       "         ...,\n",
       "         [161, 171, 188],\n",
       "         [160, 170, 187],\n",
       "         [160, 170, 187]],\n",
       " \n",
       "        [[120, 147, 173],\n",
       "         [122, 149, 175],\n",
       "         [123, 153, 178],\n",
       "         ...,\n",
       "         [161, 171, 188],\n",
       "         [160, 170, 187],\n",
       "         [160, 170, 187]],\n",
       " \n",
       "        [[123, 150, 176],\n",
       "         [124, 151, 177],\n",
       "         [125, 155, 180],\n",
       "         ...,\n",
       "         [161, 171, 188],\n",
       "         [160, 170, 187],\n",
       "         [160, 170, 187]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 182, 186],\n",
       "         [179, 178, 182],\n",
       "         [180, 179, 183],\n",
       "         ...,\n",
       "         [121, 111, 117],\n",
       "         [113, 103, 109],\n",
       "         [115, 105, 111]],\n",
       " \n",
       "        [[165, 164, 168],\n",
       "         [173, 172, 176],\n",
       "         [187, 186, 190],\n",
       "         ...,\n",
       "         [102,  92,  98],\n",
       "         [101,  91,  97],\n",
       "         [103,  93,  99]],\n",
       " \n",
       "        [[123, 122, 126],\n",
       "         [145, 144, 148],\n",
       "         [176, 175, 179],\n",
       "         ...,\n",
       "         [ 95,  85,  91],\n",
       "         [ 96,  86,  92],\n",
       "         [ 98,  88,  94]]], shape=(1080, 810, 3), dtype=uint8)\n",
       " orig_shape: (1080, 810)\n",
       " path: '/home/roni/dev_ws/yolo_tut/src/bus.jpg'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/segment/predict'\n",
       " speed: {'preprocess': 1.7047340006683953, 'inference': 45.38494000007631, 'postprocess': 6.674645002931356}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('yolov8n-seg.pt')\n",
    "results = model(source='https://ultralytics.com/images/bus.jpg', save=True)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fba06ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[KDownloading https://ultralytics.com/images/zidane.jpg to 'zidane.jpg': 100%  49.2KB 8.9MB/s 0.0ss\n",
      "image 1/1 /home/roni/dev_ws/yolo_tut/src/zidane.jpg: 384x640 2 persons, 1 tie, 15.9ms\n",
      "Speed: 2.3ms preprocess, 15.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1m/home/roni/dev_ws/yolo_tut/src/runs/segment/predict2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: ultralytics.engine.results.Masks object\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " obb: None\n",
       " orig_img: array([[[44, 51, 76],\n",
       "         [43, 50, 75],\n",
       "         [41, 48, 73],\n",
       "         ...,\n",
       "         [20, 18, 54],\n",
       "         [18, 16, 52],\n",
       "         [17, 15, 51]],\n",
       " \n",
       "        [[44, 51, 76],\n",
       "         [43, 50, 75],\n",
       "         [41, 48, 73],\n",
       "         ...,\n",
       "         [20, 18, 54],\n",
       "         [18, 16, 52],\n",
       "         [18, 16, 52]],\n",
       " \n",
       "        [[44, 51, 76],\n",
       "         [43, 50, 75],\n",
       "         [41, 48, 73],\n",
       "         ...,\n",
       "         [21, 18, 57],\n",
       "         [19, 16, 55],\n",
       "         [18, 15, 54]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[53, 44, 40],\n",
       "         [52, 43, 39],\n",
       "         [51, 42, 38],\n",
       "         ...,\n",
       "         [50, 50, 38],\n",
       "         [51, 51, 39],\n",
       "         [52, 52, 40]],\n",
       " \n",
       "        [[53, 44, 40],\n",
       "         [52, 43, 39],\n",
       "         [51, 42, 38],\n",
       "         ...,\n",
       "         [50, 50, 38],\n",
       "         [51, 51, 39],\n",
       "         [52, 52, 40]],\n",
       " \n",
       "        [[53, 44, 40],\n",
       "         [52, 43, 39],\n",
       "         [51, 42, 38],\n",
       "         ...,\n",
       "         [49, 49, 37],\n",
       "         [51, 51, 39],\n",
       "         [52, 52, 40]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/src/zidane.jpg'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/segment/predict2'\n",
       " speed: {'preprocess': 2.260601999296341, 'inference': 15.888039997662418, 'postprocess': 1.414117999956943}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO('yolov8n-seg.pt')\n",
    "results = model(source='https://ultralytics.com/images/zidane.jpg', save=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e4fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.235  Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 2060, 5739MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco8-seg.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/roni/dev_ws/yolo_tut/src/runs/segment/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1150432  ultralytics.nn.modules.head.Segment          [80, 32, 64, [64, 128, 256]]  \n",
      "YOLOv8n-seg summary: 151 layers, 3,409,968 parameters, 3,409,952 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 417/417 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 2155.2956.1 MB/s, size: 50.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/roni/dev_ws/yolo_tut/datasets/coco8-seg/labels/train.cache... 4 images, 0 backgrounds, 0 corrupt: 100%  4/4 10.7Kit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 958.2117.1 MB/s, size: 54.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/roni/dev_ws/yolo_tut/datasets/coco8-seg/labels/val.cache... 4 images, 0 backgrounds, 0 corrupt: 100%  4/4 3.0Kit/s 0.0ss\n",
      "Plotting labels to /home/roni/dev_ws/yolo_tut/src/runs/segment/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000119, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/roni/dev_ws/yolo_tut/src/runs/segment/train2\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100     0.871G      1.006      1.958      2.958      1.272         19        640: 100%  1/1 3.5it/s 0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 9.3it/s 0.1s\n",
      "                   all          4         17      0.677      0.633      0.745      0.485      0.644      0.533      0.605       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100     0.984G      1.287      3.682      2.745      1.562         23        640: 100%  1/1 8.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 12.9it/s 0.1s\n",
      "                   all          4         17       0.68      0.633      0.746      0.485       0.66        0.6       0.62       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100     0.984G      1.258      2.652      3.276      1.422         26        640: 100%  1/1 9.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 6.2it/s 0.2s\n",
      "                   all          4         17      0.681      0.633      0.745      0.498      0.661        0.6      0.618      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100     0.984G     0.9575      2.568      2.775      1.189         34        640: 100%  1/1 8.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.8it/s 0.1s\n",
      "                   all          4         17      0.687      0.633      0.746       0.51      0.667        0.6      0.618       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/100     0.984G     0.9742      2.473      3.278      1.319         24        640: 100%  1/1 9.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.7it/s 0.1s\n",
      "                   all          4         17      0.699      0.633      0.744      0.505      0.687      0.617      0.633       0.44\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/100     0.984G      1.047      3.272      2.316      1.249         26        640: 100%  1/1 9.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.3it/s 0.1s\n",
      "                   all          4         17      0.709      0.628      0.777      0.518      0.688        0.6      0.615      0.437\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/100     0.996G      1.238      2.737      3.311      1.524         21        640: 100%  1/1 9.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 15.2it/s 0.1s\n",
      "                   all          4         17      0.727      0.631       0.79       0.52      0.699      0.598      0.623      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/100     0.998G      1.083      2.928      3.947      1.411         17        640: 100%  1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.9it/s 0.1s\n",
      "                   all          4         17      0.718      0.617      0.814      0.538      0.625      0.533       0.63      0.451\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/100     0.998G      1.113      2.473      2.602      1.334         27        640: 100%  1/1 9.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.4it/s 0.1s\n",
      "                   all          4         17      0.712        0.6      0.894      0.609      0.642        0.6      0.823      0.503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/100      1.02G      0.801      2.181       1.97      1.131         23        640: 100%  1/1 7.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 12.5it/s 0.1s\n",
      "                   all          4         17      0.684      0.617      0.908      0.601      0.638       0.59      0.823      0.503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/100      1.02G      1.282      2.512      2.449      1.455         20        640: 100%  1/1 7.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 13.2it/s 0.1s\n",
      "                   all          4         17      0.736        0.6      0.904      0.592      0.717      0.583      0.846      0.505\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/100      1.03G      1.274      3.559      3.218       1.49         45        640: 100%  1/1 8.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.4it/s 0.1s\n",
      "                   all          4         17      0.746        0.6      0.906      0.575      0.727      0.583      0.849      0.517\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/100      1.03G      1.055      3.251      2.425      1.339         35        640: 100%  1/1 9.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.2it/s 0.1s\n",
      "                   all          4         17      0.775        0.6      0.809      0.537      0.755      0.583      0.765      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/100      1.03G     0.9784      2.396      2.128       1.26         32        640: 100%  1/1 1.9it/s 0.5s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.5it/s 0.1s\n",
      "                   all          4         17      0.781        0.6        0.8      0.534       0.76      0.583      0.763      0.474\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/100      1.03G      1.121       2.79      2.471       1.35         32        640: 100%  1/1 6.3it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 13.5it/s 0.1s\n",
      "                   all          4         17      0.786      0.597      0.799      0.506      0.768      0.583      0.765      0.466\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/100      1.03G     0.8108      2.144      2.173      1.119         39        640: 100%  1/1 8.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.8it/s 0.1s\n",
      "                   all          4         17      0.785      0.596      0.793       0.49      0.769      0.583      0.765      0.466\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/100      1.03G     0.9393      2.787      2.824      1.403         29        640: 100%  1/1 10.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.3it/s 0.1s\n",
      "                   all          4         17      0.802      0.593      0.793      0.507       0.79      0.583      0.765      0.466\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/100      1.03G       1.05       3.35      2.611      1.265         21        640: 100%  1/1 13.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.5it/s 0.1s\n",
      "                   all          4         17      0.802      0.593      0.793      0.507       0.79      0.583      0.765      0.466\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/100      1.03G     0.7287      2.176      1.784      1.149         21        640: 100%  1/1 8.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.7it/s 0.1s\n",
      "                   all          4         17      0.816      0.592      0.793      0.492      0.806      0.583      0.764      0.466\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/100      1.03G      1.203      2.253      2.306      1.438         16        640: 100%  1/1 13.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.4it/s 0.1s\n",
      "                   all          4         17      0.816      0.592      0.793      0.492      0.806      0.583      0.764      0.466\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/100      1.03G      1.016      2.738      2.321        1.3         29        640: 100%  1/1 9.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.8it/s 0.1s\n",
      "                   all          4         17      0.782      0.598      0.774      0.495      0.742      0.567      0.744      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     22/100      1.03G     0.9446      2.209      2.106      1.272         22        640: 100%  1/1 12.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 15.6it/s 0.1s\n",
      "                   all          4         17      0.782      0.598      0.774      0.495      0.742      0.567      0.744      0.458\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     23/100      1.03G     0.5524      1.719       1.57      1.062         19        640: 100%  1/1 9.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.9it/s 0.1s\n",
      "                   all          4         17      0.729        0.6      0.746      0.503      0.714      0.583      0.736      0.459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     24/100      1.03G      0.561      2.153      1.319      1.038         23        640: 100%  1/1 11.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.9it/s 0.1s\n",
      "                   all          4         17      0.729        0.6      0.746      0.503      0.714      0.583      0.736      0.459\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     25/100      1.04G     0.6263      1.774      1.432      1.041         19        640: 100%  1/1 10.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.5it/s 0.1s\n",
      "                   all          4         17        0.7        0.6      0.773      0.492      0.689      0.583      0.765      0.477\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     26/100      1.04G     0.9981      2.823      2.249      1.279         47        640: 100%  1/1 11.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.7it/s 0.1s\n",
      "                   all          4         17        0.7        0.6      0.773      0.492      0.689      0.583      0.765      0.477\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     27/100      1.06G     0.9227      2.653      1.646      1.198         39        640: 100%  1/1 8.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.9it/s 0.1s\n",
      "                   all          4         17      0.687        0.6      0.776      0.484      0.687        0.6      0.767      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     28/100      1.07G      1.173      2.148      1.545      1.309         25        640: 100%  1/1 12.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.0it/s 0.1s\n",
      "                   all          4         17      0.687        0.6      0.776      0.484      0.687        0.6      0.767      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     29/100      1.07G      1.085       2.34       1.63      1.387         29        640: 100%  1/1 10.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.1it/s 0.1s\n",
      "                   all          4         17      0.703        0.6      0.774      0.493      0.697        0.6      0.767      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     30/100      1.07G     0.6546      1.933      1.747      1.231         19        640: 100%  1/1 13.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.0it/s 0.1s\n",
      "                   all          4         17      0.703        0.6      0.774      0.493      0.697        0.6      0.767      0.463\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     31/100      1.07G     0.8797      1.829      1.567      1.222         23        640: 100%  1/1 9.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.4it/s 0.1s\n",
      "                   all          4         17      0.737      0.433      0.743      0.472      0.678      0.517      0.611      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     32/100      1.07G      0.737      2.248      1.707      1.112         17        640: 100%  1/1 11.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.2it/s 0.1s\n",
      "                   all          4         17      0.737      0.433      0.743      0.472      0.678      0.517      0.611      0.446\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     33/100      1.07G     0.7536      1.806      1.253      1.142         25        640: 100%  1/1 9.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.6it/s 0.1s\n",
      "                   all          4         17      0.833      0.417      0.742      0.479      0.801      0.332      0.611      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     34/100      1.07G      1.047       2.18      1.238      1.229         32        640: 100%  1/1 12.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.7it/s 0.1s\n",
      "                   all          4         17      0.833      0.417      0.742      0.479      0.801      0.332      0.611      0.448\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     35/100      1.07G     0.7385      1.761      1.016       1.08         36        640: 100%  1/1 9.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.7it/s 0.1s\n",
      "                   all          4         17      0.944      0.366      0.649      0.411      0.798      0.319      0.518      0.394\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     36/100      1.07G     0.5722      2.443      1.566      1.084         31        640: 100%  1/1 12.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.3it/s 0.1s\n",
      "                   all          4         17      0.944      0.366      0.649      0.411      0.798      0.319      0.518      0.394\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     37/100      1.07G     0.7766       2.02      1.278      1.164         19        640: 100%  1/1 10.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.3it/s 0.1s\n",
      "                   all          4         17      0.941      0.366      0.532      0.322      0.797        0.3      0.405      0.292\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     38/100      1.07G     0.8705      2.271      1.261      1.112         49        640: 100%  1/1 10.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.1it/s 0.1s\n",
      "                   all          4         17      0.941      0.366      0.532      0.322      0.797        0.3      0.405      0.292\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     39/100      1.07G      0.656      1.753     0.8984     0.9701         32        640: 100%  1/1 8.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.2it/s 0.1s\n",
      "                   all          4         17      0.909      0.385      0.533      0.318      0.838      0.306      0.406      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     40/100      1.07G     0.9552      1.771      1.202      1.174         15        640: 100%  1/1 13.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.9it/s 0.1s\n",
      "                   all          4         17      0.909      0.385      0.533      0.318      0.838      0.306      0.406      0.286\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     41/100      1.07G     0.6589      1.793     0.9079      1.012         39        640: 100%  1/1 9.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.0it/s 0.1s\n",
      "                   all          4         17      0.939      0.366      0.534      0.316      0.793        0.3      0.405      0.287\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     42/100      1.07G     0.7613       2.08      1.055      1.069         47        640: 100%  1/1 12.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.6it/s 0.1s\n",
      "                   all          4         17      0.939      0.366      0.534      0.316      0.793        0.3      0.405      0.287\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     43/100      1.07G     0.5096      1.599      1.043      1.014         18        640: 100%  1/1 10.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.2it/s 0.1s\n",
      "                   all          4         17      0.936      0.366       0.52      0.292       0.79        0.3      0.391      0.268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     44/100      1.07G     0.9868      2.829      2.118      1.249         48        640: 100%  1/1 11.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.0it/s 0.1s\n",
      "                   all          4         17      0.936      0.366       0.52      0.292       0.79        0.3      0.391      0.268\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     45/100      1.07G     0.6991       2.08      1.296      1.165         32        640: 100%  1/1 9.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.7it/s 0.1s\n",
      "                   all          4         17      0.904      0.384      0.488      0.283      0.829      0.314       0.36      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     46/100      1.07G     0.8854      1.807      1.189      1.128         19        640: 100%  1/1 12.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.9it/s 0.1s\n",
      "                   all          4         17      0.904      0.384      0.488      0.283      0.829      0.314       0.36      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     47/100      1.07G     0.9084      1.973      1.695      1.191         18        640: 100%  1/1 10.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.7it/s 0.1s\n",
      "                   all          4         17      0.932      0.366      0.471      0.293      0.932      0.366      0.463      0.257\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     48/100      1.07G     0.6743      1.519     0.8812      1.055         22        640: 100%  1/1 10.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.0it/s 0.1s\n",
      "                   all          4         17      0.932      0.366      0.471      0.293      0.932      0.366      0.463      0.257\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     49/100      1.07G     0.7491      1.935     0.9033      1.088         24        640: 100%  1/1 10.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.7it/s 0.1s\n",
      "                   all          4         17      0.934      0.366       0.43      0.279      0.934      0.366      0.421      0.232\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     50/100      1.07G     0.6815       1.85     0.9087      1.073         35        640: 100%  1/1 10.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 15.8it/s 0.1s\n",
      "                   all          4         17      0.934      0.366       0.43      0.279      0.934      0.366      0.421      0.232\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     51/100      1.07G     0.6616        1.8     0.8292       1.04         36        640: 100%  1/1 10.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.2it/s 0.1s\n",
      "                   all          4         17      0.934      0.366       0.43      0.276      0.934      0.366      0.421      0.233\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     52/100      1.07G     0.7848      1.962      1.272      1.113         44        640: 100%  1/1 12.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.1it/s 0.1s\n",
      "                   all          4         17      0.934      0.366       0.43      0.276      0.934      0.366      0.421      0.233\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     53/100      1.07G     0.6174      1.958     0.7457      1.014         22        640: 100%  1/1 11.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.5it/s 0.1s\n",
      "                   all          4         17      0.934      0.366       0.43      0.276      0.934      0.366      0.421      0.233\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     54/100      1.07G     0.6799      1.545     0.7402      1.111         19        640: 100%  1/1 9.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.1it/s 0.1s\n",
      "                   all          4         17      0.934      0.366      0.428      0.271      0.938      0.365      0.411      0.228\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     55/100      1.07G     0.6851      1.978     0.9935      1.075         40        640: 100%  1/1 12.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.9it/s 0.1s\n",
      "                   all          4         17      0.934      0.366      0.428      0.271      0.938      0.365      0.411      0.228\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     56/100      1.07G     0.5666       1.28     0.7608      1.042         15        640: 100%  1/1 11.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.4it/s 0.1s\n",
      "                   all          4         17      0.934      0.366      0.428      0.271      0.938      0.365      0.411      0.228\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     57/100      1.07G     0.6508      1.615     0.6762      1.008         29        640: 100%  1/1 6.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 13.9it/s 0.1s\n",
      "                   all          4         17      0.897      0.381      0.428      0.273      0.933      0.366      0.412      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     58/100      1.07G     0.5455      1.469     0.6336      1.059         20        640: 100%  1/1 10.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.4it/s 0.1s\n",
      "                   all          4         17      0.897      0.381      0.428      0.273      0.933      0.366      0.412      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     59/100      1.07G     0.8301      1.865     0.9751       1.07         28        640: 100%  1/1 11.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.0it/s 0.1s\n",
      "                   all          4         17      0.897      0.381      0.428      0.273      0.933      0.366      0.412      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     60/100      1.07G     0.7408      1.805      1.126      1.109         28        640: 100%  1/1 10.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 18.6it/s 0.1s\n",
      "                   all          4         17      0.897      0.392      0.426      0.273       0.93      0.366      0.411      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     61/100      1.07G     0.6351       1.42     0.8208     0.9609         30        640: 100%  1/1 11.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.9it/s 0.1s\n",
      "                   all          4         17      0.897      0.392      0.426      0.273       0.93      0.366      0.411      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     62/100      1.07G     0.4774      1.453     0.7104     0.8902         25        640: 100%  1/1 14.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.5it/s 0.0s\n",
      "                   all          4         17      0.897      0.392      0.426      0.273       0.93      0.366      0.411      0.245\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     63/100      1.07G     0.6817      1.764      1.011      1.083         26        640: 100%  1/1 11.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.0it/s 0.0s\n",
      "                   all          4         17      0.921      0.373      0.424      0.268      0.927      0.366      0.411      0.244\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     64/100      1.07G     0.5719       1.49     0.7136      0.981         21        640: 100%  1/1 14.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.9it/s 0.0s\n",
      "                   all          4         17      0.921      0.373      0.424      0.268      0.927      0.366      0.411      0.244\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     65/100      1.07G     0.7071      1.493     0.6886      1.039         21        640: 100%  1/1 11.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.6it/s 0.1s\n",
      "                   all          4         17      0.921      0.373      0.424      0.268      0.927      0.366      0.411      0.244\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     66/100      1.07G     0.6967      1.956      1.155      1.073         42        640: 100%  1/1 11.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.0it/s 0.0s\n",
      "                   all          4         17      0.922       0.37      0.425      0.268      0.927      0.366      0.418      0.243\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     67/100      1.07G     0.8083      1.988      1.166      1.189         36        640: 100%  1/1 14.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.3it/s 0.0s\n",
      "                   all          4         17      0.922       0.37      0.425      0.268      0.927      0.366      0.418      0.243\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     68/100      1.07G     0.8932      1.734     0.8768      1.218         41        640: 100%  1/1 14.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.0it/s 0.0s\n",
      "                   all          4         17      0.922       0.37      0.425      0.268      0.927      0.366      0.418      0.243\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     69/100      1.07G     0.7129      1.608     0.8379      1.101         30        640: 100%  1/1 11.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.1it/s 0.0s\n",
      "                   all          4         17      0.898      0.386      0.426      0.267      0.921      0.367      0.411      0.228\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     70/100      1.07G     0.6102      1.748     0.6002     0.9786         19        640: 100%  1/1 13.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 20.3it/s 0.0s\n",
      "                   all          4         17      0.898      0.386      0.426      0.267      0.921      0.367      0.411      0.228\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     71/100      1.07G     0.5473      1.832     0.7076     0.9878         37        640: 100%  1/1 13.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.9it/s 0.0s\n",
      "                   all          4         17      0.898      0.386      0.426      0.267      0.921      0.367      0.411      0.228\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     72/100      1.08G     0.6203      1.216     0.8816      1.051         20        640: 100%  1/1 12.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.8it/s 0.0s\n",
      "                   all          4         17      0.919       0.38      0.419      0.265      0.902      0.367      0.412      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     73/100      1.08G     0.6247      1.586     0.6941     0.9742         31        640: 100%  1/1 14.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.6it/s 0.0s\n",
      "                   all          4         17      0.919       0.38      0.419      0.265      0.902      0.367      0.412      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     74/100      1.08G     0.4807      1.616      0.566          1         23        640: 100%  1/1 14.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.0it/s 0.0s\n",
      "                   all          4         17      0.919       0.38      0.419      0.265      0.902      0.367      0.412      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     75/100      1.08G      0.642      1.597     0.6776      1.064         24        640: 100%  1/1 8.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.8it/s 0.1s\n",
      "                   all          4         17      0.911      0.383       0.42      0.267      0.865      0.369      0.412      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     76/100      1.08G     0.5595      1.501     0.6117     0.9561         31        640: 100%  1/1 12.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.9it/s 0.1s\n",
      "                   all          4         17      0.911      0.383       0.42      0.267      0.865      0.369      0.412      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     77/100      1.08G     0.6173      1.474      0.769     0.9796         25        640: 100%  1/1 12.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 20.2it/s 0.0s\n",
      "                   all          4         17      0.911      0.383       0.42      0.267      0.865      0.369      0.412      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     78/100      1.08G     0.6624       1.76      1.018      1.056         37        640: 100%  1/1 11.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.1it/s 0.0s\n",
      "                   all          4         17       0.89      0.383      0.424      0.267      0.857      0.378      0.415      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     79/100      1.08G     0.6371      1.577     0.9371      1.039         37        640: 100%  1/1 14.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.0it/s 0.0s\n",
      "                   all          4         17       0.89      0.383      0.424      0.267      0.857      0.378      0.415      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     80/100      1.08G     0.5638      1.601     0.5672     0.9621         32        640: 100%  1/1 14.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.1it/s 0.0s\n",
      "                   all          4         17       0.89      0.383      0.424      0.267      0.857      0.378      0.415      0.225\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     81/100      1.08G     0.4562      1.143     0.7121     0.9623         16        640: 100%  1/1 12.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.0it/s 0.0s\n",
      "                   all          4         17      0.895      0.383      0.421      0.283      0.861      0.378       0.41      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     82/100      1.08G     0.6073      1.455     0.6528      1.096         20        640: 100%  1/1 14.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.2it/s 0.0s\n",
      "                   all          4         17      0.895      0.383      0.421      0.283      0.861      0.378       0.41      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     83/100      1.08G     0.6506      1.573     0.7817      1.206         17        640: 100%  1/1 12.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.3it/s 0.0s\n",
      "                   all          4         17      0.895      0.383      0.421      0.283      0.861      0.378       0.41      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     84/100      1.08G      0.558      1.398     0.7929     0.9416         34        640: 100%  1/1 11.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.4it/s 0.0s\n",
      "                   all          4         17      0.848      0.383      0.421      0.266      0.833      0.383      0.404      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     85/100      1.08G     0.5522      1.756     0.8312      1.006         52        640: 100%  1/1 11.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.0it/s 0.0s\n",
      "                   all          4         17      0.848      0.383      0.421      0.266      0.833      0.383      0.404      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     86/100      1.08G     0.6117      1.621     0.8999      1.006         49        640: 100%  1/1 11.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.8it/s 0.0s\n",
      "                   all          4         17      0.848      0.383      0.421      0.266      0.833      0.383      0.404      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     87/100      1.08G     0.8204      2.339      1.596      1.233         24        640: 100%  1/1 13.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.9it/s 0.0s\n",
      "                   all          4         17      0.848      0.383      0.421      0.266      0.833      0.383      0.404      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     88/100      1.08G     0.5117      1.362     0.6164     0.9858         37        640: 100%  1/1 11.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.4it/s 0.0s\n",
      "                   all          4         17      0.815        0.4      0.419      0.269      0.815        0.4      0.404      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     89/100      1.08G     0.6315      1.239     0.6432      1.065         18        640: 100%  1/1 14.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.4it/s 0.0s\n",
      "                   all          4         17      0.815        0.4      0.419      0.269      0.815        0.4      0.404      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     90/100      1.08G     0.6054      1.481     0.6722      1.038         21        640: 100%  1/1 14.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.2it/s 0.0s\n",
      "                   all          4         17      0.815        0.4      0.419      0.269      0.815        0.4      0.404      0.223\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     91/100      1.08G     0.6449      1.847     0.6797      1.057         13        640: 100%  1/1 6.2it/s 0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 16.1it/s 0.1s\n",
      "                   all          4         17      0.815        0.4      0.419      0.269      0.815        0.4      0.404      0.223\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     92/100      1.08G      0.614      1.773     0.7111      1.087         13        640: 100%  1/1 10.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 12.6it/s 0.1s\n",
      "                   all          4         17      0.907        0.4      0.428      0.286      0.858        0.4      0.409      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     93/100      1.08G     0.5874      1.346     0.6684     0.9491         13        640: 100%  1/1 11.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 17.5it/s 0.1s\n",
      "                   all          4         17      0.907        0.4      0.428      0.286      0.858        0.4      0.409      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     94/100      1.08G     0.6091      1.695      0.812     0.9782         13        640: 100%  1/1 9.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.0it/s 0.1s\n",
      "                   all          4         17      0.907        0.4      0.428      0.286      0.858        0.4      0.409      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     95/100      1.08G     0.5626      1.259     0.5862     0.9341         13        640: 100%  1/1 11.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.2it/s 0.1s\n",
      "                   all          4         17      0.907        0.4      0.428      0.286      0.858        0.4      0.409      0.226\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     96/100      1.08G     0.5502      1.315     0.6186     0.9459         13        640: 100%  1/1 10.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 20.2it/s 0.0s\n",
      "                   all          4         17      0.909      0.397      0.426      0.285       0.86      0.383      0.407      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     97/100      1.08G     0.5239      1.436     0.6352     0.9764         13        640: 100%  1/1 12.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.3it/s 0.0s\n",
      "                   all          4         17      0.909      0.397      0.426      0.285       0.86      0.383      0.407      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     98/100      1.08G     0.5563       1.37     0.6342     0.9706         13        640: 100%  1/1 12.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 19.9it/s 0.1s\n",
      "                   all          4         17      0.909      0.397      0.426      0.285       0.86      0.383      0.407      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     99/100      1.08G     0.6061      1.611     0.7212      1.062         13        640: 100%  1/1 14.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 15.0it/s 0.1s\n",
      "                   all          4         17      0.909      0.397      0.426      0.285       0.86      0.383      0.407      0.224\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K    100/100      1.08G     0.5443      1.393     0.5486     0.9606         13        640: 100%  1/1 10.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 21.8it/s 0.0s\n",
      "                   all          4         17       0.91      0.394       0.51      0.327      0.862      0.381      0.489      0.267\n",
      "\n",
      "100 epochs completed in 0.014 hours.\n",
      "Optimizer stripped from /home/roni/dev_ws/yolo_tut/src/runs/segment/train2/weights/last.pt, 7.1MB\n",
      "Optimizer stripped from /home/roni/dev_ws/yolo_tut/src/runs/segment/train2/weights/best.pt, 7.1MB\n",
      "\n",
      "Validating /home/roni/dev_ws/yolo_tut/src/runs/segment/train2/weights/best.pt...\n",
      "Ultralytics 8.3.235  Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 2060, 5739MiB)\n",
      "YOLOv8n-seg summary (fused): 85 layers, 3,404,320 parameters, 0 gradients, 12.0 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%  1/1 22.6it/s 0.0s\n",
      "                   all          4         17      0.713        0.6      0.895      0.627      0.644        0.6      0.824      0.504\n",
      "                person          3         10      0.572        0.6      0.647      0.297      0.343        0.6      0.433      0.232\n",
      "                   dog          1          1      0.832          1      0.995      0.895      0.694          1      0.995      0.895\n",
      "                 horse          1          2      0.309          1      0.995      0.672      0.287          1      0.995      0.249\n",
      "              elephant          1          2          1          0      0.745      0.436          1          0      0.531      0.319\n",
      "              umbrella          1          1      0.564          1      0.995      0.895      0.539          1      0.995      0.895\n",
      "          potted plant          1          1          1          0      0.995      0.564          1          0      0.995      0.431\n",
      "Speed: 0.3ms preprocess, 3.0ms inference, 0.0ms loss, 1.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/roni/dev_ws/yolo_tut/src/runs/segment/train2\u001b[0m\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n-seg.pt')\n",
    "\n",
    "results = model.train(data = 'coco8-seg.yaml', epochs = 100, imgsz = 640, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21e0fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f190dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt': 100%  6.5MB 19.6MB/s 0.3s.3s<0.0s0.6s\n",
      "Ultralytics 8.3.235  Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 2060, 5739MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=coco8-pose.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n-pose.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/roni/dev_ws/yolo_tut/src/runs/pose/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=pose, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "WARNING  Dataset 'coco8-pose.yaml' images not found, missing path '/home/roni/dev_ws/yolo_tut/datasets/coco8-pose/images/val'\n",
      "\u001b[KDownloading https://ultralytics.com/assets/coco8-pose.zip to '/home/roni/dev_ws/yolo_tut/datasets/coco8-pose.zip': 100%  333.7KB 14.8MB/s 0.0s\n",
      "\u001b[KUnzipping /home/roni/dev_ws/yolo_tut/datasets/coco8-pose.zip to /home/roni/dev_ws/yolo_tut/datasets/coco8-pose...: 100%  27/27 5.9Kfiles/s 0.0s\n",
      "Dataset download success  (0.4s), saved to \u001b[1m/home/roni/dev_ws/yolo_tut/datasets\u001b[0m\n",
      "\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1035934  ultralytics.nn.modules.head.Pose             [1, [17, 3], [64, 128, 256]]  \n",
      "YOLOv8n-pose summary: 144 layers, 3,295,470 parameters, 3,295,454 gradients, 9.3 GFLOPs\n",
      "\n",
      "Transferred 397/397 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1930.2293.7 MB/s, size: 39.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/roni/dev_ws/yolo_tut/datasets/coco8-pose/labels/train... 4 images, 0 backgrounds, 0 corrupt: 100%  4/4 892.8it/s 0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/roni/dev_ws/yolo_tut/datasets/coco8-pose/labels/train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1503.5808.9 MB/s, size: 39.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/roni/dev_ws/yolo_tut/datasets/coco8-pose/labels/val... 4 images, 0 backgrounds, 0 corrupt: 100%  4/4 582.6it/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/roni/dev_ws/yolo_tut/datasets/coco8-pose/labels/val.cache\n",
      "Plotting labels to /home/roni/dev_ws/yolo_tut/src/runs/pose/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 63 weight(decay=0.0), 73 weight(decay=0.0005), 72 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/roni/dev_ws/yolo_tut/src/runs/pose/train\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      1/100     0.559G      1.092      3.973     0.5297     0.9495      1.497         11        640: 100%  1/1 1.2it/s 0.8s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 4.6it/s 0.2s\n",
      "                   all          4         14      0.926      0.898      0.907      0.668      0.846        0.5      0.535      0.352\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      2/100     0.562G      1.703      2.638     0.4552      1.159      1.757          9        640: 100%  1/1 10.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 13.1it/s 0.1s\n",
      "                   all          4         14      0.927      0.929      0.907      0.668      0.844        0.5      0.534      0.348\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      3/100     0.564G      1.493      1.533     0.2746      1.146      1.583          8        640: 100%  1/1 10.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 12.9it/s 0.1s\n",
      "                   all          4         14      0.911      0.929      0.907      0.668      0.842        0.5      0.534      0.352\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      4/100     0.566G      1.348      4.426     0.5211      1.122      1.312         19        640: 100%  1/1 10.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 22.4it/s 0.0s\n",
      "                   all          4         14      0.904      0.929      0.907      0.668      0.841        0.5      0.534      0.352\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      5/100     0.566G      1.241      4.502     0.3894      1.233      1.303         17        640: 100%  1/1 11.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 21.2it/s 0.0s\n",
      "                   all          4         14        0.9      0.929      0.907      0.668      0.839        0.5      0.535      0.352\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      6/100     0.566G     0.9028      3.505     0.2779     0.7293      1.155         14        640: 100%  1/1 6.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.7it/s 0.0s\n",
      "                   all          4         14      0.786      0.857      0.851      0.668      0.972      0.571      0.634      0.345\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      7/100       0.6G      1.099      3.453     0.2801     0.9939      1.246         13        640: 100%  1/1 10.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.7it/s 0.0s\n",
      "                   all          4         14      0.784      0.857      0.851      0.646       0.97      0.571      0.635      0.329\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      8/100       0.6G      1.138      3.262     0.4343      1.296       1.82          6        640: 100%  1/1 10.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 21.8it/s 0.0s\n",
      "                   all          4         14      0.763      0.786      0.853      0.633          1      0.617      0.652      0.301\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      9/100     0.619G      1.266      4.831     0.5306       1.73      1.323         17        640: 100%  1/1 11.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 27.4it/s 0.0s\n",
      "                   all          4         14      0.765      0.786      0.891       0.61      0.763      0.571      0.558      0.274\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     10/100     0.619G      1.278      3.288     0.3054     0.8375      1.418         17        640: 100%  1/1 12.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 27.2it/s 0.0s\n",
      "                   all          4         14      0.844      0.775      0.877      0.577      0.803      0.583      0.631      0.263\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     11/100     0.637G      1.634      3.543     0.4078      1.138      1.744         12        640: 100%  1/1 11.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.1it/s 0.0s\n",
      "                   all          4         14      0.803      0.714       0.75      0.539      0.727      0.429      0.417      0.204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     12/100     0.637G     0.8937      3.104     0.3328     0.8197      1.111         14        640: 100%  1/1 11.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 27.0it/s 0.0s\n",
      "                   all          4         14      0.725      0.714      0.712      0.483      0.733      0.429      0.418        0.2\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     13/100     0.656G     0.8368      1.733     0.3497     0.5672      1.018         13        640: 100%  1/1 11.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.2it/s 0.0s\n",
      "                   all          4         14      0.774      0.733      0.803      0.537      0.871        0.5      0.469      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     14/100     0.656G     0.8244       1.22     0.3189     0.6126      1.094         12        640: 100%  1/1 11.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 27.2it/s 0.0s\n",
      "                   all          4         14      0.813      0.621      0.757      0.482      0.673        0.5      0.464      0.232\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     15/100     0.656G      1.079      2.492     0.3897      0.737      1.245          8        640: 100%  1/1 11.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.4it/s 0.0s\n",
      "                   all          4         14      0.657      0.714      0.701      0.449      0.749      0.426       0.45      0.232\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     16/100     0.656G     0.7976       1.65     0.2801     0.5921       1.04         13        640: 100%  1/1 12.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.9it/s 0.0s\n",
      "                   all          4         14      0.512      0.643       0.67      0.429      0.533        0.5      0.417      0.181\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     17/100     0.656G      1.175      2.295     0.3175     0.7092      1.174         13        640: 100%  1/1 11.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 22.7it/s 0.0s\n",
      "                   all          4         14       0.54      0.672      0.645      0.392      0.603      0.429      0.418      0.145\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     18/100     0.656G     0.8207      1.966     0.2481     0.7804       1.21          7        640: 100%  1/1 15.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.5it/s 0.0s\n",
      "                   all          4         14       0.54      0.672      0.645      0.392      0.603      0.429      0.418      0.145\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     19/100     0.668G      1.326      4.397     0.3191      1.121      1.393         16        640: 100%  1/1 8.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.2it/s 0.0s\n",
      "                   all          4         14      0.731        0.5      0.596      0.316      0.517      0.357      0.323     0.0986\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     20/100     0.674G      1.087       2.15     0.3846     0.9433      1.083         16        640: 100%  1/1 12.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 22.8it/s 0.0s\n",
      "                   all          4         14      0.731        0.5      0.596      0.316      0.517      0.357      0.323     0.0986\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     21/100     0.686G      1.017       2.49     0.2833     0.7073      1.299          9        640: 100%  1/1 11.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.5it/s 0.0s\n",
      "                   all          4         14      0.728        0.5      0.608       0.32      0.518      0.357      0.325      0.104\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     22/100     0.686G      1.013      2.919     0.3819     0.9373      1.271          8        640: 100%  1/1 14.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.0it/s 0.0s\n",
      "                   all          4         14      0.728        0.5      0.608       0.32      0.518      0.357      0.325      0.104\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     23/100     0.701G      1.034      1.556     0.3642     0.6286      1.115         15        640: 100%  1/1 10.4it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.5it/s 0.0s\n",
      "                   all          4         14      0.808      0.429      0.567      0.274      0.534      0.286      0.236      0.091\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     24/100     0.701G     0.8629      2.792     0.3598     0.8895      1.103         18        640: 100%  1/1 14.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.3it/s 0.0s\n",
      "                   all          4         14      0.808      0.429      0.567      0.274      0.534      0.286      0.236      0.091\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     25/100     0.719G      1.096       2.26     0.3621     0.9016      1.136         17        640: 100%  1/1 10.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.0it/s 0.0s\n",
      "                   all          4         14      0.654        0.5      0.556      0.213      0.499      0.357      0.274     0.0952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     26/100     0.719G       1.11      2.863     0.2853          1      1.269         17        640: 100%  1/1 14.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.6it/s 0.0s\n",
      "                   all          4         14      0.654        0.5      0.556      0.213      0.499      0.357      0.274     0.0952\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     27/100     0.719G      1.127      2.539     0.3332     0.7084      1.186         16        640: 100%  1/1 10.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.6it/s 0.0s\n",
      "                   all          4         14      0.618        0.5       0.57      0.208      0.417      0.357      0.249     0.0863\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     28/100     0.719G      1.355      2.422     0.3583     0.8412      1.325         15        640: 100%  1/1 14.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.9it/s 0.0s\n",
      "                   all          4         14      0.618        0.5       0.57      0.208      0.417      0.357      0.249     0.0863\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     29/100     0.719G      1.067      3.619     0.3718     0.8607      1.131         17        640: 100%  1/1 11.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.1it/s 0.0s\n",
      "                   all          4         14      0.504      0.571      0.589      0.211      0.482      0.286      0.275     0.0795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     30/100     0.719G      1.426      3.059     0.3288      1.166      1.455         11        640: 100%  1/1 14.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.5it/s 0.0s\n",
      "                   all          4         14      0.504      0.571      0.589      0.211      0.482      0.286      0.275     0.0795\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     31/100     0.719G        1.1       2.17     0.3005     0.8964      1.175         11        640: 100%  1/1 11.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.1it/s 0.0s\n",
      "                   all          4         14       0.56      0.636      0.585      0.231      0.581        0.5      0.374     0.0738\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     32/100     0.719G       1.06      2.183     0.4257     0.9503      1.481          6        640: 100%  1/1 14.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.5it/s 0.0s\n",
      "                   all          4         14       0.56      0.636      0.585      0.231      0.581        0.5      0.374     0.0738\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     33/100     0.719G      1.118       3.17      0.286       0.92      1.188         15        640: 100%  1/1 11.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 27.1it/s 0.0s\n",
      "                   all          4         14      0.953      0.571      0.665      0.264      0.627      0.286      0.289     0.0984\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     34/100     0.719G     0.8209      3.457     0.2801     0.6906     0.9682         10        640: 100%  1/1 15.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 27.4it/s 0.0s\n",
      "                   all          4         14      0.953      0.571      0.665      0.264      0.627      0.286      0.289     0.0984\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     35/100     0.719G      1.296      2.585     0.3006     0.8902      1.135         22        640: 100%  1/1 11.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.6it/s 0.0s\n",
      "                   all          4         14      0.644      0.571      0.609       0.27      0.383      0.355      0.263     0.0885\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     36/100     0.719G      1.106      2.549     0.3435     0.8081      1.141         22        640: 100%  1/1 15.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.7it/s 0.0s\n",
      "                   all          4         14      0.644      0.571      0.609       0.27      0.383      0.355      0.263     0.0885\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     37/100     0.721G     0.8182       1.89     0.2534     0.6795      1.092          8        640: 100%  1/1 10.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.0it/s 0.0s\n",
      "                   all          4         14      0.836      0.429      0.545      0.261      0.287      0.357      0.255     0.0773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     38/100     0.721G      0.858       3.46      0.291     0.8206      1.073         17        640: 100%  1/1 14.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.3it/s 0.0s\n",
      "                   all          4         14      0.836      0.429      0.545      0.261      0.287      0.357      0.255     0.0773\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     39/100     0.721G      1.053      3.102     0.2724     0.6602      1.153         20        640: 100%  1/1 11.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.7it/s 0.0s\n",
      "                   all          4         14      0.431      0.714      0.526      0.228      0.835      0.143      0.192     0.0581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     40/100     0.721G      1.013       2.82     0.3437     0.7641      1.058         12        640: 100%  1/1 14.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.8it/s 0.0s\n",
      "                   all          4         14      0.431      0.714      0.526      0.228      0.835      0.143      0.192     0.0581\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     41/100     0.721G     0.8546      2.542      0.288     0.7178       1.04         21        640: 100%  1/1 11.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.5it/s 0.0s\n",
      "                   all          4         14      0.452      0.571      0.456      0.212      0.841      0.143      0.153     0.0403\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     42/100     0.721G      1.102      2.298     0.3279     0.8618      1.172         18        640: 100%  1/1 14.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.0it/s 0.0s\n",
      "                   all          4         14      0.452      0.571      0.456      0.212      0.841      0.143      0.153     0.0403\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     43/100     0.721G       1.05       2.62     0.2852     0.7615      1.088         16        640: 100%  1/1 11.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.2it/s 0.0s\n",
      "                   all          4         14      0.336      0.429      0.352      0.182      0.755      0.143      0.163     0.0527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     44/100     0.721G     0.9007      3.652     0.2529     0.7558      1.195         11        640: 100%  1/1 14.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.0it/s 0.0s\n",
      "                   all          4         14      0.336      0.429      0.352      0.182      0.755      0.143      0.163     0.0527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     45/100     0.721G     0.7196      2.394     0.2759     0.5565     0.9524         23        640: 100%  1/1 11.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.4it/s 0.0s\n",
      "                   all          4         14      0.356      0.429      0.389        0.2      0.666      0.143      0.155     0.0566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     46/100     0.721G      1.139      4.596     0.3582      1.249      1.143         15        640: 100%  1/1 15.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.6it/s 0.0s\n",
      "                   all          4         14      0.356      0.429      0.389        0.2      0.666      0.143      0.155     0.0566\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     47/100     0.721G     0.8509      1.888      0.369     0.6282     0.9188         12        640: 100%  1/1 11.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.9it/s 0.0s\n",
      "                   all          4         14      0.355      0.429      0.374      0.207       0.33     0.0714      0.053     0.0314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     48/100     0.721G          1      1.971      0.338     0.7203      1.135         13        640: 100%  1/1 15.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.4it/s 0.0s\n",
      "                   all          4         14      0.355      0.429      0.374      0.207       0.33     0.0714      0.053     0.0314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     49/100     0.721G      0.926      2.322     0.3125       0.59      1.092         15        640: 100%  1/1 11.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.7it/s 0.0s\n",
      "                   all          4         14      0.283      0.357      0.325      0.171      0.318     0.0714     0.0509     0.0274\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     50/100     0.721G      0.771      2.405     0.3205     0.6118      1.051         11        640: 100%  1/1 15.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.7it/s 0.0s\n",
      "                   all          4         14      0.283      0.357      0.325      0.171      0.318     0.0714     0.0509     0.0274\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     51/100     0.721G     0.9178      2.124     0.3199     0.5697     0.9964         18        640: 100%  1/1 11.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.4it/s 0.0s\n",
      "                   all          4         14        0.7      0.214      0.305      0.155      0.282     0.0714     0.0375     0.0167\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     52/100     0.721G     0.7372      2.794     0.2885     0.7144        1.2         11        640: 100%  1/1 13.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.0it/s 0.0s\n",
      "                   all          4         14        0.7      0.214      0.305      0.155      0.282     0.0714     0.0375     0.0167\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     53/100     0.721G     0.9973      3.231     0.3592     0.6608     0.9723         18        640: 100%  1/1 15.1it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 28.3it/s 0.0s\n",
      "                   all          4         14        0.7      0.214      0.305      0.155      0.282     0.0714     0.0375     0.0167\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     54/100     0.721G     0.9028      2.227     0.4745     0.6596      1.089         10        640: 100%  1/1 11.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.7it/s 0.0s\n",
      "                   all          4         14      0.408      0.214      0.267      0.141      0.295     0.0714     0.0308     0.0123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     55/100     0.721G     0.9402      3.449     0.2417     0.6963      1.092         13        640: 100%  1/1 9.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.1it/s 0.0s\n",
      "                   all          4         14      0.408      0.214      0.267      0.141      0.295     0.0714     0.0308     0.0123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     56/100     0.721G     0.8755      2.433     0.2853      0.857       1.08         16        640: 100%  1/1 13.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.6it/s 0.0s\n",
      "                   all          4         14      0.408      0.214      0.267      0.141      0.295     0.0714     0.0308     0.0123\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     57/100     0.721G      1.082      4.119     0.3292     0.7647      1.135         16        640: 100%  1/1 10.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.5it/s 0.0s\n",
      "                   all          4         14      0.135      0.714      0.304      0.183      0.435     0.0714      0.059      0.013\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     58/100     0.721G     0.8534      2.757     0.3323     0.6388     0.9698         15        640: 100%  1/1 13.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.9it/s 0.0s\n",
      "                   all          4         14      0.135      0.714      0.304      0.183      0.435     0.0714      0.059      0.013\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     59/100     0.721G     0.9846      2.469     0.3317     0.7016      1.024         14        640: 100%  1/1 14.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.0it/s 0.0s\n",
      "                   all          4         14      0.135      0.714      0.304      0.183      0.435     0.0714      0.059      0.013\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     60/100     0.721G     0.9594      2.257       0.33     0.7858      1.168         11        640: 100%  1/1 11.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.0it/s 0.0s\n",
      "                   all          4         14      0.243      0.214      0.238      0.153      0.249     0.0714     0.0439    0.00642\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     61/100     0.721G     0.8561      2.103     0.3366     0.6932     0.9969         18        640: 100%  1/1 15.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.4it/s 0.0s\n",
      "                   all          4         14      0.243      0.214      0.238      0.153      0.249     0.0714     0.0439    0.00642\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     62/100     0.721G     0.7387      1.996     0.2562     0.6687     0.9536         17        640: 100%  1/1 14.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.7it/s 0.0s\n",
      "                   all          4         14      0.243      0.214      0.238      0.153      0.249     0.0714     0.0439    0.00642\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     63/100     0.721G     0.5544      2.416     0.3172     0.7289      1.043          6        640: 100%  1/1 11.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.1it/s 0.0s\n",
      "                   all          4         14       0.22      0.214      0.215      0.139      0.111      0.143     0.0285    0.00959\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     64/100     0.721G     0.7538      2.784     0.3928     0.6291      1.069         11        640: 100%  1/1 14.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.7it/s 0.0s\n",
      "                   all          4         14       0.22      0.214      0.215      0.139      0.111      0.143     0.0285    0.00959\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     65/100     0.721G     0.7271      2.192     0.2492     0.5965      1.045          9        640: 100%  1/1 15.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.4it/s 0.0s\n",
      "                   all          4         14       0.22      0.214      0.215      0.139      0.111      0.143     0.0285    0.00959\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     66/100     0.721G     0.9573      2.429     0.3438      0.624      1.216         13        640: 100%  1/1 11.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 27.0it/s 0.0s\n",
      "                   all          4         14      0.269      0.143      0.191      0.128     0.0233      0.143     0.0231    0.00781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     67/100     0.721G      1.251      3.191     0.3043     0.8946      1.343         11        640: 100%  1/1 14.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.4it/s 0.0s\n",
      "                   all          4         14      0.269      0.143      0.191      0.128     0.0233      0.143     0.0231    0.00781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     68/100     0.721G     0.9024      1.887     0.3241     0.6351      1.146         12        640: 100%  1/1 11.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.4it/s 0.0s\n",
      "                   all          4         14      0.269      0.143      0.191      0.128     0.0233      0.143     0.0231    0.00781\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     69/100     0.721G      1.064      2.429     0.3148      0.707      1.189         19        640: 100%  1/1 9.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.3it/s 0.0s\n",
      "                   all          4         14      0.224      0.214      0.206      0.118      0.103     0.0714     0.0346     0.0119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     70/100     0.721G     0.8311      2.177     0.2688     0.7792     0.9982         13        640: 100%  1/1 13.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.5it/s 0.0s\n",
      "                   all          4         14      0.224      0.214      0.206      0.118      0.103     0.0714     0.0346     0.0119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     71/100     0.721G     0.8762      2.759     0.3001     0.6843      1.055         15        640: 100%  1/1 15.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.3it/s 0.0s\n",
      "                   all          4         14      0.224      0.214      0.206      0.118      0.103     0.0714     0.0346     0.0119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     72/100     0.721G     0.9169      2.484     0.3113     0.7462      1.085         20        640: 100%  1/1 10.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.1it/s 0.0s\n",
      "                   all          4         14       0.23      0.214      0.212      0.123      0.214     0.0714     0.0402     0.0192\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     73/100     0.721G     0.7823      2.401     0.3083     0.5973      1.053         11        640: 100%  1/1 14.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.4it/s 0.0s\n",
      "                   all          4         14       0.23      0.214      0.212      0.123      0.214     0.0714     0.0402     0.0192\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     74/100     0.721G      0.779      2.094     0.2922     0.6215      1.024         18        640: 100%  1/1 12.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 21.7it/s 0.0s\n",
      "                   all          4         14       0.23      0.214      0.212      0.123      0.214     0.0714     0.0402     0.0192\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     75/100     0.721G      1.086        3.7     0.3292     0.8392      1.163         16        640: 100%  1/1 10.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 21.2it/s 0.0s\n",
      "                   all          4         14      0.895      0.143      0.255      0.148      0.427     0.0714     0.0618     0.0316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     76/100     0.721G     0.5891      1.368     0.2672     0.5382     0.9074         14        640: 100%  1/1 12.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.2it/s 0.0s\n",
      "                   all          4         14      0.895      0.143      0.255      0.148      0.427     0.0714     0.0618     0.0316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     77/100     0.721G        0.9      2.111     0.2382     0.7634      1.086         18        640: 100%  1/1 13.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.8it/s 0.0s\n",
      "                   all          4         14      0.895      0.143      0.255      0.148      0.427     0.0714     0.0618     0.0316\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     78/100     0.721G      1.224      3.264     0.3527     0.8757      1.269         13        640: 100%  1/1 9.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 22.4it/s 0.0s\n",
      "                   all          4         14      0.301      0.286      0.289      0.156      0.434     0.0714     0.0669      0.034\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     79/100     0.721G     0.7274      2.658     0.2829     0.6082      1.026         20        640: 100%  1/1 11.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.5it/s 0.0s\n",
      "                   all          4         14      0.301      0.286      0.289      0.156      0.434     0.0714     0.0669      0.034\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     80/100     0.721G     0.7759      1.412     0.3084     0.5829     0.8864         13        640: 100%  1/1 11.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.2it/s 0.0s\n",
      "                   all          4         14      0.301      0.286      0.289      0.156      0.434     0.0714     0.0669      0.034\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     81/100     0.721G     0.8682      2.261     0.3401     0.7314      1.133         13        640: 100%  1/1 10.9it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.9it/s 0.0s\n",
      "                   all          4         14      0.336      0.289      0.342      0.174      0.433     0.0714     0.0709     0.0356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     82/100     0.721G     0.5308      1.392     0.1927     0.4768      0.916          7        640: 100%  1/1 14.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.3it/s 0.0s\n",
      "                   all          4         14      0.336      0.289      0.342      0.174      0.433     0.0714     0.0709     0.0356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     83/100     0.721G     0.7549      1.916     0.3264     0.6589      1.086          9        640: 100%  1/1 13.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.5it/s 0.0s\n",
      "                   all          4         14      0.336      0.289      0.342      0.174      0.433     0.0714     0.0709     0.0356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     84/100     0.721G     0.8137       3.05     0.2944     0.7234       1.13         10        640: 100%  1/1 11.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.0it/s 0.0s\n",
      "                   all          4         14      0.418      0.429      0.353       0.18      0.139      0.143     0.0754     0.0356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     85/100     0.721G     0.6686      1.261     0.2655     0.4846     0.9081         15        640: 100%  1/1 13.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 20.9it/s 0.0s\n",
      "                   all          4         14      0.418      0.429      0.353       0.18      0.139      0.143     0.0754     0.0356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     86/100     0.721G       0.71      2.524     0.2468       0.62      1.048         14        640: 100%  1/1 14.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.5it/s 0.0s\n",
      "                   all          4         14      0.418      0.429      0.353       0.18      0.139      0.143     0.0754     0.0356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     87/100     0.721G     0.9828       3.06     0.2693      1.176      1.334          6        640: 100%  1/1 13.8it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.3it/s 0.0s\n",
      "                   all          4         14      0.418      0.429      0.353       0.18      0.139      0.143     0.0754     0.0356\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     88/100     0.721G     0.6616      1.943     0.2982     0.5089     0.9497         16        640: 100%  1/1 11.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.9it/s 0.0s\n",
      "                   all          4         14      0.396      0.357      0.372       0.18      0.208      0.189     0.0855     0.0449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     89/100     0.721G     0.6741      2.081     0.2591     0.5636     0.9916         12        640: 100%  1/1 14.0it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.4it/s 0.0s\n",
      "                   all          4         14      0.396      0.357      0.372       0.18      0.208      0.189     0.0855     0.0449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     90/100     0.721G     0.6901      1.949     0.3098     0.5971      1.065         11        640: 100%  1/1 13.5it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.0it/s 0.0s\n",
      "                   all          4         14      0.396      0.357      0.372       0.18      0.208      0.189     0.0855     0.0449\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     91/100     0.721G     0.5931     0.9547      0.285     0.5424      1.007          7        640: 100%  1/1 3.1it/s 0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 16.0it/s 0.1s\n",
      "                   all          4         14      0.396      0.357      0.372       0.18      0.208      0.189     0.0855     0.0449\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     92/100     0.721G     0.7593      1.492     0.3241     0.6285     0.9647          7        640: 100%  1/1 10.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 16.1it/s 0.1s\n",
      "                   all          4         14      0.375      0.343      0.377      0.194      0.237      0.214      0.088     0.0438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     93/100     0.721G     0.6246      1.453      0.258     0.5406     0.9474          7        640: 100%  1/1 13.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 16.3it/s 0.1s\n",
      "                   all          4         14      0.375      0.343      0.377      0.194      0.237      0.214      0.088     0.0438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     94/100     0.721G     0.7234      1.275      0.242     0.5477     0.8793          7        640: 100%  1/1 12.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 26.0it/s 0.0s\n",
      "                   all          4         14      0.375      0.343      0.377      0.194      0.237      0.214      0.088     0.0438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     95/100     0.721G     0.4502      1.232      0.218     0.4724     0.9837          7        640: 100%  1/1 12.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.7it/s 0.0s\n",
      "                   all          4         14      0.375      0.343      0.377      0.194      0.237      0.214      0.088     0.0438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     96/100     0.721G     0.5858      1.259     0.2232     0.4704     0.9425          7        640: 100%  1/1 10.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.9it/s 0.0s\n",
      "                   all          4         14      0.413      0.404      0.396      0.217      0.278      0.214      0.131      0.072\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     97/100     0.721G     0.6382      1.019     0.2237     0.5066     0.9152          7        640: 100%  1/1 14.3it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.7it/s 0.0s\n",
      "                   all          4         14      0.413      0.404      0.396      0.217      0.278      0.214      0.131      0.072\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     98/100     0.721G     0.7386     0.7621     0.2247     0.5375      1.042          7        640: 100%  1/1 14.6it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 25.4it/s 0.0s\n",
      "                   all          4         14      0.413      0.404      0.396      0.217      0.278      0.214      0.131      0.072\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K     99/100     0.721G     0.6068      1.127     0.2459     0.5135     0.9816          7        640: 100%  1/1 14.7it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 23.4it/s 0.0s\n",
      "                   all          4         14      0.413      0.404      0.396      0.217      0.278      0.214      0.131      0.072\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K    100/100     0.721G     0.5469       1.01     0.2759     0.5149     0.8911          7        640: 100%  1/1 11.2it/s 0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 24.9it/s 0.0s\n",
      "                   all          4         14      0.465        0.5      0.412      0.202       0.26      0.214       0.13     0.0688\n",
      "\n",
      "100 epochs completed in 0.012 hours.\n",
      "Optimizer stripped from /home/roni/dev_ws/yolo_tut/src/runs/pose/train/weights/last.pt, 6.9MB\n",
      "Optimizer stripped from /home/roni/dev_ws/yolo_tut/src/runs/pose/train/weights/best.pt, 6.9MB\n",
      "\n",
      "Validating /home/roni/dev_ws/yolo_tut/src/runs/pose/train/weights/best.pt...\n",
      "Ultralytics 8.3.235  Python-3.12.3 torch-2.9.1+cu128 CUDA:0 (NVIDIA GeForce RTX 2060, 5739MiB)\n",
      "YOLOv8n-pose summary (fused): 81 layers, 3,289,964 parameters, 0 gradients, 9.2 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%  1/1 34.6it/s 0.0s\n",
      "                   all          4         14      0.902      0.929      0.907      0.668       0.84        0.5      0.535      0.339\n",
      "Speed: 0.2ms preprocess, 2.8ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1m/home/roni/dev_ws/yolo_tut/src/runs/pose/train\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "results = model.train(data = \"coco8-pose.yaml\", epochs = 100, imgsz = 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6c7d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING  \n",
      "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (frame 1/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 39.1ms\n",
      "video 1/1 (frame 2/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 3/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 4/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.3ms\n",
      "video 1/1 (frame 5/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 6/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 7/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 8/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.2ms\n",
      "video 1/1 (frame 9/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.8ms\n",
      "video 1/1 (frame 10/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 11/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.4ms\n",
      "video 1/1 (frame 12/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 13/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 14/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 15/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 16/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 17/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 18/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 19/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 20/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 21/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 22/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.9ms\n",
      "video 1/1 (frame 23/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 24/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.6ms\n",
      "video 1/1 (frame 25/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.6ms\n",
      "video 1/1 (frame 26/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.0ms\n",
      "video 1/1 (frame 27/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 28/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.6ms\n",
      "video 1/1 (frame 29/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 30/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 31/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 32/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.4ms\n",
      "video 1/1 (frame 33/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 34/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 35/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 36/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 37/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 38/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 39/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 40/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 41/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 42/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 43/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 44/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.1ms\n",
      "video 1/1 (frame 45/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 46/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 47/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.2ms\n",
      "video 1/1 (frame 48/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 49/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 50/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 51/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.6ms\n",
      "video 1/1 (frame 52/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.9ms\n",
      "video 1/1 (frame 53/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 54/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.0ms\n",
      "video 1/1 (frame 55/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.3ms\n",
      "video 1/1 (frame 56/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 7.5ms\n",
      "video 1/1 (frame 57/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.6ms\n",
      "video 1/1 (frame 58/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 59/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.4ms\n",
      "video 1/1 (frame 60/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 61/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 62/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 63/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.4ms\n",
      "video 1/1 (frame 64/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 65/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 66/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 7.1ms\n",
      "video 1/1 (frame 67/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.9ms\n",
      "video 1/1 (frame 68/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 69/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 7.4ms\n",
      "video 1/1 (frame 70/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.3ms\n",
      "video 1/1 (frame 71/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 72/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 7.4ms\n",
      "video 1/1 (frame 73/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 8.7ms\n",
      "video 1/1 (frame 74/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 7.1ms\n",
      "video 1/1 (frame 75/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 76/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 77/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 78/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 79/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 80/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 8.1ms\n",
      "video 1/1 (frame 81/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 82/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.4ms\n",
      "video 1/1 (frame 83/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 84/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 8.1ms\n",
      "video 1/1 (frame 85/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 8.1ms\n",
      "video 1/1 (frame 86/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.1ms\n",
      "video 1/1 (frame 87/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 88/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 9.0ms\n",
      "video 1/1 (frame 89/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 8.5ms\n",
      "video 1/1 (frame 90/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.6ms\n",
      "video 1/1 (frame 91/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.8ms\n",
      "video 1/1 (frame 92/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 7.8ms\n",
      "video 1/1 (frame 93/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 94/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 95/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 8.1ms\n",
      "video 1/1 (frame 96/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.9ms\n",
      "video 1/1 (frame 97/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 8.6ms\n",
      "video 1/1 (frame 98/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 7.6ms\n",
      "video 1/1 (frame 99/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.1ms\n",
      "video 1/1 (frame 100/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 8.2ms\n",
      "video 1/1 (frame 101/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 7.6ms\n",
      "video 1/1 (frame 102/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.8ms\n",
      "video 1/1 (frame 103/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 7.7ms\n",
      "video 1/1 (frame 104/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 6.0ms\n",
      "video 1/1 (frame 105/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 7.3ms\n",
      "video 1/1 (frame 106/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 13 persons, 7.1ms\n",
      "video 1/1 (frame 107/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 6.7ms\n",
      "video 1/1 (frame 108/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 14 persons, 8.0ms\n",
      "video 1/1 (frame 109/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 7.1ms\n",
      "video 1/1 (frame 110/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 5.9ms\n",
      "video 1/1 (frame 111/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 6.4ms\n",
      "video 1/1 (frame 112/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 9.0ms\n",
      "video 1/1 (frame 113/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 114/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.8ms\n",
      "video 1/1 (frame 115/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.6ms\n",
      "video 1/1 (frame 116/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.2ms\n",
      "video 1/1 (frame 117/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.7ms\n",
      "video 1/1 (frame 118/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.5ms\n",
      "video 1/1 (frame 119/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 8.4ms\n",
      "video 1/1 (frame 120/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 6.3ms\n",
      "video 1/1 (frame 121/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.5ms\n",
      "video 1/1 (frame 122/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.2ms\n",
      "video 1/1 (frame 123/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 7.2ms\n",
      "video 1/1 (frame 124/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 125/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 126/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 127/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 128/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.5ms\n",
      "video 1/1 (frame 129/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 130/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 7.2ms\n",
      "video 1/1 (frame 131/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.0ms\n",
      "video 1/1 (frame 132/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 133/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 7.7ms\n",
      "video 1/1 (frame 134/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 7.2ms\n",
      "video 1/1 (frame 135/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 136/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.9ms\n",
      "video 1/1 (frame 137/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 7.2ms\n",
      "video 1/1 (frame 138/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.7ms\n",
      "video 1/1 (frame 139/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 140/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 7.0ms\n",
      "video 1/1 (frame 141/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.8ms\n",
      "video 1/1 (frame 142/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 143/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 144/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 8.1ms\n",
      "video 1/1 (frame 145/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 146/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 147/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 148/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.8ms\n",
      "video 1/1 (frame 149/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 150/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 151/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 7.1ms\n",
      "video 1/1 (frame 152/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 153/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 154/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 155/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 156/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 157/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.7ms\n",
      "video 1/1 (frame 158/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 159/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 160/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 161/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 162/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.3ms\n",
      "video 1/1 (frame 163/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 164/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 165/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 166/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.4ms\n",
      "video 1/1 (frame 167/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 168/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 169/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 170/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 171/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.8ms\n",
      "video 1/1 (frame 172/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 173/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 7.2ms\n",
      "video 1/1 (frame 174/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 175/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 176/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 177/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 178/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 179/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 180/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 181/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 10.9ms\n",
      "video 1/1 (frame 182/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 183/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 9.0ms\n",
      "video 1/1 (frame 184/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 7.3ms\n",
      "video 1/1 (frame 185/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 186/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 187/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 188/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 189/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 190/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.4ms\n",
      "video 1/1 (frame 191/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 192/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 193/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 194/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 195/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 196/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 197/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 198/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 199/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.7ms\n",
      "video 1/1 (frame 200/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.3ms\n",
      "video 1/1 (frame 201/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 202/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 203/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 204/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 205/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.9ms\n",
      "video 1/1 (frame 206/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 9.3ms\n",
      "video 1/1 (frame 207/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.3ms\n",
      "video 1/1 (frame 208/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.8ms\n",
      "video 1/1 (frame 209/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 210/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 211/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 212/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 213/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 214/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 215/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 216/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 217/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 218/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 219/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 220/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 221/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 222/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 223/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 224/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 225/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 226/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 227/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 228/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 229/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 230/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 231/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 232/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.6ms\n",
      "video 1/1 (frame 233/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 234/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 235/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 236/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 237/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 238/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 239/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 240/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 241/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 242/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 243/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 244/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 245/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 246/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 247/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 248/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 11.9ms\n",
      "video 1/1 (frame 249/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.4ms\n",
      "video 1/1 (frame 250/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.5ms\n",
      "video 1/1 (frame 251/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.9ms\n",
      "video 1/1 (frame 252/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.7ms\n",
      "video 1/1 (frame 253/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.5ms\n",
      "video 1/1 (frame 254/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 5.6ms\n",
      "video 1/1 (frame 255/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.3ms\n",
      "video 1/1 (frame 256/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 257/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.1ms\n",
      "video 1/1 (frame 258/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 259/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.8ms\n",
      "video 1/1 (frame 260/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 261/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 262/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.5ms\n",
      "video 1/1 (frame 263/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.9ms\n",
      "video 1/1 (frame 264/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.7ms\n",
      "video 1/1 (frame 265/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.1ms\n",
      "video 1/1 (frame 266/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 267/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.6ms\n",
      "video 1/1 (frame 268/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 269/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 270/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 271/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 272/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 273/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 274/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.5ms\n",
      "video 1/1 (frame 275/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 276/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 277/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 278/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.7ms\n",
      "video 1/1 (frame 279/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 280/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 281/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 282/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 283/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.7ms\n",
      "video 1/1 (frame 284/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 285/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 286/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 287/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 7.0ms\n",
      "video 1/1 (frame 288/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 289/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 290/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 291/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 292/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 293/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 294/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 7.2ms\n",
      "video 1/1 (frame 295/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 296/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 297/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 298/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 299/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 300/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 301/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 302/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 6.5ms\n",
      "video 1/1 (frame 303/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 304/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.3ms\n",
      "video 1/1 (frame 305/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 7.1ms\n",
      "video 1/1 (frame 306/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 307/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.6ms\n",
      "video 1/1 (frame 308/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 309/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.6ms\n",
      "video 1/1 (frame 310/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 311/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.8ms\n",
      "video 1/1 (frame 312/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 313/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 314/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.9ms\n",
      "video 1/1 (frame 315/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 316/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 317/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 7.2ms\n",
      "video 1/1 (frame 318/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 319/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.5ms\n",
      "video 1/1 (frame 320/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.4ms\n",
      "video 1/1 (frame 321/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 322/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 323/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 324/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.5ms\n",
      "video 1/1 (frame 325/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 326/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 327/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 328/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 329/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 9 persons, 5.9ms\n",
      "video 1/1 (frame 330/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 331/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 332/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 333/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 334/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 335/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 336/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 6.3ms\n",
      "video 1/1 (frame 337/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 5.5ms\n",
      "video 1/1 (frame 338/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 7.2ms\n",
      "video 1/1 (frame 339/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 11 persons, 5.5ms\n",
      "video 1/1 (frame 340/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 12 persons, 12.1ms\n",
      "video 1/1 (frame 341/341) /home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4: 384x640 10 persons, 6.0ms\n",
      "Speed: 1.9ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [148, 151, 149],\n",
       "         [149, 152, 150]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [147, 150, 148],\n",
       "         [149, 152, 150]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [147, 150, 148],\n",
       "         [148, 151, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.183077000040612, 'inference': 39.11691200005407, 'postprocess': 1.1957970000366913},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [147, 150, 148],\n",
       "         [147, 150, 148]],\n",
       " \n",
       "        [[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [146, 149, 147],\n",
       "         [147, 150, 148]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2801750000335232, 'inference': 6.657841999981429, 'postprocess': 1.0201000000051863},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [147, 150, 148],\n",
       "         [147, 150, 148]],\n",
       " \n",
       "        [[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [146, 149, 147],\n",
       "         [147, 150, 148]],\n",
       " \n",
       "        [[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7507700000578552, 'inference': 6.334650999974656, 'postprocess': 1.025546000050781},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [143, 143, 143],\n",
       "         [143, 143, 143]],\n",
       " \n",
       "        [[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.337076000027082, 'inference': 7.266337000032763, 'postprocess': 1.5465239999912228},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142]],\n",
       " \n",
       "        [[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [143, 143, 143],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         [126, 129, 127],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.438300000017989, 'inference': 5.613031999928353, 'postprocess': 1.1597079999319249},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [143, 143, 143],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2321109999220425, 'inference': 5.80774399998063, 'postprocess': 1.532839999981661},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [143, 143, 143],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [143, 143, 143],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8543669999644408, 'inference': 6.103433999896879, 'postprocess': 1.0859629999231402},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [139, 139, 139],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.188789000001634, 'inference': 6.200829000022168, 'postprocess': 1.0038730000587748},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7866480000066076, 'inference': 6.797327999947811, 'postprocess': 1.1126269999977012},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0176660000288393, 'inference': 6.788047999975788, 'postprocess': 1.2170120000973839},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.040098000065882, 'inference': 6.443803000024673, 'postprocess': 1.1537719999523688},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.851482999972177, 'inference': 6.209117000025799, 'postprocess': 1.2106780000067374},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [143, 143, 143],\n",
       "         [143, 143, 143],\n",
       "         [142, 142, 142]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         [197, 200, 198],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.50105599991457, 'inference': 6.177783999987696, 'postprocess': 1.6223789999685323},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [143, 143, 143],\n",
       "         [143, 143, 143],\n",
       "         [142, 142, 142]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [201, 204, 202],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [201, 204, 202],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         [197, 200, 198],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3595550000218282, 'inference': 6.341750999922624, 'postprocess': 0.9663590000172917},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [203, 206, 204],\n",
       "         [208, 211, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         [194, 197, 195],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.3062259999733214, 'inference': 5.9108890000061365, 'postprocess': 1.1934130000099685},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [205, 208, 206],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [198, 201, 199],\n",
       "         [192, 195, 193],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4558189999434035, 'inference': 6.257724999954917, 'postprocess': 1.4106309999988298},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [205, 208, 206],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [203, 206, 204],\n",
       "         [198, 201, 199],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [197, 200, 198],\n",
       "         [190, 193, 191],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.128699000058077, 'inference': 6.319835999988754, 'postprocess': 1.1734079999996538},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [201, 204, 202],\n",
       "         [192, 195, 193],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[198, 201, 199],\n",
       "         [198, 201, 199],\n",
       "         [186, 189, 187],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         [186, 189, 187],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2411669999892183, 'inference': 6.31635100000949, 'postprocess': 1.3881739999987985},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [201, 204, 202],\n",
       "         [187, 190, 188],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[198, 201, 199],\n",
       "         [197, 200, 198],\n",
       "         [183, 186, 184],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [197, 200, 198],\n",
       "         [183, 186, 184],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.340166999942994, 'inference': 5.753012000013769, 'postprocess': 1.220449000015833},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [199, 202, 200],\n",
       "         [182, 185, 183],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [192, 195, 193],\n",
       "         [180, 183, 181],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [185, 188, 186],\n",
       "         [185, 188, 186],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9554259999949863, 'inference': 5.733526999961214, 'postprocess': 1.753859000018565},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         [125, 128, 126],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [192, 195, 193],\n",
       "         [179, 182, 180],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [185, 188, 186],\n",
       "         [183, 186, 184],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [182, 185, 183],\n",
       "         [190, 193, 191],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0775220000359695, 'inference': 5.978278000043247, 'postprocess': 0.9809150000137379},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [187, 190, 188],\n",
       "         [182, 185, 183],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[198, 201, 199],\n",
       "         [183, 186, 184],\n",
       "         [186, 189, 187],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [183, 186, 184],\n",
       "         [193, 196, 194],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.3907409999992524, 'inference': 6.909875000019383, 'postprocess': 1.511744000026738},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[197, 200, 198],\n",
       "         [184, 187, 185],\n",
       "         [186, 189, 187],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[189, 192, 190],\n",
       "         [184, 187, 185],\n",
       "         [193, 196, 194],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[184, 187, 185],\n",
       "         [185, 188, 186],\n",
       "         [198, 201, 199],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2353530000837054, 'inference': 6.316699999956654, 'postprocess': 1.1434609999696477},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [183, 186, 184],\n",
       "         [191, 194, 192],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [185, 188, 186],\n",
       "         [197, 200, 198],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[180, 183, 181],\n",
       "         [189, 192, 190],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0736310000302183, 'inference': 6.5550020000273435, 'postprocess': 1.7043949999333563},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [183, 186, 184],\n",
       "         [191, 194, 192],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [185, 188, 186],\n",
       "         [197, 200, 198],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[180, 183, 181],\n",
       "         [189, 192, 190],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.1629120000170587, 'inference': 6.605558000046585, 'postprocess': 1.4984169999934238},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [184, 187, 185],\n",
       "         [196, 199, 197],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [190, 193, 191],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[182, 185, 183],\n",
       "         [192, 195, 193],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5277300000207106, 'inference': 6.957870000064759, 'postprocess': 0.9643419999747493},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [186, 189, 187],\n",
       "         [197, 200, 198],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [191, 194, 192],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [196, 199, 197],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.5214260000439026, 'inference': 5.914521000022432, 'postprocess': 2.258388999962335},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [131, 131, 131],\n",
       "         [128, 128, 128]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [135, 135, 135],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [140, 140, 140],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[189, 192, 190],\n",
       "         [197, 200, 198],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3257529999464168, 'inference': 6.556072999956086, 'postprocess': 0.9557160000213116},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [139, 139, 139],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [140, 140, 140],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [143, 143, 143],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[189, 192, 190],\n",
       "         [197, 200, 198],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.989496999954099, 'inference': 6.24719399991136, 'postprocess': 1.0464279999951032},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [143, 143, 143],\n",
       "         [140, 140, 140],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [143, 143, 143],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [145, 145, 145],\n",
       "         [143, 143, 143]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[189, 192, 190],\n",
       "         [197, 200, 198],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.9744409999921118, 'inference': 6.406392000030792, 'postprocess': 1.2665000000424698},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [143, 143, 143],\n",
       "         [142, 142, 142],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [144, 144, 144],\n",
       "         [142, 142, 142]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [146, 146, 146],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [196, 199, 197],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4594770000257995, 'inference': 6.234518000042044, 'postprocess': 1.199810000002799},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [143, 143, 143],\n",
       "         [142, 142, 142],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [144, 144, 144],\n",
       "         [142, 142, 142]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [146, 146, 146],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8307510000568072, 'inference': 5.377566999982264, 'postprocess': 0.9641510000619746},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [144, 144, 144],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.094645999932254, 'inference': 6.7846509999753835, 'postprocess': 1.0810559999754332},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [143, 143, 143],\n",
       "         [142, 142, 142],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.553825999939363, 'inference': 6.432159000041793, 'postprocess': 1.152580999928432},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2757829999827663, 'inference': 5.681106999986696, 'postprocess': 0.9855949999746372},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0033600000033402, 'inference': 5.830755000033605, 'postprocess': 0.9785280000187413},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.653358999988086, 'inference': 6.0268979999591465, 'postprocess': 1.2331469999935507},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[184, 187, 185],\n",
       "         [186, 189, 187],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[184, 187, 185],\n",
       "         [192, 195, 193],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [199, 202, 200],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4219810000213329, 'inference': 5.831622999949104, 'postprocess': 1.3044739999941157},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [196, 199, 197],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7730490000076315, 'inference': 5.7518119999713235, 'postprocess': 0.9951319999572661},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[184, 187, 185],\n",
       "         [186, 189, 187],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[184, 187, 185],\n",
       "         [192, 195, 193],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [199, 202, 200],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7538339999418895, 'inference': 5.631785999980821, 'postprocess': 2.0924780000086685},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [196, 199, 197],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.57306299999982, 'inference': 6.5077659999133175, 'postprocess': 1.6091119999828152},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [189, 192, 190],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [196, 199, 197],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.3304439999947135, 'inference': 6.778089000022192, 'postprocess': 2.181152999924052},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[185, 188, 186],\n",
       "         [190, 193, 191],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [196, 199, 197],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.0287639999642124, 'inference': 6.3020970000025045, 'postprocess': 1.0627440000234856},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [190, 193, 191],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [196, 199, 197],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.499450999948749, 'inference': 7.0832430000109525, 'postprocess': 1.109762999931263},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [190, 193, 191],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [196, 199, 197],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5719670000180486, 'inference': 6.678390999923067, 'postprocess': 1.172560999975758},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [190, 193, 191],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[187, 190, 188],\n",
       "         [196, 199, 197],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.288081999973656, 'inference': 5.799396000043089, 'postprocess': 1.2706549999847994},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[189, 192, 190],\n",
       "         [197, 200, 198],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.4048330000141505, 'inference': 7.1793030000435465, 'postprocess': 1.1984730000449417},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[189, 192, 190],\n",
       "         [197, 200, 198],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.379614999995283, 'inference': 6.184016000020165, 'postprocess': 1.1328649999313711},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[189, 192, 190],\n",
       "         [197, 200, 198],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.319007999995847, 'inference': 5.514890999961608, 'postprocess': 1.2036019999186465},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[186, 189, 187],\n",
       "         [192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[189, 192, 190],\n",
       "         [197, 200, 198],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[190, 193, 191],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2072289999878194, 'inference': 5.588712999951895, 'postprocess': 1.014369000017723},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [132, 135, 133],\n",
       "         [132, 135, 133],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [132, 135, 133],\n",
       "         [132, 135, 133],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [130, 133, 131],\n",
       "         [130, 133, 131],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [196, 199, 197],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.976761000013539, 'inference': 6.602125999961572, 'postprocess': 1.4553519999935816},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [130, 133, 131],\n",
       "         [130, 133, 131],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [130, 133, 131],\n",
       "         [130, 133, 131],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [130, 133, 131],\n",
       "         [130, 133, 131],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [196, 199, 197],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7100950000212833, 'inference': 6.902724000042326, 'postprocess': 1.1066539999546876},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [198, 201, 199],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2808989999939513, 'inference': 5.793587999960437, 'postprocess': 1.5175749999798427},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [198, 201, 199],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.2541939999882743, 'inference': 6.988511000031394, 'postprocess': 1.6005050000558185},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [133, 136, 134],\n",
       "         [134, 137, 135],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [133, 136, 134],\n",
       "         [133, 136, 134],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [132, 135, 133],\n",
       "         [133, 136, 134],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [198, 201, 199],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4740350000010949, 'inference': 7.346172999973533, 'postprocess': 1.2661579999075911},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [132, 135, 133],\n",
       "         [134, 137, 135],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [132, 135, 133],\n",
       "         [134, 137, 135],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [132, 135, 133],\n",
       "         [133, 136, 134],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [198, 201, 199],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.4525689999563838, 'inference': 7.498285999986365, 'postprocess': 1.7908099999885962},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [129, 132, 130],\n",
       "         [132, 135, 133],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [129, 132, 130],\n",
       "         [132, 135, 133],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [130, 133, 131],\n",
       "         [132, 135, 133],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [198, 201, 199],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.5261990000208243, 'inference': 7.628596000017751, 'postprocess': 1.2491620000218973},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [128, 131, 129],\n",
       "         [133, 136, 134],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [128, 131, 129],\n",
       "         [133, 136, 134],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [130, 133, 131],\n",
       "         [134, 137, 135],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2111680000543856, 'inference': 5.838008999944577, 'postprocess': 1.274174999934985},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [127, 130, 128],\n",
       "         [132, 135, 133],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [127, 130, 128],\n",
       "         [132, 135, 133],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [128, 131, 129],\n",
       "         [133, 136, 134],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3143749999926513, 'inference': 5.448817000001327, 'postprocess': 1.450193999971816},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [126, 129, 127],\n",
       "         [130, 133, 131],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [126, 129, 127],\n",
       "         [130, 133, 131],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [127, 130, 128],\n",
       "         [132, 135, 133],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.725078000049507, 'inference': 6.8754530000205705, 'postprocess': 1.370614999927966},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [125, 128, 126],\n",
       "         [129, 132, 130],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [125, 128, 126],\n",
       "         [129, 132, 130],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [127, 130, 128],\n",
       "         [130, 133, 131],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7688170000838, 'inference': 6.007807000059984, 'postprocess': 0.991779000059978},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [130, 133, 131],\n",
       "         [127, 130, 128],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [128, 131, 129],\n",
       "         [127, 130, 128],\n",
       "         [132, 135, 133]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [128, 131, 129],\n",
       "         [129, 132, 130],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[192, 195, 193],\n",
       "         [200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.7645860000120592, 'inference': 6.429904000015085, 'postprocess': 1.4734040000803361},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [135, 138, 136],\n",
       "         [130, 133, 131],\n",
       "         [139, 142, 140]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [134, 137, 135],\n",
       "         [129, 132, 130],\n",
       "         [136, 139, 137]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [130, 133, 131],\n",
       "         [129, 132, 130],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.287432000026456, 'inference': 7.415002000016102, 'postprocess': 1.0230509999473725},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [135, 138, 136],\n",
       "         [134, 137, 135],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [135, 138, 136],\n",
       "         [133, 136, 134],\n",
       "         [140, 143, 141]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [133, 136, 134],\n",
       "         [132, 135, 133],\n",
       "         [135, 138, 136]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4181840000446755, 'inference': 6.168770999920525, 'postprocess': 1.1822339999980613},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [134, 137, 135],\n",
       "         [136, 139, 137],\n",
       "         [147, 150, 148]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [134, 137, 135],\n",
       "         [136, 139, 137],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [132, 135, 133],\n",
       "         [134, 137, 135],\n",
       "         [137, 140, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[193, 196, 194],\n",
       "         [201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.110893999997643, 'inference': 6.2529509999649235, 'postprocess': 0.9789410000848875},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[122, 122, 122],\n",
       "         [123, 123, 123],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [129, 132, 130],\n",
       "         [136, 139, 137],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[122, 122, 122],\n",
       "         [123, 123, 123],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[122, 122, 122],\n",
       "         [122, 122, 122],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [134, 137, 135],\n",
       "         [133, 136, 134],\n",
       "         [137, 140, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6184940000130155, 'inference': 7.083637000050658, 'postprocess': 1.7877310000358193},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 125, 125],\n",
       "         [130, 130, 130],\n",
       "         [123, 123, 123],\n",
       "         ...,\n",
       "         [129, 132, 130],\n",
       "         [136, 139, 137],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[119, 119, 119],\n",
       "         [135, 135, 135],\n",
       "         [122, 122, 122],\n",
       "         ...,\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[ 88,  88,  88],\n",
       "         [136, 136, 136],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [134, 137, 135],\n",
       "         [133, 136, 134],\n",
       "         [137, 140, 138]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.7135440000165545, 'inference': 6.904596000026686, 'postprocess': 1.2308689999827038},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[ 11,  11,  11],\n",
       "         [  8,   8,   8],\n",
       "         [  8,   8,   8],\n",
       "         ...,\n",
       "         [116, 119, 117],\n",
       "         [135, 138, 136],\n",
       "         [140, 143, 141]],\n",
       " \n",
       "        [[ 65,  65,  65],\n",
       "         [ 18,  18,  18],\n",
       "         [ 12,  12,  12],\n",
       "         ...,\n",
       "         [126, 129, 127],\n",
       "         [133, 136, 134],\n",
       "         [139, 142, 140]],\n",
       " \n",
       "        [[109, 109, 109],\n",
       "         [ 50,  50,  50],\n",
       "         [  5,   5,   5],\n",
       "         ...,\n",
       "         [128, 131, 129],\n",
       "         [130, 133, 131],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7634929999985616, 'inference': 6.232443999920179, 'postprocess': 1.006595000035304},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[143, 143, 143],\n",
       "         [ 86,  86,  86],\n",
       "         [  2,   2,   2],\n",
       "         ...,\n",
       "         [112, 115, 113],\n",
       "         [134, 137, 135],\n",
       "         [137, 140, 138]],\n",
       " \n",
       "        [[117, 117, 117],\n",
       "         [111, 111, 111],\n",
       "         [ 48,  48,  48],\n",
       "         ...,\n",
       "         [128, 131, 129],\n",
       "         [133, 136, 134],\n",
       "         [137, 140, 138]],\n",
       " \n",
       "        [[103, 103, 103],\n",
       "         [124, 124, 124],\n",
       "         [105, 105, 105],\n",
       "         ...,\n",
       "         [134, 137, 135],\n",
       "         [130, 133, 131],\n",
       "         [134, 137, 135]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4382630000682184, 'inference': 7.3558540000249195, 'postprocess': 2.158548000011251},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[104, 104, 104],\n",
       "         [114, 114, 114],\n",
       "         [121, 121, 121],\n",
       "         ...,\n",
       "         [ 97, 100,  98],\n",
       "         [133, 136, 134],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        [[108, 108, 108],\n",
       "         [116, 116, 116],\n",
       "         [122, 122, 122],\n",
       "         ...,\n",
       "         [113, 116, 114],\n",
       "         [141, 144, 142],\n",
       "         [141, 144, 142]],\n",
       " \n",
       "        [[110, 110, 110],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [129, 132, 130],\n",
       "         [149, 152, 150],\n",
       "         [149, 152, 150]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [203, 206, 204],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.9267700000209516, 'inference': 7.330579999916154, 'postprocess': 1.1070659999177224},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[100, 100, 100],\n",
       "         [118, 118, 118],\n",
       "         [118, 118, 118],\n",
       "         ...,\n",
       "         [ 45,  48,  46],\n",
       "         [100, 103, 101],\n",
       "         [137, 140, 138]],\n",
       " \n",
       "        [[105, 105, 105],\n",
       "         [114, 114, 114],\n",
       "         [103, 103, 103],\n",
       "         ...,\n",
       "         [ 50,  53,  51],\n",
       "         [109, 112, 110],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[117, 117, 117],\n",
       "         [110, 110, 110],\n",
       "         [ 94,  94,  94],\n",
       "         ...,\n",
       "         [ 61,  64,  62],\n",
       "         [128, 131, 129],\n",
       "         [151, 154, 152]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[198, 201, 199],\n",
       "         [204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5033119999543487, 'inference': 5.784618000006958, 'postprocess': 0.9678139999778068},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[100, 100, 100],\n",
       "         [112, 112, 112],\n",
       "         [116, 116, 116],\n",
       "         ...,\n",
       "         [ 37,  40,  38],\n",
       "         [ 69,  72,  70],\n",
       "         [126, 129, 127]],\n",
       " \n",
       "        [[109, 109, 109],\n",
       "         [111, 111, 111],\n",
       "         [101, 101, 101],\n",
       "         ...,\n",
       "         [ 36,  39,  37],\n",
       "         [ 78,  81,  79],\n",
       "         [133, 136, 134]],\n",
       " \n",
       "        [[116, 116, 116],\n",
       "         [112, 112, 112],\n",
       "         [100, 100, 100],\n",
       "         ...,\n",
       "         [ 36,  39,  37],\n",
       "         [ 95,  98,  96],\n",
       "         [147, 150, 148]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.7154730000802374, 'inference': 7.4485009999989416, 'postprocess': 1.7434269999512253},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[ 37,  37,  37],\n",
       "         [ 72,  72,  72],\n",
       "         [116, 116, 116],\n",
       "         ...,\n",
       "         [ 72,  72,  72],\n",
       "         [ 52,  52,  52],\n",
       "         [ 41,  41,  41]],\n",
       " \n",
       "        [[ 90,  90,  90],\n",
       "         [111, 111, 111],\n",
       "         [117, 117, 117],\n",
       "         ...,\n",
       "         [ 65,  65,  65],\n",
       "         [ 53,  53,  53],\n",
       "         [ 52,  52,  52]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [105, 105, 105],\n",
       "         ...,\n",
       "         [ 51,  51,  51],\n",
       "         [ 41,  41,  41],\n",
       "         [ 55,  55,  55]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9374639999796273, 'inference': 8.706378000056247, 'postprocess': 2.500985999972727},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[  9,   9,   9],\n",
       "         [  0,   0,   0],\n",
       "         [ 29,  29,  29],\n",
       "         ...,\n",
       "         [ 67,  67,  67],\n",
       "         [ 58,  58,  58],\n",
       "         [ 48,  48,  48]],\n",
       " \n",
       "        [[ 24,  24,  24],\n",
       "         [ 32,  32,  32],\n",
       "         [ 72,  72,  72],\n",
       "         ...,\n",
       "         [ 59,  59,  59],\n",
       "         [ 55,  55,  55],\n",
       "         [ 55,  55,  55]],\n",
       " \n",
       "        [[ 71,  71,  71],\n",
       "         [ 83,  83,  83],\n",
       "         [ 97,  97,  97],\n",
       "         ...,\n",
       "         [ 51,  51,  51],\n",
       "         [ 45,  45,  45],\n",
       "         [ 52,  52,  52]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4722120000669747, 'inference': 7.130601999961073, 'postprocess': 1.6027350000058505},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[  0,   0,   0],\n",
       "         [ 20,  20,  20],\n",
       "         [ 12,  12,  12],\n",
       "         ...,\n",
       "         [ 61,  61,  61],\n",
       "         [ 62,  62,  62],\n",
       "         [ 55,  55,  55]],\n",
       " \n",
       "        [[  4,   4,   4],\n",
       "         [  2,   2,   2],\n",
       "         [  5,   5,   5],\n",
       "         ...,\n",
       "         [ 53,  53,  53],\n",
       "         [ 57,  57,  57],\n",
       "         [ 58,  58,  58]],\n",
       " \n",
       "        [[ 11,  11,  11],\n",
       "         [  8,   8,   8],\n",
       "         [ 29,  29,  29],\n",
       "         ...,\n",
       "         [ 50,  50,  50],\n",
       "         [ 47,  47,  47],\n",
       "         [ 47,  47,  47]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [204, 207, 205],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.657308000038938, 'inference': 6.5128819999245025, 'postprocess': 1.1202030000276864},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[114, 114, 114],\n",
       "         [107, 107, 107],\n",
       "         [ 87,  87,  87],\n",
       "         ...,\n",
       "         [ 57,  60,  58],\n",
       "         [ 54,  57,  55],\n",
       "         [ 50,  53,  51]],\n",
       " \n",
       "        [[112, 112, 112],\n",
       "         [ 91,  91,  91],\n",
       "         [ 64,  64,  64],\n",
       "         ...,\n",
       "         [ 54,  57,  55],\n",
       "         [ 52,  55,  53],\n",
       "         [ 44,  47,  45]],\n",
       " \n",
       "        [[108, 108, 108],\n",
       "         [ 86,  86,  86],\n",
       "         [ 69,  69,  69],\n",
       "         ...,\n",
       "         [ 49,  52,  50],\n",
       "         [ 48,  51,  49],\n",
       "         [ 50,  53,  51]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8457129999660538, 'inference': 5.9121610000829605, 'postprocess': 1.041137000015624},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 54,  57,  55],\n",
       "         [ 56,  59,  57],\n",
       "         [ 51,  54,  52]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 55,  58,  56],\n",
       "         [ 56,  59,  57],\n",
       "         [ 48,  51,  49]],\n",
       " \n",
       "        [[112, 112, 112],\n",
       "         [112, 112, 112],\n",
       "         [112, 112, 112],\n",
       "         ...,\n",
       "         [ 51,  54,  52],\n",
       "         [ 51,  54,  52],\n",
       "         [ 48,  51,  49]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [203, 206, 204],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.405836999992971, 'inference': 6.145919999994476, 'postprocess': 0.9857159999455689},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 62,  65,  63],\n",
       "         [ 48,  51,  49],\n",
       "         [ 63,  66,  64]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 52,  55,  53],\n",
       "         [ 44,  47,  45],\n",
       "         [ 63,  66,  64]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 42,  45,  43],\n",
       "         [ 54,  57,  55],\n",
       "         [ 61,  64,  62]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [203, 206, 204],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.8182600000263847, 'inference': 6.409443999928044, 'postprocess': 1.5682759999435802},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 71,  74,  72],\n",
       "         [ 51,  54,  52],\n",
       "         [ 62,  65,  63]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 59,  62,  60],\n",
       "         [ 50,  53,  51],\n",
       "         [ 59,  62,  60]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 38,  41,  39],\n",
       "         [ 49,  52,  50],\n",
       "         [ 59,  62,  60]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [203, 206, 204],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.817376000052718, 'inference': 6.438779000063732, 'postprocess': 1.3525310000659374},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 63,  66,  64],\n",
       "         [ 50,  53,  51],\n",
       "         [ 62,  65,  63]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 55,  58,  56],\n",
       "         [ 42,  45,  43],\n",
       "         [ 64,  67,  65]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 44,  47,  45],\n",
       "         [ 52,  55,  53],\n",
       "         [ 61,  64,  62]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [203, 206, 204],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.0593179999414133, 'inference': 8.100495000007868, 'postprocess': 1.5830530001039733},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 69,  72,  70],\n",
       "         [ 56,  59,  57],\n",
       "         [ 52,  55,  53]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 55,  58,  56],\n",
       "         [ 56,  59,  57],\n",
       "         [ 65,  68,  66]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 45,  48,  46],\n",
       "         [ 61,  64,  62],\n",
       "         [ 63,  66,  64]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [204, 207, 205],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [203, 206, 204],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3011640000968328, 'inference': 5.735369000035462, 'postprocess': 1.0153350000337014},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 69,  72,  70],\n",
       "         [ 56,  59,  57],\n",
       "         [ 52,  55,  53]],\n",
       " \n",
       "        [[114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         [114, 114, 114],\n",
       "         ...,\n",
       "         [ 55,  58,  56],\n",
       "         [ 56,  59,  57],\n",
       "         [ 65,  68,  66]],\n",
       " \n",
       "        [[116, 116, 116],\n",
       "         [116, 116, 116],\n",
       "         [116, 116, 116],\n",
       "         ...,\n",
       "         [ 45,  48,  46],\n",
       "         [ 61,  64,  62],\n",
       "         [ 63,  66,  64]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.843508999968435, 'inference': 5.447429999890119, 'postprocess': 1.0016029999633247},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[119, 119, 119],\n",
       "         [119, 119, 119],\n",
       "         [119, 119, 119],\n",
       "         ...,\n",
       "         [ 78,  81,  79],\n",
       "         [ 49,  52,  50],\n",
       "         [ 61,  64,  62]],\n",
       " \n",
       "        [[124, 124, 124],\n",
       "         [124, 124, 124],\n",
       "         [124, 124, 124],\n",
       "         ...,\n",
       "         [ 64,  67,  65],\n",
       "         [ 42,  45,  43],\n",
       "         [ 72,  75,  73]],\n",
       " \n",
       "        [[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [ 58,  61,  59],\n",
       "         [ 48,  51,  49],\n",
       "         [ 64,  67,  65]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.469864000000598, 'inference': 6.107033999910527, 'postprocess': 1.0057520000827935},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [ 85,  88,  86],\n",
       "         [ 51,  54,  52],\n",
       "         [ 52,  55,  53]],\n",
       " \n",
       "        [[125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [ 70,  73,  71],\n",
       "         [ 45,  48,  46],\n",
       "         [ 69,  72,  70]],\n",
       " \n",
       "        [[125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [ 62,  65,  63],\n",
       "         [ 43,  46,  44],\n",
       "         [ 69,  72,  70]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.2395699999151475, 'inference': 8.148600000026818, 'postprocess': 1.368120999927669},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 126, 126],\n",
       "         [125, 125, 125],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [105, 108, 106],\n",
       "         [ 63,  66,  64],\n",
       "         [ 54,  57,  55]],\n",
       " \n",
       "        [[125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [ 90,  93,  91],\n",
       "         [ 50,  53,  51],\n",
       "         [ 65,  68,  66]],\n",
       " \n",
       "        [[125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [ 75,  78,  76],\n",
       "         [ 42,  45,  43],\n",
       "         [ 65,  68,  66]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.2132929999543194, 'inference': 8.14636499990229, 'postprocess': 1.0756319999245534},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[125, 125, 125],\n",
       "         [124, 124, 124],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [125, 128, 126],\n",
       "         [ 75,  78,  76],\n",
       "         [ 55,  58,  56]],\n",
       " \n",
       "        [[125, 125, 125],\n",
       "         [124, 124, 124],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [108, 111, 109],\n",
       "         [ 55,  58,  56],\n",
       "         [ 62,  65,  63]],\n",
       " \n",
       "        [[125, 125, 125],\n",
       "         [124, 124, 124],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [ 86,  89,  87],\n",
       "         [ 40,  43,  41],\n",
       "         [ 61,  64,  62]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[201, 204, 202],\n",
       "         [200, 203, 201],\n",
       "         [203, 206, 204],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         [201, 204, 202],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2461349999739468, 'inference': 7.114980000096693, 'postprocess': 1.0239770000453063},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 126, 126],\n",
       "         [125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [141, 144, 142],\n",
       "         [ 84,  87,  85],\n",
       "         [ 49,  52,  50]],\n",
       " \n",
       "        [[125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [128, 131, 129],\n",
       "         [ 69,  72,  70],\n",
       "         [ 55,  58,  56]],\n",
       " \n",
       "        [[126, 126, 126],\n",
       "         [125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [102, 105, 103],\n",
       "         [ 48,  51,  49],\n",
       "         [ 56,  59,  57]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [200, 203, 201],\n",
       "         [200, 203, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         [199, 202, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9448470000043017, 'inference': 5.7199909999781084, 'postprocess': 1.1206479999827934},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [141, 144, 142],\n",
       "         [160, 163, 161],\n",
       "         [126, 129, 127]],\n",
       " \n",
       "        [[125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         [124, 124, 124],\n",
       "         ...,\n",
       "         [149, 152, 150],\n",
       "         [147, 150, 148],\n",
       "         [ 93,  96,  94]],\n",
       " \n",
       "        [[126, 126, 126],\n",
       "         [125, 125, 125],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [148, 151, 149],\n",
       "         [101, 104, 102],\n",
       "         [ 44,  47,  45]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[197, 200, 198],\n",
       "         [198, 201, 199],\n",
       "         [198, 201, 199],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[197, 200, 198],\n",
       "         [197, 200, 198],\n",
       "         [197, 200, 198],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[197, 200, 198],\n",
       "         [197, 200, 198],\n",
       "         [197, 200, 198],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.80960000000141, 'inference': 9.012920000031954, 'postprocess': 1.5133429999423242},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [141, 144, 142],\n",
       "         [160, 163, 161],\n",
       "         [126, 129, 127]],\n",
       " \n",
       "        [[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [141, 144, 142],\n",
       "         [160, 163, 161],\n",
       "         [126, 129, 127]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [149, 152, 150],\n",
       "         [147, 150, 148],\n",
       "         [ 93,  96,  94]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [196, 199, 197],\n",
       "         [196, 199, 197],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [194, 197, 195],\n",
       "         [194, 197, 195],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[194, 197, 195],\n",
       "         [194, 197, 195],\n",
       "         [194, 197, 195],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.049259000045822, 'inference': 8.466287000032935, 'postprocess': 1.3298860000077184},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [141, 144, 142],\n",
       "         [157, 160, 158],\n",
       "         [135, 138, 136]],\n",
       " \n",
       "        [[126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         [125, 125, 125],\n",
       "         ...,\n",
       "         [141, 144, 142],\n",
       "         [157, 160, 158],\n",
       "         [137, 140, 138]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [156, 159, 157],\n",
       "         [130, 133, 131]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [192, 195, 193],\n",
       "         [192, 195, 193],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.59567499995228, 'inference': 6.573778999950264, 'postprocess': 1.194953999970494},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [144, 147, 145],\n",
       "         [147, 150, 148]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [143, 146, 144],\n",
       "         [148, 151, 149]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [139, 142, 140],\n",
       "         [139, 142, 140],\n",
       "         [154, 157, 155]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [192, 195, 193],\n",
       "         [192, 195, 193],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.549348000002283, 'inference': 6.836985000063578, 'postprocess': 1.3895790000333363},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [143, 146, 144],\n",
       "         [148, 151, 149]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [143, 146, 144],\n",
       "         [148, 151, 149]],\n",
       " \n",
       "        [[128, 128, 128],\n",
       "         [126, 126, 126],\n",
       "         [126, 126, 126],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [144, 147, 145],\n",
       "         [147, 150, 148]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [192, 195, 193],\n",
       "         [192, 195, 193],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         [191, 194, 192],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.736806000053548, 'inference': 7.833261000087077, 'postprocess': 1.3886289999618384},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [143, 146, 144],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [185, 188, 186],\n",
       "         [186, 189, 187],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [185, 188, 186],\n",
       "         [185, 188, 186],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [184, 187, 185],\n",
       "         [184, 187, 185],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3962409999521697, 'inference': 5.754936000016642, 'postprocess': 1.024808000011035},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [185, 188, 186],\n",
       "         [186, 189, 187],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [185, 188, 186],\n",
       "         [185, 188, 186],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[183, 186, 184],\n",
       "         [184, 187, 185],\n",
       "         [184, 187, 185],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8288410000195654, 'inference': 6.006952000007004, 'postprocess': 1.711563000071692},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [144, 147, 145],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[182, 185, 183],\n",
       "         [183, 186, 184],\n",
       "         [184, 187, 185],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[182, 185, 183],\n",
       "         [184, 187, 185],\n",
       "         [184, 187, 185],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[182, 185, 183],\n",
       "         [183, 186, 184],\n",
       "         [183, 186, 184],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0671240000638136, 'inference': 8.101491000047645, 'postprocess': 1.4134200000626151},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[179, 182, 180],\n",
       "         [180, 183, 181],\n",
       "         [182, 185, 183],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[179, 182, 180],\n",
       "         [182, 185, 183],\n",
       "         [182, 185, 183],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[179, 182, 180],\n",
       "         [182, 185, 183],\n",
       "         [182, 185, 183],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8571509999674163, 'inference': 6.922461000044677, 'postprocess': 1.2116359999936321},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [129, 129, 129],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[173, 176, 174],\n",
       "         [173, 176, 174],\n",
       "         [176, 179, 177],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[171, 174, 172],\n",
       "         [171, 174, 172],\n",
       "         [173, 176, 174],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[169, 172, 170],\n",
       "         [169, 172, 170],\n",
       "         [171, 174, 172],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.80913199992483, 'inference': 8.6379089999582, 'postprocess': 1.6019469999264402},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [129, 129, 129],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[141, 144, 142],\n",
       "         [144, 147, 145],\n",
       "         [151, 154, 152],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[143, 146, 144],\n",
       "         [147, 150, 148],\n",
       "         [154, 157, 155],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[146, 149, 147],\n",
       "         [149, 152, 150],\n",
       "         [153, 156, 154],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.239387999907194, 'inference': 7.617826000000605, 'postprocess': 1.373938999904567},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [129, 129, 129],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[170, 173, 171],\n",
       "         [170, 173, 171],\n",
       "         [170, 173, 171],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[170, 173, 171],\n",
       "         [171, 174, 172],\n",
       "         [172, 175, 173],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[170, 173, 171],\n",
       "         [171, 174, 172],\n",
       "         [173, 176, 174],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2535849999721904, 'inference': 6.0895999999956985, 'postprocess': 1.404116999992766},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [129, 129, 129],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[175, 178, 176],\n",
       "         [175, 178, 176],\n",
       "         [175, 178, 176],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[173, 176, 174],\n",
       "         [175, 178, 176],\n",
       "         [176, 179, 177],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[175, 178, 176],\n",
       "         [176, 179, 177],\n",
       "         [176, 179, 177],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.22243600001093, 'inference': 8.19480100005876, 'postprocess': 1.7673730000069554},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [151, 151, 151]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [151, 151, 151]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [151, 151, 151],\n",
       "         [151, 151, 151]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[179, 179, 179],\n",
       "         [179, 179, 179],\n",
       "         [179, 179, 179],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[179, 179, 179],\n",
       "         [179, 179, 179],\n",
       "         [180, 180, 180],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[180, 180, 180],\n",
       "         [180, 180, 180],\n",
       "         [180, 180, 180],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7824420000351893, 'inference': 7.557132999977512, 'postprocess': 1.5974019999021039},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[180, 180, 180],\n",
       "         [179, 179, 179],\n",
       "         [179, 179, 179],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[181, 181, 181],\n",
       "         [180, 180, 180],\n",
       "         [180, 180, 180],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[181, 181, 181],\n",
       "         [181, 181, 181],\n",
       "         [181, 181, 181],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.785475999985465, 'inference': 6.775435000008656, 'postprocess': 1.6398110000181987},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[179, 179, 179],\n",
       "         [180, 180, 180],\n",
       "         [179, 179, 179],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[181, 181, 181],\n",
       "         [181, 181, 181],\n",
       "         [180, 180, 180],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[181, 181, 181],\n",
       "         [181, 181, 181],\n",
       "         [180, 180, 180],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8370999999888227, 'inference': 7.7467590000424025, 'postprocess': 1.1174770000934586},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[182, 182, 182],\n",
       "         [182, 182, 182],\n",
       "         [182, 182, 182],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[183, 183, 183],\n",
       "         [185, 185, 185],\n",
       "         [183, 183, 183],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[183, 183, 183],\n",
       "         [185, 185, 185],\n",
       "         [183, 183, 183],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3868780000620973, 'inference': 6.00639999993291, 'postprocess': 1.3257699999940087},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[182, 182, 182],\n",
       "         [182, 182, 182],\n",
       "         [182, 182, 182],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[182, 182, 182],\n",
       "         [182, 182, 182],\n",
       "         [183, 183, 183],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[182, 182, 182],\n",
       "         [183, 183, 183],\n",
       "         [185, 185, 185],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.060759999925722, 'inference': 7.281138999928771, 'postprocess': 1.5048609999439577},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[185, 185, 185],\n",
       "         [186, 186, 186],\n",
       "         [186, 186, 186],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[185, 185, 185],\n",
       "         [186, 186, 186],\n",
       "         [186, 186, 186],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[185, 185, 185],\n",
       "         [186, 186, 186],\n",
       "         [186, 186, 186],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.863457999978891, 'inference': 7.090509000022394, 'postprocess': 1.1678619999884177},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[193, 193, 193],\n",
       "         [193, 193, 193],\n",
       "         [193, 193, 193],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[194, 194, 194],\n",
       "         [194, 194, 194],\n",
       "         [194, 194, 194],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[194, 194, 194],\n",
       "         [194, 194, 194],\n",
       "         [194, 194, 194],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.547539000010147, 'inference': 6.683561999921039, 'postprocess': 1.2977280000541214},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[195, 195, 195],\n",
       "         [196, 196, 196],\n",
       "         [196, 196, 196],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[195, 195, 195],\n",
       "         [196, 196, 196],\n",
       "         [196, 196, 196],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[195, 195, 195],\n",
       "         [196, 196, 196],\n",
       "         [196, 196, 196],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8831770000815595, 'inference': 7.975331000011465, 'postprocess': 1.236587999983385},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[197, 197, 197],\n",
       "         [199, 199, 199],\n",
       "         [199, 199, 199],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[197, 197, 197],\n",
       "         [199, 199, 199],\n",
       "         [199, 199, 199],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[199, 199, 199],\n",
       "         [199, 199, 199],\n",
       "         [199, 199, 199],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4536249999537176, 'inference': 7.079305000047498, 'postprocess': 1.2272010000060618},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 200, 200],\n",
       "         [200, 200, 200],\n",
       "         [200, 200, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[201, 201, 201],\n",
       "         [201, 201, 201],\n",
       "         [200, 200, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[201, 201, 201],\n",
       "         [201, 201, 201],\n",
       "         [201, 201, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.6251909999928102, 'inference': 5.943203000015274, 'postprocess': 1.132545000018581},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[200, 200, 200],\n",
       "         [200, 200, 200],\n",
       "         [200, 200, 200],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[201, 201, 201],\n",
       "         [201, 201, 201],\n",
       "         [201, 201, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[201, 201, 201],\n",
       "         [201, 201, 201],\n",
       "         [201, 201, 201],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.437956000017948, 'inference': 6.393072000037137, 'postprocess': 1.193763000060244},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[203, 203, 203],\n",
       "         [203, 203, 203],\n",
       "         [203, 203, 203],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [203, 203, 203],\n",
       "         [203, 203, 203],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [203, 203, 203],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 4.079498000010062, 'inference': 8.989178999968317, 'postprocess': 1.259512000046925},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[203, 203, 203],\n",
       "         [203, 203, 203],\n",
       "         [203, 203, 203],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [203, 203, 203],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [203, 203, 203],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.075625999964359, 'inference': 6.363616000044203, 'postprocess': 1.3758759999973336},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.616082999997161, 'inference': 6.843688000003567, 'postprocess': 1.1425470000858695},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [227, 227, 227]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [227, 227, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.7056360000633504, 'inference': 6.569925999997395, 'postprocess': 1.1400669999375168},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [153, 153, 153],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [151, 151, 151],\n",
       "         [151, 151, 151]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.203022999992754, 'inference': 6.231732000060219, 'postprocess': 1.0114719999592126},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [151, 151, 151],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [151, 151, 151],\n",
       "         [151, 151, 151]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3966719999416455, 'inference': 6.749345999992329, 'postprocess': 1.2511070000300606},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.5681719999965935, 'inference': 6.454446000020653, 'postprocess': 1.63962999999967},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3493769999968208, 'inference': 8.353465000027427, 'postprocess': 1.2496560000272439},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 206, 206],\n",
       "         [206, 206, 206],\n",
       "         [206, 206, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[206, 206, 206],\n",
       "         [206, 206, 206],\n",
       "         [206, 206, 206],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[206, 206, 206],\n",
       "         [206, 206, 206],\n",
       "         [206, 206, 206],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.247844999918925, 'inference': 6.267390999937561, 'postprocess': 1.135065999960716},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [143, 143, 143],\n",
       "         [150, 150, 150],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [152, 152, 152],\n",
       "         [151, 151, 151]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9913520000045537, 'inference': 6.47070899992741, 'postprocess': 1.0590740000679943},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [122, 122, 122],\n",
       "         [145, 145, 145],\n",
       "         [157, 157, 157]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [139, 139, 139],\n",
       "         [152, 152, 152],\n",
       "         [156, 156, 156]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [157, 157, 157],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.6362549999703333, 'inference': 6.164928999965014, 'postprocess': 1.898386999982904},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 43,  43,  43],\n",
       "         [ 47,  47,  47],\n",
       "         [ 71,  71,  71]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 88,  88,  88],\n",
       "         [ 51,  51,  51],\n",
       "         [ 62,  62,  62]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 52,  52,  52],\n",
       "         [ 48,  48,  48],\n",
       "         [123, 123, 123]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3586599999371174, 'inference': 7.220566000000872, 'postprocess': 1.716387999977087},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 39,  39,  39],\n",
       "         [ 23,  23,  23],\n",
       "         [ 11,  11,  11]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 73,  73,  73],\n",
       "         [ 46,  46,  46],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 71,  71,  71],\n",
       "         [ 37,  37,  37],\n",
       "         [ 31,  31,  31]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4424249999365202, 'inference': 5.565045000025748, 'postprocess': 1.5239460000202598},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 36,  36,  36],\n",
       "         [ 31,  31,  31]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 54,  54,  54],\n",
       "         [ 46,  46,  46],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 58,  58,  58],\n",
       "         [ 27,  27,  27],\n",
       "         [ 23,  23,  23]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[203, 206, 204],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         [204, 207, 205],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8723890000273968, 'inference': 6.29619599999387, 'postprocess': 1.0895739999341458},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 30,  30,  30],\n",
       "         [ 23,  23,  23],\n",
       "         [ 31,  31,  31]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 38,  38,  38],\n",
       "         [ 53,  53,  53]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 41,  41,  41],\n",
       "         [ 20,  20,  20],\n",
       "         [ 53,  53,  53]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223],\n",
       "         [221, 224, 222]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7599819999531974, 'inference': 6.299391000084142, 'postprocess': 0.9838839999929405},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 25,  25,  25],\n",
       "         [ 31,  31,  31]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 25,  25,  25],\n",
       "         [ 32,  32,  32]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 60,  60,  60],\n",
       "         [ 26,  26,  26],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [222, 225, 223],\n",
       "         [221, 224, 222]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.2563249999620894, 'inference': 5.999316999918847, 'postprocess': 1.1773159999393101},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [ 25,  25,  25],\n",
       "         [ 17,  17,  17],\n",
       "         [ 24,  24,  24]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 20,  20,  20],\n",
       "         [ 22,  22,  22],\n",
       "         [ 32,  32,  32]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 11,  11,  11],\n",
       "         [ 12,  12,  12],\n",
       "         [ 25,  25,  25]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222],\n",
       "         [220, 223, 221]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [220, 223, 221],\n",
       "         [221, 224, 222],\n",
       "         [220, 223, 221]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         ...,\n",
       "         [220, 223, 221],\n",
       "         [220, 223, 221],\n",
       "         [220, 223, 221]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8792609999991328, 'inference': 6.542612000089321, 'postprocess': 1.1755069999708212},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 30,  30,  30],\n",
       "         [ 18,  18,  18],\n",
       "         [ 22,  22,  22]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 29,  29,  29],\n",
       "         [ 22,  22,  22],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 25,  25,  25],\n",
       "         [ 15,  15,  15],\n",
       "         [ 22,  22,  22]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [221, 224, 222],\n",
       "         [221, 224, 222]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9389900000987836, 'inference': 6.300488999954723, 'postprocess': 1.07153900000867},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 32,  32,  32],\n",
       "         [ 17,  17,  17],\n",
       "         [ 18,  18,  18]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 30,  30,  30],\n",
       "         [ 22,  22,  22],\n",
       "         [ 29,  29,  29]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 19,  19,  19],\n",
       "         [ 22,  22,  22]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.8314050000517454, 'inference': 7.23241900004723, 'postprocess': 1.605624999911015},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 55,  55,  55],\n",
       "         [ 29,  29,  29],\n",
       "         [ 17,  17,  17]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 15,  15,  15],\n",
       "         [ 17,  17,  17]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 40,  40,  40],\n",
       "         [ 15,  15,  15],\n",
       "         [ 12,  12,  12]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7213959999935469, 'inference': 5.988797999975759, 'postprocess': 1.1054249999915555},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 73,  73,  73],\n",
       "         [ 44,  44,  44],\n",
       "         [ 19,  19,  19]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 51,  51,  51],\n",
       "         [ 22,  22,  22],\n",
       "         [ 16,  16,  16]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 45,  45,  45],\n",
       "         [ 16,  16,  16],\n",
       "         [ 15,  15,  15]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.556514999947467, 'inference': 6.7410489999701895, 'postprocess': 1.0490150000350695},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 64,  64,  64],\n",
       "         [ 69,  69,  69],\n",
       "         [ 60,  60,  60]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 66,  66,  66],\n",
       "         [ 71,  71,  71],\n",
       "         [ 55,  55,  55]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 68,  68,  68],\n",
       "         [ 59,  59,  59],\n",
       "         [ 37,  37,  37]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2833060000048135, 'inference': 7.7158060000783735, 'postprocess': 1.8892419999474441},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 40,  40,  40],\n",
       "         [ 54,  54,  54],\n",
       "         [ 80,  80,  80]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 41,  41,  41],\n",
       "         [ 57,  57,  57],\n",
       "         [ 71,  71,  71]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 40,  40,  40],\n",
       "         [ 66,  66,  66],\n",
       "         [ 64,  64,  64]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [227, 227, 227],\n",
       "         [227, 227, 227]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3862180001069646, 'inference': 7.175394000000779, 'postprocess': 1.6509679999217042},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 23,  23,  23],\n",
       "         [ 40,  40,  40],\n",
       "         [ 54,  54,  54]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 24,  24,  24],\n",
       "         [ 41,  41,  41],\n",
       "         [ 54,  54,  54]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 29,  29,  29],\n",
       "         [ 41,  41,  41],\n",
       "         [ 59,  59,  59]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 55,  55,  55],\n",
       "         [ 58,  58,  58],\n",
       "         [136, 136, 136]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 64,  64,  64],\n",
       "         [ 85,  85,  85],\n",
       "         [182, 182, 182]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 79,  79,  79],\n",
       "         [101, 101, 101],\n",
       "         [196, 196, 196]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.263919000029091, 'inference': 6.285862000027009, 'postprocess': 1.0220750000371481},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 33,  33,  33],\n",
       "         [ 17,  17,  17],\n",
       "         [ 34,  34,  34]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 33,  33,  33],\n",
       "         [ 17,  17,  17],\n",
       "         [ 34,  34,  34]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 33,  33,  33],\n",
       "         [ 17,  17,  17],\n",
       "         [ 34,  34,  34]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 37,  37,  37],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 38,  38,  38],\n",
       "         [ 44,  44,  44]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 38,  38,  38],\n",
       "         [ 39,  39,  39],\n",
       "         [ 43,  43,  43]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.947071000017786, 'inference': 6.8874309999955585, 'postprocess': 1.2066079999613066},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 24,  24,  24],\n",
       "         [ 24,  24,  24]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 32,  32,  32],\n",
       "         [ 25,  25,  25],\n",
       "         [ 25,  25,  25]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 23,  23,  23],\n",
       "         [ 23,  23,  23]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 39,  39,  39],\n",
       "         [ 38,  38,  38],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 38,  38,  38],\n",
       "         [ 37,  37,  37],\n",
       "         [ 37,  37,  37]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 38,  38,  38],\n",
       "         [ 36,  36,  36],\n",
       "         [ 34,  34,  34]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.688823999984379, 'inference': 7.190798000010545, 'postprocess': 1.0474509999767179},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 29,  29,  29],\n",
       "         [ 26,  26,  26],\n",
       "         [ 23,  23,  23]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [ 29,  29,  29],\n",
       "         [ 26,  26,  26],\n",
       "         [ 24,  24,  24]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 29,  29,  29],\n",
       "         [ 25,  25,  25],\n",
       "         [ 22,  22,  22]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 53,  53,  53],\n",
       "         [ 48,  48,  48],\n",
       "         [ 40,  40,  40]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 51,  51,  51],\n",
       "         [ 52,  52,  52],\n",
       "         [ 41,  41,  41]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 47,  47,  47],\n",
       "         [ 52,  52,  52],\n",
       "         [ 43,  43,  43]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7266130000734847, 'inference': 6.723176000036801, 'postprocess': 1.0328889999300372},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 51,  51,  51],\n",
       "         [ 26,  26,  26],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 43,  43,  43],\n",
       "         [ 18,  18,  18],\n",
       "         [ 20,  20,  20]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 45,  45,  45],\n",
       "         [ 30,  30,  30],\n",
       "         [ 24,  24,  24]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 59,  59,  59],\n",
       "         [ 64,  64,  64],\n",
       "         [ 71,  71,  71]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 57,  57,  57],\n",
       "         [ 64,  64,  64],\n",
       "         [ 68,  68,  68]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [ 50,  50,  50],\n",
       "         [ 58,  58,  58],\n",
       "         [ 61,  61,  61]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.1698819999992338, 'inference': 6.9026899999471425, 'postprocess': 1.0145549999833747},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [ 79,  79,  79],\n",
       "         [ 10,  10,  10],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 78,  78,  78],\n",
       "         [ 10,  10,  10],\n",
       "         [ 29,  29,  29]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 68,  68,  68],\n",
       "         [ 19,  19,  19],\n",
       "         [ 31,  31,  31]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 41,  41,  41],\n",
       "         [ 51,  51,  51]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 43,  43,  43],\n",
       "         [ 52,  52,  52]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 39,  39,  39],\n",
       "         [ 47,  47,  47]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.5280439999496593, 'inference': 7.023491999916587, 'postprocess': 1.5529060000289974},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [107, 107, 107],\n",
       "         [ 26,  26,  26],\n",
       "         [ 31,  31,  31]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [102, 102, 102],\n",
       "         [ 23,  23,  23],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 82,  82,  82],\n",
       "         [ 19,  19,  19],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [ 38,  38,  38],\n",
       "         [ 40,  40,  40],\n",
       "         [ 41,  41,  41]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [ 38,  38,  38],\n",
       "         [ 40,  40,  40],\n",
       "         [ 43,  43,  43]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 38,  38,  38],\n",
       "         [ 40,  40,  40]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.627178000011554, 'inference': 6.7986519999294615, 'postprocess': 1.105536999943979},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [133, 133, 133],\n",
       "         [ 43,  43,  43],\n",
       "         [ 26,  26,  26]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [126, 126, 126],\n",
       "         [ 33,  33,  33],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [102, 102, 102],\n",
       "         [ 17,  17,  17],\n",
       "         [ 25,  25,  25]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [ 46,  46,  46],\n",
       "         [ 45,  45,  45],\n",
       "         [ 45,  45,  45]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [ 45,  45,  45],\n",
       "         [ 44,  44,  44],\n",
       "         [ 41,  41,  41]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [ 43,  43,  43],\n",
       "         [ 43,  43,  43],\n",
       "         [ 40,  40,  40]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0045210000034785, 'inference': 6.54081999994105, 'postprocess': 1.0769419999405727},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [172, 172, 172],\n",
       "         [107, 107, 107],\n",
       "         [ 34,  34,  34]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [157, 157, 157],\n",
       "         [ 74,  74,  74],\n",
       "         [ 24,  24,  24]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [136, 136, 136],\n",
       "         [ 51,  51,  51],\n",
       "         [ 20,  20,  20]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9597350000140068, 'inference': 6.255393999936132, 'postprocess': 1.1331439999366921},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [179, 179, 179],\n",
       "         [131, 131, 131],\n",
       "         [ 50,  50,  50]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [168, 168, 168],\n",
       "         [102, 102, 102],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [ 69,  69,  69],\n",
       "         [ 23,  23,  23]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5121689999659793, 'inference': 8.066790000043511, 'postprocess': 2.431405000038467},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [173, 173, 173],\n",
       "         [146, 146, 146],\n",
       "         [ 67,  67,  67]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [166, 166, 166],\n",
       "         [128, 128, 128],\n",
       "         [ 58,  58,  58]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [101, 101, 101],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.308146000042143, 'inference': 6.092090000038297, 'postprocess': 0.9981980000475232},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [167, 167, 167],\n",
       "         [163, 163, 163],\n",
       "         [ 83,  83,  83]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [161, 161, 161],\n",
       "         [147, 147, 147],\n",
       "         [ 75,  75,  75]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [128, 128, 128],\n",
       "         [ 57,  57,  57]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.946746000044186, 'inference': 6.240578000074493, 'postprocess': 1.048174999937146},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [159, 159, 159],\n",
       "         [165, 165, 165],\n",
       "         [115, 115, 115]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [161, 161, 161],\n",
       "         [153, 153, 153],\n",
       "         [103, 103, 103]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [158, 158, 158],\n",
       "         [138, 138, 138],\n",
       "         [ 79,  79,  79]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [211, 211, 211],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [211, 211, 211],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [211, 211, 211],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.3309780000081446, 'inference': 6.580384000017148, 'postprocess': 1.4689700000189987},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [138, 138, 138],\n",
       "         [165, 165, 165],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [160, 160, 160],\n",
       "         [143, 143, 143]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [163, 163, 163],\n",
       "         [156, 156, 156],\n",
       "         [118, 118, 118]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.7333439999210896, 'inference': 6.7514030000666025, 'postprocess': 1.1113369999975475},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [130, 130, 130],\n",
       "         [159, 159, 159],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [159, 159, 159],\n",
       "         [151, 151, 151]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [160, 160, 160],\n",
       "         [153, 153, 153],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.750946000105614, 'inference': 6.415056999912849, 'postprocess': 1.4675229999738804},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [121, 121, 121],\n",
       "         [153, 153, 153],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [158, 158, 158],\n",
       "         [158, 158, 158]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [157, 157, 157],\n",
       "         [151, 151, 151],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.432159999922078, 'inference': 6.5688870000712996, 'postprocess': 0.9706839999807926},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [ 89,  89,  89],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [126, 126, 126],\n",
       "         [161, 161, 161],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [151, 151, 151],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.548394999986158, 'inference': 7.100732000026255, 'postprocess': 1.0569339999619842},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 67,  67,  67],\n",
       "         [118, 118, 118],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 81,  81,  81],\n",
       "         [147, 147, 147],\n",
       "         [154, 154, 154]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [111, 111, 111],\n",
       "         [158, 158, 158],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.5803400000086185, 'inference': 6.442765000088002, 'postprocess': 1.616814000044542},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [103, 103, 103],\n",
       "         [ 72,  72,  72],\n",
       "         [117, 117, 117]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 86,  86,  86],\n",
       "         [ 81,  81,  81],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 66,  66,  66],\n",
       "         [105, 105, 105],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5050289999862798, 'inference': 6.182806999959212, 'postprocess': 1.178573999936816},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [ 90,  90,  90],\n",
       "         [ 82,  82,  82]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [123, 123, 123],\n",
       "         [ 78,  78,  78],\n",
       "         [ 97,  97,  97]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 94,  94,  94],\n",
       "         [ 71,  71,  71],\n",
       "         [121, 121, 121]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3030620000336057, 'inference': 6.41995900002712, 'postprocess': 1.1147230000005948},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [102, 102, 102],\n",
       "         [ 75,  75,  75]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [107, 107, 107],\n",
       "         [ 81,  81,  81]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [101, 101, 101],\n",
       "         [ 85,  85,  85]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.195971000015561, 'inference': 6.401629000038156, 'postprocess': 1.45653900005982},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [132, 132, 132],\n",
       "         [ 93,  93,  93]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [133, 133, 133],\n",
       "         [ 97,  97,  97]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [135, 135, 135],\n",
       "         [ 98,  98,  98]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.225213999987318, 'inference': 6.5628309999965495, 'postprocess': 1.0592630000019199},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [149, 149, 149],\n",
       "         [132, 132, 132]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [149, 149, 149],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [150, 150, 150],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9164699999691948, 'inference': 6.712510000056682, 'postprocess': 1.0710750000271219},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3758620000317023, 'inference': 6.859721999944668, 'postprocess': 1.0588449999886507},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2320969999564113, 'inference': 6.4362800000026255, 'postprocess': 1.0646460000316438},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6595940001025156, 'inference': 6.106113999976515, 'postprocess': 0.9883090000357697},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6421959999206592, 'inference': 5.796402999976635, 'postprocess': 0.9669490000305814},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3960530000076687, 'inference': 5.280237999954807, 'postprocess': 1.083255000025929},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3785849999976563, 'inference': 6.287946000043121, 'postprocess': 1.0055890001012813},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.30904599996029, 'inference': 6.0440280000193525, 'postprocess': 0.9497969999756606},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3718249999783438, 'inference': 6.296592000012424, 'postprocess': 0.9707629999411438},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4601760000232389, 'inference': 6.433786000002328, 'postprocess': 1.0523999999350053},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5182270000195786, 'inference': 5.808291999983339, 'postprocess': 1.756227999976545},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.449118999971688, 'inference': 6.7200270000284945, 'postprocess': 1.3330219999261317},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6434970000318572, 'inference': 5.475559000046815, 'postprocess': 0.9654799999907482},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4472870000190596, 'inference': 5.868713000040771, 'postprocess': 1.0238570000637992},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.5600110000141285, 'inference': 6.786529000009978, 'postprocess': 1.4044810000086727},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.8878079999685724, 'inference': 6.065487000000758, 'postprocess': 1.0983250000435874},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [150, 150, 150],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0021670000005543, 'inference': 7.151690999990024, 'postprocess': 1.8118570000069667},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [151, 151, 151]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [150, 150, 150],\n",
       "         [143, 143, 143]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2060650000194073, 'inference': 5.691835999982686, 'postprocess': 0.9481949999781136},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [151, 151, 151],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [150, 150, 150],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2510809999639605, 'inference': 6.304819999968458, 'postprocess': 1.8097509999961403},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [154, 154, 154],\n",
       "         [152, 152, 152]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [154, 154, 154],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         [128, 128, 128]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3356680000242704, 'inference': 5.82326500000363, 'postprocess': 1.000960000055784},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [157, 157, 157],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [158, 158, 158],\n",
       "         [143, 143, 143]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [156, 156, 156],\n",
       "         [125, 125, 125]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2101849999908154, 'inference': 5.733572000053755, 'postprocess': 0.9947610000153873},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [157, 157, 157],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [156, 156, 156],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [157, 157, 157],\n",
       "         [147, 147, 147],\n",
       "         [111, 111, 111]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2269659999901705, 'inference': 6.030941999938477, 'postprocess': 0.9315290000131426},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [156, 156, 156],\n",
       "         [143, 143, 143]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [157, 157, 157],\n",
       "         [152, 152, 152],\n",
       "         [129, 129, 129]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [158, 158, 158],\n",
       "         [139, 139, 139],\n",
       "         [103, 103, 103]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.46057999995719, 'inference': 5.246129000056499, 'postprocess': 0.9176339999612537},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [152, 152, 152],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [157, 157, 157],\n",
       "         [145, 145, 145],\n",
       "         [ 98,  98,  98]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230],\n",
       "         [230, 230, 230]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2638649999416884, 'inference': 5.800217000000885, 'postprocess': 0.9379200000694254},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [153, 153, 153],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [152, 152, 152],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [157, 157, 157],\n",
       "         [145, 145, 145],\n",
       "         [ 98,  98,  98]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.415526000026148, 'inference': 10.924190999958228, 'postprocess': 1.6749760000038805},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [137, 137, 137]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [153, 153, 153],\n",
       "         [118, 118, 118]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [157, 157, 157],\n",
       "         [136, 136, 136],\n",
       "         [ 75,  75,  75]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2285239999982878, 'inference': 5.6619800000135, 'postprocess': 1.0048499999584237},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [150, 150, 150],\n",
       "         [135, 135, 135]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [154, 154, 154],\n",
       "         [105, 105, 105]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [126, 126, 126],\n",
       "         [ 52,  52,  52]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.152386000034312, 'inference': 8.960429000012482, 'postprocess': 1.2971919999245074},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [151, 151, 151],\n",
       "         [ 96,  96,  96]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [118, 118, 118],\n",
       "         [ 46,  46,  46]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.8206320000663254, 'inference': 7.349844000032135, 'postprocess': 1.113991000011083},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [152, 152, 152],\n",
       "         [100, 100, 100]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [121, 121, 121],\n",
       "         [ 47,  47,  47]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9004609999910826, 'inference': 6.057262000012997, 'postprocess': 1.0209580000264395},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [131, 131, 131]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [152, 152, 152],\n",
       "         [ 96,  96,  96]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [118, 118, 118],\n",
       "         [ 45,  45,  45]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2743199999931676, 'inference': 6.098769999994147, 'postprocess': 0.9847639998952218},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [152, 152, 152],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [151, 151, 151],\n",
       "         [ 93,  93,  93]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [115, 115, 115],\n",
       "         [ 41,  41,  41]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4432770000212258, 'inference': 5.9999400000378955, 'postprocess': 1.122063000025264},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [151, 151, 151],\n",
       "         [129, 129, 129]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [150, 150, 150],\n",
       "         [ 91,  91,  91]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [114, 114, 114],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7826609999929133, 'inference': 5.583832000070288, 'postprocess': 1.0817270000416102},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [151, 151, 151],\n",
       "         [129, 129, 129]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [154, 154, 154],\n",
       "         [150, 150, 150],\n",
       "         [ 91,  91,  91]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [114, 114, 114],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7412060000197016, 'inference': 5.720256000017798, 'postprocess': 1.0526329999720474},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [157, 157, 157],\n",
       "         [116, 116, 116]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [161, 161, 161],\n",
       "         [150, 150, 150],\n",
       "         [ 88,  88,  88]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [156, 156, 156],\n",
       "         [117, 117, 117],\n",
       "         [ 45,  45,  45]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4463179999211206, 'inference': 6.393638000076862, 'postprocess': 1.015994999988834},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [159, 159, 159],\n",
       "         [154, 154, 154],\n",
       "         [ 88,  88,  88]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [166, 166, 166],\n",
       "         [142, 142, 142],\n",
       "         [ 61,  61,  61]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [153, 153, 153],\n",
       "         [101, 101, 101],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4250430000402048, 'inference': 5.275945999983378, 'postprocess': 0.9253470000203379},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [159, 159, 159],\n",
       "         [146, 146, 146],\n",
       "         [ 69,  69,  69]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [166, 166, 166],\n",
       "         [125, 125, 125],\n",
       "         [ 44,  44,  44]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [ 79,  79,  79],\n",
       "         [ 19,  19,  19]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4290899999878093, 'inference': 5.58213599992996, 'postprocess': 1.0876350000899038},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [164, 164, 164],\n",
       "         [ 95,  95,  95],\n",
       "         [ 47,  47,  47]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [ 57,  57,  57],\n",
       "         [ 17,  17,  17]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [122, 122, 122],\n",
       "         [ 38,  38,  38],\n",
       "         [ 18,  18,  18]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3132519999317083, 'inference': 5.6533489999992526, 'postprocess': 1.4757069999404848},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [137, 137, 137],\n",
       "         [ 53,  53,  53],\n",
       "         [ 55,  55,  55]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 93,  93,  93],\n",
       "         [ 15,  15,  15],\n",
       "         [ 18,  18,  18]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 71,  71,  71],\n",
       "         [ 27,  27,  27],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4330100000279344, 'inference': 5.497569999988627, 'postprocess': 1.0121120000121664},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         [131, 131, 131],\n",
       "         ...,\n",
       "         [115, 115, 115],\n",
       "         [ 50,  50,  50],\n",
       "         [ 58,  58,  58]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 68,  68,  68],\n",
       "         [ 12,  12,  12],\n",
       "         [ 19,  19,  19]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 55,  55,  55],\n",
       "         [ 26,  26,  26],\n",
       "         [ 31,  31,  31]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4186820000077205, 'inference': 5.8833970000478075, 'postprocess': 1.4667789999975867},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 57,  57,  57],\n",
       "         [ 62,  62,  62],\n",
       "         [ 76,  76,  76]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 16,  16,  16],\n",
       "         [ 22,  22,  22],\n",
       "         [ 29,  29,  29]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 23,  23,  23],\n",
       "         [ 27,  27,  27],\n",
       "         [ 19,  19,  19]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7119820000743857, 'inference': 6.464469000093231, 'postprocess': 1.064166999981353},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 48,  48,  48],\n",
       "         [ 73,  73,  73],\n",
       "         [ 78,  78,  78]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 11,  11,  11],\n",
       "         [ 29,  29,  29],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 24,  24,  24],\n",
       "         [ 25,  25,  25],\n",
       "         [ 18,  18,  18]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8784190000360468, 'inference': 6.3783860000512504, 'postprocess': 1.0468739999396348},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 60,  60,  60],\n",
       "         [ 71,  71,  71],\n",
       "         [ 80,  80,  80]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 39,  39,  39],\n",
       "         [ 55,  55,  55]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 19,  19,  19],\n",
       "         [ 18,  18,  18],\n",
       "         [ 32,  32,  32]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4838999999255975, 'inference': 6.237571000042408, 'postprocess': 1.4125120000016977},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 52,  52,  52],\n",
       "         [ 72,  72,  72],\n",
       "         [ 91,  91,  91]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 26,  26,  26],\n",
       "         [ 47,  47,  47],\n",
       "         [ 83,  83,  83]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 17,  17,  17],\n",
       "         [ 25,  25,  25],\n",
       "         [ 61,  61,  61]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7571050000242394, 'inference': 6.724891999965621, 'postprocess': 1.0279739999532467},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 55,  55,  55],\n",
       "         [ 78,  78,  78],\n",
       "         [ 94,  94,  94]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 17,  17,  17],\n",
       "         [ 55,  55,  55],\n",
       "         [ 93,  93,  93]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 26,  26,  26],\n",
       "         [ 40,  40,  40],\n",
       "         [ 69,  69,  69]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4627169999812395, 'inference': 5.264478000071904, 'postprocess': 1.1036409999860552},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 50,  50,  50],\n",
       "         [ 86,  86,  86],\n",
       "         [ 98,  98,  98]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 26,  26,  26],\n",
       "         [ 67,  67,  67],\n",
       "         [ 86,  86,  86]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 30,  30,  30],\n",
       "         [ 52,  52,  52],\n",
       "         [ 66,  66,  66]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.431233999937831, 'inference': 5.562619999977869, 'postprocess': 1.0672680000425316},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 48,  48,  48],\n",
       "         [ 86,  86,  86],\n",
       "         [ 95,  95,  95]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 75,  75,  75],\n",
       "         [ 85,  85,  85]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 32,  32,  32],\n",
       "         [ 68,  68,  68],\n",
       "         [ 76,  76,  76]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4434729999948104, 'inference': 6.252756999970188, 'postprocess': 1.0228249999499894},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 75,  75,  75],\n",
       "         [ 95,  95,  95],\n",
       "         [ 82,  82,  82]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 71,  71,  71],\n",
       "         [ 90,  90,  90],\n",
       "         [ 76,  76,  76]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 68,  68,  68],\n",
       "         [ 89,  89,  89],\n",
       "         [ 76,  76,  76]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2170469999546185, 'inference': 5.823362999990422, 'postprocess': 1.003358000048138},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 90,  90,  90],\n",
       "         [ 88,  88,  88],\n",
       "         [ 78,  78,  78]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 85,  85,  85],\n",
       "         [ 82,  82,  82],\n",
       "         [ 72,  72,  72]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 85,  85,  85],\n",
       "         [ 82,  82,  82],\n",
       "         [ 72,  72,  72]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5801179999925807, 'inference': 6.296947999999247, 'postprocess': 1.0288440000749688},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 95,  95,  95],\n",
       "         [ 86,  86,  86],\n",
       "         [ 78,  78,  78]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 90,  90,  90],\n",
       "         [ 81,  81,  81],\n",
       "         [ 72,  72,  72]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 88,  88,  88],\n",
       "         [ 81,  81,  81],\n",
       "         [ 72,  72,  72]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3328630000160047, 'inference': 6.902447000015854, 'postprocess': 0.9783349998997437},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 95,  95,  95],\n",
       "         [ 81,  81,  81],\n",
       "         [ 76,  76,  76]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 89,  89,  89],\n",
       "         [ 75,  75,  75],\n",
       "         [ 71,  71,  71]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 89,  89,  89],\n",
       "         [ 75,  75,  75],\n",
       "         [ 71,  71,  71]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.523472000007132, 'inference': 9.273204000010082, 'postprocess': 1.4478980000376396},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 95,  95,  95],\n",
       "         [ 81,  81,  81],\n",
       "         [ 76,  76,  76]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 89,  89,  89],\n",
       "         [ 75,  75,  75],\n",
       "         [ 71,  71,  71]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 89,  89,  89],\n",
       "         [ 75,  75,  75],\n",
       "         [ 71,  71,  71]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3948600000048827, 'inference': 5.30394999998407, 'postprocess': 0.9532609999496344},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 82,  82,  82],\n",
       "         [ 94,  94,  94],\n",
       "         [ 81,  81,  81]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 83,  83,  83],\n",
       "         [ 88,  88,  88],\n",
       "         [ 74,  74,  74]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 75,  75,  75],\n",
       "         [ 80,  80,  80],\n",
       "         [ 64,  64,  64]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.753063000023758, 'inference': 6.801207999956205, 'postprocess': 1.140763999956107},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [ 55,  55,  55],\n",
       "         [ 96,  96,  96],\n",
       "         [ 88,  88,  88]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 57,  57,  57],\n",
       "         [ 95,  95,  95],\n",
       "         [ 82,  82,  82]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [ 52,  52,  52],\n",
       "         [ 88,  88,  88],\n",
       "         [ 72,  72,  72]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.718295999988186, 'inference': 6.033043999991605, 'postprocess': 1.1637630000222998},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 23,  23,  23],\n",
       "         [ 64,  64,  64],\n",
       "         [ 96,  96,  96]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 30,  30,  30],\n",
       "         [ 62,  62,  62],\n",
       "         [ 90,  90,  90]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 68,  68,  68],\n",
       "         [ 87,  87,  87]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.512509000008322, 'inference': 5.767288000015469, 'postprocess': 0.9882299999617317},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[122, 122, 122],\n",
       "         [145, 145, 145],\n",
       "         [143, 143, 143],\n",
       "         ...,\n",
       "         [ 20,  20,  20],\n",
       "         [ 36,  36,  36],\n",
       "         [ 76,  76,  76]],\n",
       " \n",
       "        [[ 60,  60,  60],\n",
       "         [132, 132, 132],\n",
       "         [145, 145, 145],\n",
       "         ...,\n",
       "         [ 26,  26,  26],\n",
       "         [ 37,  37,  37],\n",
       "         [ 74,  74,  74]],\n",
       " \n",
       "        [[ 18,  18,  18],\n",
       "         [ 91,  91,  91],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [ 32,  32,  32],\n",
       "         [ 44,  44,  44],\n",
       "         [ 71,  71,  71]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.387095000040972, 'inference': 6.140922999975373, 'postprocess': 1.0127280000915562},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[  3,   3,   3],\n",
       "         [ 90,  90,  90],\n",
       "         [149, 149, 149],\n",
       "         ...,\n",
       "         [ 17,  17,  17],\n",
       "         [  6,   6,   6],\n",
       "         [ 55,  55,  55]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [ 29,  29,  29],\n",
       "         [118, 118, 118],\n",
       "         ...,\n",
       "         [ 23,  23,  23],\n",
       "         [ 11,  11,  11],\n",
       "         [ 57,  57,  57]],\n",
       " \n",
       "        [[ 11,  11,  11],\n",
       "         [  5,   5,   5],\n",
       "         [ 48,  48,  48],\n",
       "         ...,\n",
       "         [ 27,  27,  27],\n",
       "         [ 19,  19,  19],\n",
       "         [ 54,  54,  54]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2590589999490476, 'inference': 5.756749000056516, 'postprocess': 0.9986300000264237},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[ 16,  16,  16],\n",
       "         [ 16,  16,  16],\n",
       "         [ 16,  16,  16],\n",
       "         ...,\n",
       "         [ 34,  34,  34],\n",
       "         [ 12,  12,  12],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[ 16,  16,  16],\n",
       "         [ 17,  17,  17],\n",
       "         [ 16,  16,  16],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 20,  20,  20],\n",
       "         [ 12,  12,  12]],\n",
       " \n",
       "        [[ 17,  17,  17],\n",
       "         [ 16,  16,  16],\n",
       "         [ 16,  16,  16],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 26,  26,  26],\n",
       "         [ 27,  27,  27]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.352152000094975, 'inference': 5.664112999966164, 'postprocess': 0.990381999940837},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[ 20,  20,  20],\n",
       "         [ 17,  17,  17],\n",
       "         [ 16,  16,  16],\n",
       "         ...,\n",
       "         [ 34,  34,  34],\n",
       "         [ 12,  12,  12],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[ 20,  20,  20],\n",
       "         [ 13,  13,  13],\n",
       "         [ 15,  15,  15],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 20,  20,  20],\n",
       "         [ 12,  12,  12]],\n",
       " \n",
       "        [[ 37,  37,  37],\n",
       "         [ 23,  23,  23],\n",
       "         [ 19,  19,  19],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 26,  26,  26],\n",
       "         [ 27,  27,  27]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6841679999970438, 'inference': 5.8621470000161935, 'postprocess': 2.0694140000614425},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[ 22,  22,  22],\n",
       "         [ 15,  15,  15],\n",
       "         [ 15,  15,  15],\n",
       "         ...,\n",
       "         [ 32,  32,  32],\n",
       "         [ 23,  23,  23],\n",
       "         [ 16,  16,  16]],\n",
       " \n",
       "        [[ 27,  27,  27],\n",
       "         [ 23,  23,  23],\n",
       "         [ 24,  24,  24],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 30,  30,  30],\n",
       "         [ 25,  25,  25]],\n",
       " \n",
       "        [[ 29,  29,  29],\n",
       "         [ 31,  31,  31],\n",
       "         [ 31,  31,  31],\n",
       "         ...,\n",
       "         [ 38,  38,  38],\n",
       "         [ 34,  34,  34],\n",
       "         [ 33,  33,  33]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2380680000205757, 'inference': 6.2507310000228244, 'postprocess': 0.9317620000501847},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[ 23,  23,  23],\n",
       "         [ 13,  13,  13],\n",
       "         [ 17,  17,  17],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 25,  25,  25],\n",
       "         [ 27,  27,  27]],\n",
       " \n",
       "        [[ 71,  71,  71],\n",
       "         [ 25,  25,  25],\n",
       "         [  4,   4,   4],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 32,  32,  32],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [ 80,  80,  80],\n",
       "         [ 40,  40,  40],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 38,  38,  38],\n",
       "         [ 40,  40,  40]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.5866580000410977, 'inference': 5.983964000051856, 'postprocess': 0.9659560000727652},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[ 97,  97,  97],\n",
       "         [108, 108, 108],\n",
       "         [ 71,  71,  71],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 38,  38,  38],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [144, 144, 144],\n",
       "         [117, 117, 117],\n",
       "         ...,\n",
       "         [ 34,  34,  34],\n",
       "         [ 38,  38,  38],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        [[153, 153, 153],\n",
       "         [156, 156, 156],\n",
       "         [150, 150, 150],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 33,  33,  33],\n",
       "         [ 32,  32,  32]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.1567380000533376, 'inference': 5.560792999972364, 'postprocess': 1.0070590000168522},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 37,  37,  37],\n",
       "         [ 25,  25,  25]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 37,  37,  37],\n",
       "         [ 32,  32,  32]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 34,  34,  34],\n",
       "         [ 36,  36,  36],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.143445999990945, 'inference': 6.376792999958525, 'postprocess': 0.9683780000386832},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [ 34,  34,  34],\n",
       "         [ 38,  38,  38],\n",
       "         [ 34,  34,  34]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 33,  33,  33],\n",
       "         [ 38,  38,  38],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 33,  33,  33],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.076965000014752, 'inference': 6.299452000007477, 'postprocess': 0.963806000072509},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [ 34,  34,  34],\n",
       "         [ 34,  34,  34],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 32,  32,  32],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 29,  29,  29],\n",
       "         [ 27,  27,  27],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7910450000044875, 'inference': 5.897505999996611, 'postprocess': 0.9473500000467538},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [128, 128, 128],\n",
       "         ...,\n",
       "         [ 33,  33,  33],\n",
       "         [ 33,  33,  33],\n",
       "         [ 33,  33,  33]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 30,  30,  30],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 27,  27,  27],\n",
       "         [ 27,  27,  27]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7007879999937359, 'inference': 5.828348000022743, 'postprocess': 0.9813310000481579},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         [130, 130, 130],\n",
       "         ...,\n",
       "         [ 30,  30,  30],\n",
       "         [ 33,  33,  33],\n",
       "         [ 33,  33,  33]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 32,  32,  32],\n",
       "         [ 31,  31,  31],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [ 33,  33,  33],\n",
       "         [ 31,  31,  31],\n",
       "         [ 27,  27,  27]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4342949999672783, 'inference': 5.843201999937264, 'postprocess': 0.9524890000420783},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         [129, 129, 129],\n",
       "         ...,\n",
       "         [ 34,  34,  34],\n",
       "         [ 31,  31,  31],\n",
       "         [ 30,  30,  30]],\n",
       " \n",
       "        [[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 37,  37,  37],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 38,  38,  38],\n",
       "         [ 37,  37,  37],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.268142000048101, 'inference': 6.1550389999638355, 'postprocess': 1.0156099999676371},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 44,  44,  44],\n",
       "         [ 33,  33,  33],\n",
       "         [ 27,  27,  27]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 44,  44,  44],\n",
       "         [ 45,  45,  45],\n",
       "         [ 43,  43,  43]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 43,  43,  43],\n",
       "         [ 44,  44,  44],\n",
       "         [ 43,  43,  43]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4980060000198137, 'inference': 6.72106399997574, 'postprocess': 1.0402309999335557},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [131, 131, 131],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [ 53,  53,  53],\n",
       "         [ 43,  43,  43],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 41,  41,  41],\n",
       "         [ 43,  43,  43],\n",
       "         [ 47,  47,  47]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [ 29,  29,  29],\n",
       "         [ 41,  41,  41],\n",
       "         [ 41,  41,  41]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4257530000350016, 'inference': 5.634895999946821, 'postprocess': 1.0557210000570194},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [131, 131, 131],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [ 68,  68,  68],\n",
       "         [ 29,  29,  29],\n",
       "         [ 53,  53,  53]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [132, 132, 132],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [ 76,  76,  76],\n",
       "         [ 27,  27,  27],\n",
       "         [ 47,  47,  47]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [ 91,  91,  91],\n",
       "         [ 26,  26,  26],\n",
       "         [ 34,  34,  34]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[204, 207, 205],\n",
       "         [205, 208, 206],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.3276590000023134, 'inference': 5.952502000013737, 'postprocess': 0.9826629999452052},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[129, 129, 129],\n",
       "         [131, 131, 131],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [ 47,  47,  47],\n",
       "         [ 37,  37,  37]],\n",
       " \n",
       "        [[131, 131, 131],\n",
       "         [132, 132, 132],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [ 59,  59,  59],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [ 72,  72,  72],\n",
       "         [ 33,  33,  33]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.049422999993112, 'inference': 5.832087999920077, 'postprocess': 1.2657969999736451},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [159, 159, 159],\n",
       "         [140, 140, 140],\n",
       "         [ 81,  81,  81]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149],\n",
       "         [104, 104, 104]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [153, 153, 153],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2685949999422519, 'inference': 5.9608479999724295, 'postprocess': 1.19575700000496},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4436860000159868, 'inference': 5.777601000090726, 'postprocess': 0.9571350000214807},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         [132, 132, 132],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4321630000040386, 'inference': 5.642636999937167, 'postprocess': 1.3525020000315635},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.230663999990611, 'inference': 5.742042000065339, 'postprocess': 0.9998010000344948},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         [133, 133, 133],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.1213770000940713, 'inference': 6.639801999995143, 'postprocess': 1.0586820000071384},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]],\n",
       " \n",
       "        [[207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9380939999109614, 'inference': 6.061778999992384, 'postprocess': 1.3167869999506365},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [129, 129, 129]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [153, 153, 153],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.1898849999170125, 'inference': 5.5652430000918685, 'postprocess': 0.9629099999983737},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [129, 129, 129]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [153, 153, 153],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.658030000044164, 'inference': 5.639804999987064, 'postprocess': 0.9848649999639747},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [227, 230, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.1258699999625605, 'inference': 6.335934999924575, 'postprocess': 1.0159509999994043},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [223, 229, 224],\n",
       "         [223, 229, 224],\n",
       "         [223, 229, 224]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 228, 223],\n",
       "         [223, 229, 224],\n",
       "         [223, 229, 224]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 228, 223],\n",
       "         [223, 229, 224],\n",
       "         [224, 230, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.508354999918083, 'inference': 6.393613999989611, 'postprocess': 1.0407180000129301},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [133, 133, 133],\n",
       "         [135, 135, 135],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5827880000642836, 'inference': 5.73454599998513, 'postprocess': 1.2242940000533054},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9653690000041024, 'inference': 5.55758899997727, 'postprocess': 1.0320780000938612},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2212630000476565, 'inference': 5.689085999961208, 'postprocess': 1.0064740000643724},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4567489999990357, 'inference': 5.971097000042391, 'postprocess': 1.0875320000423017},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 135, 135],\n",
       "         [136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2443950000715631, 'inference': 6.233848999954716, 'postprocess': 1.2075710000090112},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 136, 136],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5031220000309986, 'inference': 5.809863999957088, 'postprocess': 1.1821740000641512},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.895935999982612, 'inference': 5.805163999980323, 'postprocess': 1.0786559998905432},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4557310000782309, 'inference': 5.661128000042481, 'postprocess': 0.997501000028933},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9016570000758293, 'inference': 6.010987999957251, 'postprocess': 1.2408149999600937},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4108189999433307, 'inference': 6.156078999993042, 'postprocess': 1.2300549999508803},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [220, 223, 221],\n",
       "         [220, 223, 221],\n",
       "         [220, 223, 221]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [220, 223, 221],\n",
       "         [220, 223, 221],\n",
       "         [220, 223, 221]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [220, 223, 221],\n",
       "         [220, 223, 221],\n",
       "         [220, 223, 221]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.4404330000370464, 'inference': 11.879807000013898, 'postprocess': 1.663855000060721},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9947910000155389, 'inference': 5.445567000037954, 'postprocess': 0.9278450000920202},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4210919999868565, 'inference': 5.494698999996217, 'postprocess': 0.9996600000476974},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2777549999327675, 'inference': 5.9274299999287905, 'postprocess': 0.9998179999683998},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.472415999955956, 'inference': 5.743527000049653, 'postprocess': 0.9956110000075569},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         [205, 208, 206],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5193410000620133, 'inference': 5.525841000007858, 'postprocess': 0.9779939999816634},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [223, 223, 223],\n",
       "         [223, 223, 223],\n",
       "         [222, 222, 222]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [223, 223, 223],\n",
       "         [222, 222, 222],\n",
       "         [223, 223, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [223, 223, 223],\n",
       "         [224, 224, 224]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.582166999924084, 'inference': 5.583890000025349, 'postprocess': 0.9472640000467436},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [221, 221, 221],\n",
       "         [221, 221, 221],\n",
       "         [221, 221, 221]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [221, 221, 221],\n",
       "         [221, 221, 221],\n",
       "         [221, 221, 221]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [221, 221, 221],\n",
       "         [221, 221, 221],\n",
       "         [221, 221, 221]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2484850000191727, 'inference': 6.320221999999376, 'postprocess': 1.1895989999857193},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [217, 217, 217],\n",
       "         [217, 217, 217]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [217, 217, 217],\n",
       "         [217, 217, 217]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216],\n",
       "         [217, 217, 217]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6155409999782933, 'inference': 5.837475000021186, 'postprocess': 0.9901790000412802},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [217, 217, 217],\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.4572230000785567, 'inference': 6.141705999993974, 'postprocess': 1.137076999953024},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [214, 214, 214],\n",
       "         [215, 215, 215],\n",
       "         [214, 214, 214]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [214, 214, 214],\n",
       "         [215, 215, 215],\n",
       "         [216, 216, 216]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [217, 217, 217],\n",
       "         [216, 216, 216]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.2004249999554304, 'inference': 5.821315000048344, 'postprocess': 1.1264259999279602},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [208, 208, 208],\n",
       "         [208, 208, 208]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.5916899999174348, 'inference': 6.75656800001434, 'postprocess': 1.2200900000607362},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [208, 208, 208]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4674880000029589, 'inference': 5.982461000030526, 'postprocess': 0.9917009999753645},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [206, 206, 206],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [206, 206, 206],\n",
       "         [204, 204, 204],\n",
       "         [204, 204, 204]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [206, 206, 206],\n",
       "         [204, 204, 204],\n",
       "         [203, 203, 203]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.4588879999782876, 'inference': 5.548267999984091, 'postprocess': 1.1257969999860507},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [201, 201, 201],\n",
       "         [200, 200, 200],\n",
       "         [199, 199, 199]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [200, 200, 200],\n",
       "         [200, 200, 200],\n",
       "         [197, 197, 197]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [201, 201, 201],\n",
       "         [199, 199, 199],\n",
       "         [199, 199, 199]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4091699999880802, 'inference': 6.463869000072009, 'postprocess': 1.2246789999608154},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [202, 202, 202],\n",
       "         [202, 202, 202],\n",
       "         [201, 201, 201]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [201, 201, 201],\n",
       "         [201, 201, 201],\n",
       "         [199, 199, 199]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [201, 201, 201],\n",
       "         [200, 200, 200],\n",
       "         [199, 199, 199]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4864400000078604, 'inference': 5.854355999986183, 'postprocess': 1.1403270000300836},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [206, 206, 206]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [206, 206, 206],\n",
       "         [206, 206, 206],\n",
       "         [206, 206, 206]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [207, 207, 207],\n",
       "         [207, 207, 207],\n",
       "         [206, 206, 206]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8284709999534243, 'inference': 6.734939000011764, 'postprocess': 1.082562000078724},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[142, 138, 141],\n",
       "         [142, 138, 141],\n",
       "         [141, 137, 140],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [209, 209, 209]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [208, 208, 208]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [208, 208, 208]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2422769999602679, 'inference': 6.071061999932681, 'postprocess': 1.0703179999609347},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [215, 215, 215],\n",
       "         [215, 215, 215],\n",
       "         [215, 215, 215]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [215, 215, 215],\n",
       "         [215, 215, 215],\n",
       "         [215, 215, 215]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [214, 214, 214],\n",
       "         [214, 214, 214],\n",
       "         [214, 214, 214]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3580410000031407, 'inference': 5.643071000008604, 'postprocess': 1.0710310000376921},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145],\n",
       "         [145, 145, 145]],\n",
       " \n",
       "        [[144, 140, 143],\n",
       "         [144, 140, 143],\n",
       "         [142, 138, 141],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216],\n",
       "         [217, 217, 217]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [215, 215, 215],\n",
       "         [216, 216, 216]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2553810000781596, 'inference': 6.593726999994942, 'postprocess': 1.283822000004875},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [218, 218, 218],\n",
       "         [218, 218, 218],\n",
       "         [217, 217, 217]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [217, 217, 217],\n",
       "         [217, 217, 217],\n",
       "         [216, 216, 216]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216],\n",
       "         [216, 216, 216]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.586520000046221, 'inference': 5.89780999996492, 'postprocess': 1.2835139999651801},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [142, 142, 142],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 222, 222],\n",
       "         [222, 222, 222],\n",
       "         [221, 221, 221]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 222, 222],\n",
       "         [222, 222, 222],\n",
       "         [221, 221, 221]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [221, 221, 221],\n",
       "         [221, 221, 221],\n",
       "         [221, 221, 221]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3852490000090256, 'inference': 5.734485999937533, 'postprocess': 1.0591369999701783},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3687279999885504, 'inference': 6.117308999932902, 'postprocess': 1.1446190000015122},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224],\n",
       "         [224, 224, 224]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.215741999999409, 'inference': 5.988516000002164, 'postprocess': 0.9545270000899109},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [137, 137, 137],\n",
       "         [133, 133, 133]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [138, 138, 138],\n",
       "         [139, 139, 139]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [150, 150, 150],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.853630999993584, 'inference': 5.70327000002635, 'postprocess': 1.016440000057628},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [122, 122, 122],\n",
       "         [151, 151, 151],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [125, 125, 125],\n",
       "         [153, 153, 153],\n",
       "         [143, 143, 143]],\n",
       " \n",
       "        [[142, 142, 142],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [135, 135, 135],\n",
       "         [159, 159, 159],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7452839999805292, 'inference': 5.798501999947803, 'postprocess': 1.1969039999257802},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 41,  41,  41],\n",
       "         [ 27,  27,  27],\n",
       "         [110, 110, 110]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 31,  31,  31],\n",
       "         [ 20,  20,  20],\n",
       "         [110, 110, 110]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 38,  38,  38],\n",
       "         [ 19,  19,  19],\n",
       "         [ 81,  81,  81]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0361910000019634, 'inference': 6.489824000027511, 'postprocess': 1.2163810000629383},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 47,  47,  47],\n",
       "         [ 47,  47,  47],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 44,  44,  44],\n",
       "         [ 43,  43,  43],\n",
       "         [ 38,  38,  38]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 44,  44,  44],\n",
       "         [ 44,  44,  44],\n",
       "         [ 31,  31,  31]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225],\n",
       "         [225, 225, 225]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3897369999540388, 'inference': 5.54596099993887, 'postprocess': 1.1413460000539999},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 44,  44,  44],\n",
       "         [ 47,  47,  47]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 44,  44,  44],\n",
       "         [ 47,  47,  47]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 36,  36,  36],\n",
       "         [ 44,  44,  44],\n",
       "         [ 47,  47,  47]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.3510879999548706, 'inference': 5.578968000008899, 'postprocess': 1.2369699999226214},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 37,  37,  37],\n",
       "         [ 39,  39,  39],\n",
       "         [ 48,  48,  48]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 32,  32,  32],\n",
       "         [ 32,  32,  32],\n",
       "         [ 46,  46,  46]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 40,  40,  40],\n",
       "         [ 34,  34,  34],\n",
       "         [ 44,  44,  44]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6193650000104753, 'inference': 5.79422999999224, 'postprocess': 1.006006000011439},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 74,  74,  74],\n",
       "         [ 33,  33,  33],\n",
       "         [ 50,  50,  50]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 71,  71,  71],\n",
       "         [ 37,  37,  37],\n",
       "         [ 51,  51,  51]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [ 67,  67,  67],\n",
       "         [ 20,  20,  20],\n",
       "         [ 43,  43,  43]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2024159999555195, 'inference': 6.742766999991545, 'postprocess': 1.8083899999510322},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 47,  50,  48],\n",
       "         [ 48,  51,  49],\n",
       "         [ 48,  51,  49]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 43,  46,  44],\n",
       "         [ 47,  50,  48],\n",
       "         [ 45,  48,  46]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 82,  85,  83],\n",
       "         [ 58,  61,  59],\n",
       "         [ 48,  51,  49]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.333780999971168, 'inference': 5.9734320000188745, 'postprocess': 1.0062160000643416},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 47,  50,  48],\n",
       "         [ 48,  51,  49],\n",
       "         [ 48,  51,  49]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 47,  50,  48],\n",
       "         [ 48,  51,  49],\n",
       "         [ 48,  51,  49]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 47,  50,  48],\n",
       "         [ 48,  51,  49],\n",
       "         [ 47,  50,  48]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3516770000023826, 'inference': 5.527144999973643, 'postprocess': 1.0833669999783524},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 43,  46,  44],\n",
       "         [ 44,  47,  45],\n",
       "         [ 44,  47,  45]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 40,  43,  41],\n",
       "         [ 40,  43,  41],\n",
       "         [ 40,  43,  41]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 47,  50,  48],\n",
       "         [ 47,  50,  48],\n",
       "         [ 47,  50,  48]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4959810000618745, 'inference': 6.030848999898808, 'postprocess': 1.014212999962183},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 94,  97,  95],\n",
       "         [ 59,  62,  60],\n",
       "         [ 43,  46,  44]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 54,  57,  55],\n",
       "         [ 44,  47,  45],\n",
       "         [ 38,  41,  39]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 38,  41,  39],\n",
       "         [ 44,  47,  45],\n",
       "         [ 49,  52,  50]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6478080000297268, 'inference': 5.602526000075159, 'postprocess': 1.0572659999752432},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [142, 142, 142],\n",
       "         [110, 110, 110]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [108, 108, 108],\n",
       "         [ 82,  82,  82]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [ 98,  98,  98],\n",
       "         [ 65,  65,  65],\n",
       "         [ 50,  50,  50]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.560865999977068, 'inference': 6.675765000068168, 'postprocess': 1.029505000019526},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [132, 132, 132],\n",
       "         [ 65,  65,  65]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [111, 111, 111],\n",
       "         [ 53,  53,  53]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [142, 142, 142],\n",
       "         [ 86,  86,  86],\n",
       "         [ 36,  36,  36]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4937669999426362, 'inference': 6.0045820000595995, 'postprocess': 1.0043459999451443},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [135, 135, 135],\n",
       "         [ 66,  66,  66]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [119, 119, 119],\n",
       "         [ 58,  58,  58]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [ 89,  89,  89],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223],\n",
       "         [222, 225, 223]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9375580000087211, 'inference': 5.933631000061723, 'postprocess': 1.0480570000481748},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [135, 135, 135],\n",
       "         [ 66,  66,  66]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [119, 119, 119],\n",
       "         [ 58,  58,  58]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [ 89,  89,  89],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6035120000879033, 'inference': 6.518821999975444, 'postprocess': 1.0446329999922455},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [151, 151, 151],\n",
       "         [135, 135, 135],\n",
       "         [ 66,  66,  66]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [119, 119, 119],\n",
       "         [ 58,  58,  58]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [140, 140, 140],\n",
       "         [ 89,  89,  89],\n",
       "         [ 39,  39,  39]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4313890000039464, 'inference': 6.963358999996672, 'postprocess': 0.9720089999518677},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [152, 152, 152],\n",
       "         [133, 133, 133],\n",
       "         [ 66,  66,  66]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [129, 129, 129],\n",
       "         [ 64,  64,  64]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 144, 144],\n",
       "         [104, 104, 104],\n",
       "         [ 48,  48,  48]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8816179999703309, 'inference': 6.403587000022526, 'postprocess': 1.4785529999699065},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [151, 151, 151],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [149, 149, 149],\n",
       "         [130, 130, 130]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [108, 108, 108]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.0447560000320664, 'inference': 6.913507000035679, 'postprocess': 1.6361479999886797},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [151, 151, 151],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [151, 151, 151],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [151, 151, 151],\n",
       "         [144, 144, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9336500000690648, 'inference': 6.417671999997765, 'postprocess': 0.9369729999662013},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [147, 147, 147],\n",
       "         [150, 150, 150]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4130419999673904, 'inference': 5.983555000057095, 'postprocess': 1.1384480000060648},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7552690000002258, 'inference': 6.242977000056271, 'postprocess': 0.9780870000213326},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.379294999992453, 'inference': 5.699713000012707, 'postprocess': 0.9989750000158892},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         [140, 140, 140],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.27062699993985, 'inference': 7.201301999998577, 'postprocess': 0.9936569999808853},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5859769999906348, 'inference': 6.444758000043294, 'postprocess': 1.3994839999895703},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6993440000305782, 'inference': 5.71274000003541, 'postprocess': 0.9603360000483008},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8473160000667121, 'inference': 6.816542000024128, 'postprocess': 0.9598220000270885},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8856959999311584, 'inference': 5.711320000045816, 'postprocess': 0.9331779999683931},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.9498919999705322, 'inference': 5.95971199993528, 'postprocess': 1.0117340000306285},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6234049999184208, 'inference': 6.402907000051528, 'postprocess': 1.0209039999153902},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [147, 150, 148],\n",
       "         [147, 150, 148],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [143, 146, 144],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [142, 145, 143],\n",
       "         [143, 146, 144],\n",
       "         [142, 145, 143]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.722122000046511, 'inference': 5.9711110000080225, 'postprocess': 0.9935040000073059},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.8032110000140165, 'inference': 6.4599979999684365, 'postprocess': 1.3243710000097053},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [224, 227, 225],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8727440000247952, 'inference': 5.774869000106264, 'postprocess': 1.126358999954391},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.766152000072907, 'inference': 5.319702000065263, 'postprocess': 0.9542450000026292},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.6765250000835294, 'inference': 7.053289999930712, 'postprocess': 1.661409000007552},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [150, 150, 150],\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.6505190000088987, 'inference': 5.596800000034818, 'postprocess': 1.245470999947429},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2645580000025802, 'inference': 6.572308000045268, 'postprocess': 1.1120930000743101},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2320579999141046, 'inference': 5.692877999990742, 'postprocess': 0.9550120000767492},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.347209999925326, 'inference': 6.603100999996059, 'postprocess': 1.0685230000717638},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         [139, 139, 139],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.506666000068435, 'inference': 5.650749999972504, 'postprocess': 1.198256000066067},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.7395950000036464, 'inference': 6.767529000057948, 'postprocess': 1.3729730000022755},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4625009999917893, 'inference': 6.074914999999237, 'postprocess': 1.0972810000566824},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [145, 145, 145],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [225, 228, 226],\n",
       "         [226, 229, 227],\n",
       "         [226, 229, 227]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.730108000037035, 'inference': 6.481072999918069, 'postprocess': 1.1971099999072976},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.383805000045868, 'inference': 5.92082200000732, 'postprocess': 1.0229779999235689},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.953675999970983, 'inference': 6.36582399999952, 'postprocess': 1.1038879999887286},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8370850000337668, 'inference': 6.608328999959667, 'postprocess': 0.9848920000194994},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.3142569999663465, 'inference': 7.196505000024445, 'postprocess': 1.4992879999908837},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2651950000872603, 'inference': 5.9222969999837005, 'postprocess': 1.0785019999275391},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.5666609999698267, 'inference': 5.452753999975357, 'postprocess': 0.950952999914989},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.5950400000738227, 'inference': 5.380388999924435, 'postprocess': 0.9245390000387488},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2230649999764864, 'inference': 6.4473969999880865, 'postprocess': 0.9581389999766543},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.2001219999765453, 'inference': 5.5951690000028975, 'postprocess': 1.0557099999459751},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3817539999081419, 'inference': 5.728610000005574, 'postprocess': 1.685213999962798},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3416599999800383, 'inference': 5.491942999924504, 'postprocess': 1.6324619999750212},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[138, 138, 138],\n",
       "         [137, 137, 137],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        [[138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         [138, 138, 138],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.9516780000449216, 'inference': 5.7090929999503714, 'postprocess': 0.9429519999457625},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [136, 139, 137],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4838580000287038, 'inference': 6.432244000052378, 'postprocess': 1.4683900000136418},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [136, 139, 137],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8095670000093378, 'inference': 5.820559999961006, 'postprocess': 0.9975069999654806},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [136, 139, 137],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.777851000042574, 'inference': 6.22337000004336, 'postprocess': 1.0110659999327254},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [136, 139, 137],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3408080000090195, 'inference': 5.904267999994772, 'postprocess': 1.0061650000352529},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [136, 139, 137],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         [210, 210, 210],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4270049999822731, 'inference': 5.748104999952375, 'postprocess': 1.1191950000011275},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [136, 139, 137],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.4562220000016168, 'inference': 5.472321999945962, 'postprocess': 1.1741970000684887},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[137, 140, 138],\n",
       "         [136, 139, 137],\n",
       "         [137, 140, 138],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.262252000036824, 'inference': 5.809606999946482, 'postprocess': 0.971664000076089},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [228, 228, 228],\n",
       "         [228, 228, 228],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.1553470000080779, 'inference': 6.163919000073292, 'postprocess': 0.9505020000233344},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.8445990000373058, 'inference': 6.233429000076285, 'postprocess': 1.3181069999745887},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.2617319999890242, 'inference': 6.117702999972607, 'postprocess': 1.0684149999633519},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         [207, 210, 208],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.493160999984866, 'inference': 6.338278999919567, 'postprocess': 1.2173979999943185},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.411232000009477, 'inference': 5.505412000047727, 'postprocess': 1.137115000005906},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         [206, 209, 207],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 2.957285999968917, 'inference': 7.246615000099155, 'postprocess': 1.4040199999953984},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         [136, 139, 137],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [146, 149, 147],\n",
       "         [144, 147, 145],\n",
       "         [144, 147, 145]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [149, 149, 149],\n",
       "         [147, 147, 147],\n",
       "         [147, 147, 147]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.3494499999069376, 'inference': 5.475993000004564, 'postprocess': 0.9899780000068858},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [134, 137, 135],\n",
       "         [134, 137, 135],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[133, 136, 134],\n",
       "         [134, 137, 135],\n",
       "         [134, 137, 135],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 3.2527820000041174, 'inference': 12.14575300002707, 'postprocess': 1.6384699999889563},\n",
       " ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: ultralytics.engine.results.Keypoints object\n",
       " masks: None\n",
       " names: {0: 'person'}\n",
       " obb: None\n",
       " orig_img: array([[[135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         [135, 138, 136],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[134, 137, 135],\n",
       "         [134, 137, 135],\n",
       "         [134, 137, 135],\n",
       "         ...,\n",
       "         [144, 147, 145],\n",
       "         [143, 146, 144],\n",
       "         [143, 146, 144]],\n",
       " \n",
       "        [[133, 136, 134],\n",
       "         [134, 137, 135],\n",
       "         [134, 137, 135],\n",
       "         ...,\n",
       "         [147, 147, 147],\n",
       "         [146, 146, 146],\n",
       "         [146, 146, 146]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]],\n",
       " \n",
       "        [[209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         [209, 209, 209],\n",
       "         ...,\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229],\n",
       "         [229, 229, 229]]], shape=(720, 1280, 3), dtype=uint8)\n",
       " orig_shape: (720, 1280)\n",
       " path: '/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4'\n",
       " probs: None\n",
       " save_dir: '/home/roni/dev_ws/yolo_tut/src/runs/pose/predict'\n",
       " speed: {'preprocess': 1.164002999985314, 'inference': 5.9806140000091546, 'postprocess': 1.0949980000987125}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_path = \"/home/roni/dev_ws/yolo_tut/datasets/people_walk.mp4\"\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "model(source = source_path, show = True, imgsz = 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5faa734a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING  \n",
      "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
      "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
      "\n",
      "Example:\n",
      "    results = model(source=..., stream=True)  # generator of Results objects\n",
      "    for r in results:\n",
      "        boxes = r.boxes  # Boxes object for bbox outputs\n",
      "        masks = r.masks  # Masks object for segment masks outputs\n",
      "        probs = r.probs  # Class probabilities for classification outputs\n",
      "\n",
      "video 1/1 (frame 1/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.9ms\n",
      "video 1/1 (frame 2/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.6ms\n",
      "video 1/1 (frame 3/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.5ms\n",
      "video 1/1 (frame 4/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 5.8ms\n",
      "video 1/1 (frame 5/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.1ms\n",
      "video 1/1 (frame 6/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 5.9ms\n",
      "video 1/1 (frame 7/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 6.8ms\n",
      "video 1/1 (frame 8/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 5.7ms\n",
      "video 1/1 (frame 9/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 21 persons, 8.2ms\n",
      "video 1/1 (frame 10/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 5.8ms\n",
      "video 1/1 (frame 11/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 6.1ms\n",
      "video 1/1 (frame 12/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 6.5ms\n",
      "video 1/1 (frame 13/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 6.5ms\n",
      "video 1/1 (frame 14/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.9ms\n",
      "video 1/1 (frame 15/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.3ms\n",
      "video 1/1 (frame 16/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.6ms\n",
      "video 1/1 (frame 17/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.8ms\n",
      "video 1/1 (frame 18/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.4ms\n",
      "video 1/1 (frame 19/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.7ms\n",
      "video 1/1 (frame 20/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.5ms\n",
      "video 1/1 (frame 21/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.8ms\n",
      "video 1/1 (frame 22/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.4ms\n",
      "video 1/1 (frame 23/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.7ms\n",
      "video 1/1 (frame 24/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 6.1ms\n",
      "video 1/1 (frame 25/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 6.4ms\n",
      "video 1/1 (frame 26/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 6.7ms\n",
      "video 1/1 (frame 27/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 5.8ms\n",
      "video 1/1 (frame 28/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.5ms\n",
      "video 1/1 (frame 29/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 6.2ms\n",
      "video 1/1 (frame 30/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 5.8ms\n",
      "video 1/1 (frame 31/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 5.6ms\n",
      "video 1/1 (frame 32/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 6.3ms\n",
      "video 1/1 (frame 33/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.8ms\n",
      "video 1/1 (frame 34/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.0ms\n",
      "video 1/1 (frame 35/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 22 persons, 6.8ms\n",
      "video 1/1 (frame 36/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.2ms\n",
      "video 1/1 (frame 37/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.3ms\n",
      "video 1/1 (frame 38/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 7.9ms\n",
      "video 1/1 (frame 39/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 7.0ms\n",
      "video 1/1 (frame 40/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 6.6ms\n",
      "video 1/1 (frame 41/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 7.1ms\n",
      "video 1/1 (frame 42/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.5ms\n",
      "video 1/1 (frame 43/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 7.1ms\n",
      "video 1/1 (frame 44/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.4ms\n",
      "video 1/1 (frame 45/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 46/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.1ms\n",
      "video 1/1 (frame 47/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 48/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.5ms\n",
      "video 1/1 (frame 49/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 50/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 51/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.4ms\n",
      "video 1/1 (frame 52/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.9ms\n",
      "video 1/1 (frame 53/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.9ms\n",
      "video 1/1 (frame 54/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 55/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 56/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 57/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 58/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.9ms\n",
      "video 1/1 (frame 59/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.4ms\n",
      "video 1/1 (frame 60/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 61/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 62/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 63/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 64/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 65/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 66/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 67/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 68/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 69/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.6ms\n",
      "video 1/1 (frame 70/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 71/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 72/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 73/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 74/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.6ms\n",
      "video 1/1 (frame 75/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.4ms\n",
      "video 1/1 (frame 76/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.6ms\n",
      "video 1/1 (frame 77/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 78/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 79/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 80/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 81/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 82/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.4ms\n",
      "video 1/1 (frame 83/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 84/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.2ms\n",
      "video 1/1 (frame 85/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.5ms\n",
      "video 1/1 (frame 86/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 87/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 88/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 89/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 90/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 91/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 92/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 93/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 94/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 95/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 96/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 97/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.5ms\n",
      "video 1/1 (frame 98/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.9ms\n",
      "video 1/1 (frame 99/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 7.1ms\n",
      "video 1/1 (frame 100/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.7ms\n",
      "video 1/1 (frame 101/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.5ms\n",
      "video 1/1 (frame 102/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 103/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.0ms\n",
      "video 1/1 (frame 104/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.6ms\n",
      "video 1/1 (frame 105/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.7ms\n",
      "video 1/1 (frame 106/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.9ms\n",
      "video 1/1 (frame 107/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.5ms\n",
      "video 1/1 (frame 108/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 109/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 110/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.6ms\n",
      "video 1/1 (frame 111/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.0ms\n",
      "video 1/1 (frame 112/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.8ms\n",
      "video 1/1 (frame 113/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.3ms\n",
      "video 1/1 (frame 114/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 115/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 116/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 117/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 7.1ms\n",
      "video 1/1 (frame 118/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.5ms\n",
      "video 1/1 (frame 119/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.6ms\n",
      "video 1/1 (frame 120/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 121/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 122/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.0ms\n",
      "video 1/1 (frame 123/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.4ms\n",
      "video 1/1 (frame 124/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.5ms\n",
      "video 1/1 (frame 125/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 9.1ms\n",
      "video 1/1 (frame 126/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 8.0ms\n",
      "video 1/1 (frame 127/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.3ms\n",
      "video 1/1 (frame 128/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.2ms\n",
      "video 1/1 (frame 129/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 7.4ms\n",
      "video 1/1 (frame 130/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.9ms\n",
      "video 1/1 (frame 131/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.8ms\n",
      "video 1/1 (frame 132/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.3ms\n",
      "video 1/1 (frame 133/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 134/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 8.2ms\n",
      "video 1/1 (frame 135/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.4ms\n",
      "video 1/1 (frame 136/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 137/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 138/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.5ms\n",
      "video 1/1 (frame 139/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 140/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.7ms\n",
      "video 1/1 (frame 141/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 8.3ms\n",
      "video 1/1 (frame 142/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 143/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.9ms\n",
      "video 1/1 (frame 144/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.2ms\n",
      "video 1/1 (frame 145/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 146/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 147/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.9ms\n",
      "video 1/1 (frame 148/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 149/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.9ms\n",
      "video 1/1 (frame 150/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 151/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.0ms\n",
      "video 1/1 (frame 152/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 153/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.4ms\n",
      "video 1/1 (frame 154/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.3ms\n",
      "video 1/1 (frame 155/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 156/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 157/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 158/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 159/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.8ms\n",
      "video 1/1 (frame 160/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.7ms\n",
      "video 1/1 (frame 161/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 162/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 163/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 164/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 165/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 166/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 167/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 168/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 169/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 170/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 171/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 172/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 173/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 174/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 175/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 176/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.3ms\n",
      "video 1/1 (frame 177/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 178/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 179/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 180/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 181/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 182/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 183/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 184/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 185/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 186/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 187/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 188/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 189/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 190/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 191/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.4ms\n",
      "video 1/1 (frame 192/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.6ms\n",
      "video 1/1 (frame 193/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 194/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 195/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 196/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 197/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 198/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 199/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 200/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 201/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 202/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 203/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 204/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 205/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.4ms\n",
      "video 1/1 (frame 206/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.3ms\n",
      "video 1/1 (frame 207/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 208/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 209/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 210/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 211/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 212/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 213/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 214/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.4ms\n",
      "video 1/1 (frame 215/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 216/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.9ms\n",
      "video 1/1 (frame 217/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 218/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 219/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.2ms\n",
      "video 1/1 (frame 220/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 221/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 222/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 223/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.9ms\n",
      "video 1/1 (frame 224/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.5ms\n",
      "video 1/1 (frame 225/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.2ms\n",
      "video 1/1 (frame 226/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 227/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 228/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.4ms\n",
      "video 1/1 (frame 229/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 230/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 231/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 232/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.6ms\n",
      "video 1/1 (frame 233/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.1ms\n",
      "video 1/1 (frame 234/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.9ms\n",
      "video 1/1 (frame 235/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.3ms\n",
      "video 1/1 (frame 236/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 237/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 238/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.3ms\n",
      "video 1/1 (frame 239/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 240/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 241/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 242/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 243/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 244/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.5ms\n",
      "video 1/1 (frame 245/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 246/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 247/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 248/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 249/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 250/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.5ms\n",
      "video 1/1 (frame 251/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 252/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.2ms\n",
      "video 1/1 (frame 253/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.3ms\n",
      "video 1/1 (frame 254/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 255/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 256/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 257/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 258/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 259/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 260/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 261/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 262/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 263/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 264/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 265/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 7.0ms\n",
      "video 1/1 (frame 266/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 7.2ms\n",
      "video 1/1 (frame 267/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.2ms\n",
      "video 1/1 (frame 268/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.0ms\n",
      "video 1/1 (frame 269/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 270/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 271/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 272/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 273/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 274/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 275/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 276/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 277/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 278/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 279/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.1ms\n",
      "video 1/1 (frame 280/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 281/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 282/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 283/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 284/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 285/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 286/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 287/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 288/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 289/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.6ms\n",
      "video 1/1 (frame 290/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 291/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.3ms\n",
      "video 1/1 (frame 292/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 293/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 294/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 295/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 296/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.2ms\n",
      "video 1/1 (frame 297/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 298/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 299/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 300/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.1ms\n",
      "video 1/1 (frame 301/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 302/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 303/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.2ms\n",
      "video 1/1 (frame 304/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 305/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 306/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 307/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 308/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 7.9ms\n",
      "video 1/1 (frame 309/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.0ms\n",
      "video 1/1 (frame 310/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.9ms\n",
      "video 1/1 (frame 311/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.6ms\n",
      "video 1/1 (frame 312/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 313/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 314/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 315/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 316/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 317/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 318/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 319/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 320/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 321/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 322/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 323/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 324/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.4ms\n",
      "video 1/1 (frame 325/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 326/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 327/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 328/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 329/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 330/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 331/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 332/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 333/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 334/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 335/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 336/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 337/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 338/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 339/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 340/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 341/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 342/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 343/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 344/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 345/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 346/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 347/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 348/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 349/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 350/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 351/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 352/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 353/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 354/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 355/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 356/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 357/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 358/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 359/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 360/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 361/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 362/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 363/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 364/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 365/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 366/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.6ms\n",
      "video 1/1 (frame 367/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.9ms\n",
      "video 1/1 (frame 368/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 369/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 370/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 371/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.6ms\n",
      "video 1/1 (frame 372/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 373/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 374/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 375/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.6ms\n",
      "video 1/1 (frame 376/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.2ms\n",
      "video 1/1 (frame 377/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.5ms\n",
      "video 1/1 (frame 378/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 379/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 380/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.6ms\n",
      "video 1/1 (frame 381/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 382/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 383/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 384/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.6ms\n",
      "video 1/1 (frame 385/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.1ms\n",
      "video 1/1 (frame 386/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.3ms\n",
      "video 1/1 (frame 387/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 388/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 389/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 390/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 391/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 392/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 393/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 394/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 395/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 396/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 397/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 398/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 399/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 400/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 401/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 402/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.1ms\n",
      "video 1/1 (frame 403/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.6ms\n",
      "video 1/1 (frame 404/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.0ms\n",
      "video 1/1 (frame 405/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.3ms\n",
      "video 1/1 (frame 406/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.0ms\n",
      "video 1/1 (frame 407/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 408/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 409/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 410/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 411/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 412/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 413/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.6ms\n",
      "video 1/1 (frame 414/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 415/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.5ms\n",
      "video 1/1 (frame 416/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 417/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.4ms\n",
      "video 1/1 (frame 418/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 419/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 420/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 421/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 422/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 423/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 424/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 425/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 426/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.7ms\n",
      "video 1/1 (frame 427/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 428/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.2ms\n",
      "video 1/1 (frame 429/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 430/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 431/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 432/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 433/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 434/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 435/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 436/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 437/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 438/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 439/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 440/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 441/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 442/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 443/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.2ms\n",
      "video 1/1 (frame 444/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 445/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.2ms\n",
      "video 1/1 (frame 446/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 447/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 448/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.4ms\n",
      "video 1/1 (frame 449/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 450/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 451/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 452/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.4ms\n",
      "video 1/1 (frame 453/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 454/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 455/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 456/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 457/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.7ms\n",
      "video 1/1 (frame 458/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 459/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 460/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 461/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 462/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.5ms\n",
      "video 1/1 (frame 463/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.5ms\n",
      "video 1/1 (frame 464/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.6ms\n",
      "video 1/1 (frame 465/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 466/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 467/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 468/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 469/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 470/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 471/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 472/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 473/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 474/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 475/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 476/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 477/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 478/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 479/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 480/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 481/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 482/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 483/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 484/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 485/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 486/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.8ms\n",
      "video 1/1 (frame 487/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 488/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 489/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 490/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 491/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 492/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 493/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 494/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 495/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 496/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 497/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 498/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 499/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 500/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 501/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 502/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 503/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 504/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 505/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 506/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 507/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 508/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 509/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 510/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 511/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 512/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 513/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 514/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 515/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 516/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 517/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 518/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 519/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 520/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 521/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.9ms\n",
      "video 1/1 (frame 522/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 523/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 524/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 525/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 526/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 527/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 528/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 529/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.0ms\n",
      "video 1/1 (frame 530/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 531/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 532/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 533/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 534/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.3ms\n",
      "video 1/1 (frame 535/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 536/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 537/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 538/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 539/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 540/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 541/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 542/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.3ms\n",
      "video 1/1 (frame 543/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 544/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 545/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 546/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 547/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 548/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 549/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 550/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 551/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 552/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 553/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 554/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.8ms\n",
      "video 1/1 (frame 555/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 556/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 557/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 558/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 559/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 560/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 561/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 562/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.2ms\n",
      "video 1/1 (frame 563/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 564/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 565/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 566/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 567/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 568/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 569/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 570/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 571/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 572/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 573/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 574/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 575/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 576/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 577/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 578/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 579/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 580/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 581/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 582/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 583/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 584/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 585/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 586/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 587/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 588/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 589/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.9ms\n",
      "video 1/1 (frame 590/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 591/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 592/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 593/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 594/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 595/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 596/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 597/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 598/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 599/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 600/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 601/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 602/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 603/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 604/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 605/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 606/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 607/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 608/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 609/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 610/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 611/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 612/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 613/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.2ms\n",
      "video 1/1 (frame 614/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 615/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 616/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 617/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 618/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 619/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 620/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 621/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 622/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.1ms\n",
      "video 1/1 (frame 623/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 624/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 625/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 626/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 627/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 628/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 629/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 630/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 631/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 632/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 633/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.1ms\n",
      "video 1/1 (frame 634/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 635/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 636/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 637/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 638/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 639/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 640/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 641/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 642/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 643/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 644/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 645/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 646/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 647/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 648/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 649/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 650/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 651/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 652/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 653/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 654/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 655/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 656/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 657/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 658/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 659/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 660/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 661/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 662/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 663/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 664/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 665/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 666/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 667/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 668/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 669/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 670/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 671/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.9ms\n",
      "video 1/1 (frame 672/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 673/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.3ms\n",
      "video 1/1 (frame 674/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 675/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.3ms\n",
      "video 1/1 (frame 676/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.8ms\n",
      "video 1/1 (frame 677/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.4ms\n",
      "video 1/1 (frame 678/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.1ms\n",
      "video 1/1 (frame 679/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.3ms\n",
      "video 1/1 (frame 680/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 681/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.6ms\n",
      "video 1/1 (frame 682/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.7ms\n",
      "video 1/1 (frame 683/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.4ms\n",
      "video 1/1 (frame 684/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.1ms\n",
      "video 1/1 (frame 685/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.3ms\n",
      "video 1/1 (frame 686/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.1ms\n",
      "video 1/1 (frame 687/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.7ms\n",
      "video 1/1 (frame 688/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.7ms\n",
      "video 1/1 (frame 689/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.1ms\n",
      "video 1/1 (frame 690/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.4ms\n",
      "video 1/1 (frame 691/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.7ms\n",
      "video 1/1 (frame 692/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 10.0ms\n",
      "video 1/1 (frame 693/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.4ms\n",
      "video 1/1 (frame 694/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.4ms\n",
      "video 1/1 (frame 695/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.5ms\n",
      "video 1/1 (frame 696/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 697/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 698/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 699/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 700/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.4ms\n",
      "video 1/1 (frame 701/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 702/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 703/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 704/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.9ms\n",
      "video 1/1 (frame 705/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 706/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 707/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 708/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 709/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 710/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 711/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 712/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 713/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 714/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 715/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 716/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 717/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 718/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 719/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 720/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 721/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 722/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 723/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 724/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 725/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 726/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 727/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.8ms\n",
      "video 1/1 (frame 728/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 729/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 730/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 731/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 732/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 733/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 734/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 735/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 736/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 737/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 738/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 739/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 740/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 741/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 742/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 743/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 744/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 745/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 746/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 747/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 748/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 749/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 750/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 751/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 752/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 753/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 754/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 755/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 756/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 757/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 758/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 759/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.7ms\n",
      "video 1/1 (frame 760/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 761/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 762/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 763/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.4ms\n",
      "video 1/1 (frame 764/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 765/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 766/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 767/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.7ms\n",
      "video 1/1 (frame 768/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 769/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 770/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 771/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 772/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 773/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 774/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.3ms\n",
      "video 1/1 (frame 775/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 776/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.8ms\n",
      "video 1/1 (frame 777/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 7.0ms\n",
      "video 1/1 (frame 778/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 7.0ms\n",
      "video 1/1 (frame 779/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.4ms\n",
      "video 1/1 (frame 780/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.0ms\n",
      "video 1/1 (frame 781/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.6ms\n",
      "video 1/1 (frame 782/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.1ms\n",
      "video 1/1 (frame 783/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.4ms\n",
      "video 1/1 (frame 784/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 785/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 7.4ms\n",
      "video 1/1 (frame 786/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 787/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.8ms\n",
      "video 1/1 (frame 788/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.3ms\n",
      "video 1/1 (frame 789/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.5ms\n",
      "video 1/1 (frame 790/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 791/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 792/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 10.4ms\n",
      "video 1/1 (frame 793/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.6ms\n",
      "video 1/1 (frame 794/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 795/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.5ms\n",
      "video 1/1 (frame 796/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.0ms\n",
      "video 1/1 (frame 797/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 798/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.6ms\n",
      "video 1/1 (frame 799/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.7ms\n",
      "video 1/1 (frame 800/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.3ms\n",
      "video 1/1 (frame 801/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 802/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.6ms\n",
      "video 1/1 (frame 803/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 10.1ms\n",
      "video 1/1 (frame 804/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 805/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 806/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 807/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 808/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 809/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 810/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 811/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.6ms\n",
      "video 1/1 (frame 812/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 813/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 814/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 815/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 816/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 817/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 818/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 819/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 820/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 821/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 822/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 823/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 824/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 825/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 826/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 827/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 828/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.4ms\n",
      "video 1/1 (frame 829/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 830/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 831/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 832/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 833/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 834/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 835/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 836/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 837/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 838/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 839/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 840/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 841/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 842/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 843/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 844/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 845/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 846/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 847/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 848/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 849/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 9.6ms\n",
      "video 1/1 (frame 850/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 851/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 852/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 853/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 854/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 855/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.8ms\n",
      "video 1/1 (frame 856/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 857/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 858/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 859/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 860/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 861/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 862/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 863/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 864/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 865/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.1ms\n",
      "video 1/1 (frame 866/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 867/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 868/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 869/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 870/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 871/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 872/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 873/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 874/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 875/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 876/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 877/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 878/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 879/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 880/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 881/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 882/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 883/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 884/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 885/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.0ms\n",
      "video 1/1 (frame 886/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.1ms\n",
      "video 1/1 (frame 887/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 888/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.6ms\n",
      "video 1/1 (frame 889/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 890/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 891/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 892/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 893/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 894/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 895/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 896/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 897/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 898/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 899/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 900/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 901/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.5ms\n",
      "video 1/1 (frame 902/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.8ms\n",
      "video 1/1 (frame 903/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 904/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 905/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.4ms\n",
      "video 1/1 (frame 906/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 7.0ms\n",
      "video 1/1 (frame 907/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.3ms\n",
      "video 1/1 (frame 908/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.9ms\n",
      "video 1/1 (frame 909/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.3ms\n",
      "video 1/1 (frame 910/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 7.7ms\n",
      "video 1/1 (frame 911/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 912/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.3ms\n",
      "video 1/1 (frame 913/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.7ms\n",
      "video 1/1 (frame 914/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 915/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 916/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 917/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 918/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.6ms\n",
      "video 1/1 (frame 919/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.5ms\n",
      "video 1/1 (frame 920/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.3ms\n",
      "video 1/1 (frame 921/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.8ms\n",
      "video 1/1 (frame 922/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 923/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.9ms\n",
      "video 1/1 (frame 924/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.5ms\n",
      "video 1/1 (frame 925/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.5ms\n",
      "video 1/1 (frame 926/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 927/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 928/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 929/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 930/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.6ms\n",
      "video 1/1 (frame 931/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 932/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 933/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 934/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 935/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 936/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 937/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.2ms\n",
      "video 1/1 (frame 938/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.1ms\n",
      "video 1/1 (frame 939/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 940/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.7ms\n",
      "video 1/1 (frame 941/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 942/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.3ms\n",
      "video 1/1 (frame 943/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 944/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 945/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 946/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 947/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 948/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 949/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 950/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 951/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 952/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 953/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 954/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 955/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 956/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.8ms\n",
      "video 1/1 (frame 957/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 958/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 959/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 960/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 961/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 962/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 963/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 964/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 965/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 966/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 967/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 968/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 969/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 970/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 971/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 972/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 973/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 974/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 975/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 976/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 977/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 978/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 979/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 980/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 981/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 982/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 983/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 984/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 985/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.1ms\n",
      "video 1/1 (frame 986/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 987/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 988/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 989/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.7ms\n",
      "video 1/1 (frame 990/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 991/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 992/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 993/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 994/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 995/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 996/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 997/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.5ms\n",
      "video 1/1 (frame 998/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.2ms\n",
      "video 1/1 (frame 999/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1000/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 1001/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1002/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 1003/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 1004/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 1005/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 1006/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 1007/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 1008/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 1009/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 1010/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 1011/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 1012/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 1013/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 1014/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 1015/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 1016/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 1017/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 1018/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1019/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1020/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 1021/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 1022/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1023/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1024/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1025/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 1026/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1027/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1028/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1029/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 1030/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1031/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1032/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1033/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1034/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 1035/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 1036/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1037/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1038/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1039/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1040/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 1041/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.9ms\n",
      "video 1/1 (frame 1042/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1043/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1044/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 1045/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1046/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 1047/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 1048/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.2ms\n",
      "video 1/1 (frame 1049/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.2ms\n",
      "video 1/1 (frame 1050/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1051/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1052/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1053/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1054/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1055/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1056/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1057/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1058/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1059/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1060/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 1061/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1062/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1063/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1064/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1065/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1066/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.7ms\n",
      "video 1/1 (frame 1067/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1068/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1069/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 1070/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1071/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 1072/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1073/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 1074/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1075/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1076/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1077/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1078/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1079/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1080/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1081/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1082/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1083/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1084/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1085/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1086/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1087/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1088/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1089/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1090/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1091/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1092/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1093/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1094/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1095/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 1096/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.3ms\n",
      "video 1/1 (frame 1097/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1098/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1099/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 1100/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 1101/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1102/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1103/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 1104/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 1105/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 1106/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 1107/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 1108/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 1109/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.3ms\n",
      "video 1/1 (frame 1110/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 1111/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.9ms\n",
      "video 1/1 (frame 1112/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.9ms\n",
      "video 1/1 (frame 1113/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 1114/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 1115/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.8ms\n",
      "video 1/1 (frame 1116/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.2ms\n",
      "video 1/1 (frame 1117/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.7ms\n",
      "video 1/1 (frame 1118/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.8ms\n",
      "video 1/1 (frame 1119/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 1120/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.9ms\n",
      "video 1/1 (frame 1121/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.2ms\n",
      "video 1/1 (frame 1122/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.4ms\n",
      "video 1/1 (frame 1123/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.3ms\n",
      "video 1/1 (frame 1124/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.3ms\n",
      "video 1/1 (frame 1125/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.1ms\n",
      "video 1/1 (frame 1126/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.3ms\n",
      "video 1/1 (frame 1127/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.4ms\n",
      "video 1/1 (frame 1128/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.6ms\n",
      "video 1/1 (frame 1129/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 6.2ms\n",
      "video 1/1 (frame 1130/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 5.4ms\n",
      "video 1/1 (frame 1131/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.7ms\n",
      "video 1/1 (frame 1132/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.2ms\n",
      "video 1/1 (frame 1133/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.9ms\n",
      "video 1/1 (frame 1134/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.7ms\n",
      "video 1/1 (frame 1135/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.5ms\n",
      "video 1/1 (frame 1136/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.3ms\n",
      "video 1/1 (frame 1137/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.0ms\n",
      "video 1/1 (frame 1138/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.6ms\n",
      "video 1/1 (frame 1139/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.7ms\n",
      "video 1/1 (frame 1140/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.8ms\n",
      "video 1/1 (frame 1141/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.7ms\n",
      "video 1/1 (frame 1142/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 5.4ms\n",
      "video 1/1 (frame 1143/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.1ms\n",
      "video 1/1 (frame 1144/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.1ms\n",
      "video 1/1 (frame 1145/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.6ms\n",
      "video 1/1 (frame 1146/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.1ms\n",
      "video 1/1 (frame 1147/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.0ms\n",
      "video 1/1 (frame 1148/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.9ms\n",
      "video 1/1 (frame 1149/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.5ms\n",
      "video 1/1 (frame 1150/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.2ms\n",
      "video 1/1 (frame 1151/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.2ms\n",
      "video 1/1 (frame 1152/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.7ms\n",
      "video 1/1 (frame 1153/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.0ms\n",
      "video 1/1 (frame 1154/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 7.6ms\n",
      "video 1/1 (frame 1155/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 19 persons, 6.5ms\n",
      "video 1/1 (frame 1156/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.2ms\n",
      "video 1/1 (frame 1157/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 17 persons, 6.4ms\n",
      "video 1/1 (frame 1158/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 1159/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 1160/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 1161/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 1162/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 1163/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 1164/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 1165/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 1166/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 1167/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 1168/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 1169/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 1170/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 1171/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 1172/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 1173/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 1174/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 1175/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 1176/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 1177/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 1178/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 1179/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 1180/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1181/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1182/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 1183/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 1184/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 1185/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1186/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 1187/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 1188/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 1189/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 1190/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 1191/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.0ms\n",
      "video 1/1 (frame 1192/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 1193/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 1194/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 1195/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 1196/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 1197/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.7ms\n",
      "video 1/1 (frame 1198/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 1199/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 1200/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1201/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 1202/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 1203/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1204/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1205/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 1206/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 1207/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.8ms\n",
      "video 1/1 (frame 1208/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 1209/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 1210/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 1211/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 1212/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.2ms\n",
      "video 1/1 (frame 1213/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.5ms\n",
      "video 1/1 (frame 1214/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 1215/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 1216/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 1217/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 1218/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 1219/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 1220/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 1221/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.0ms\n",
      "video 1/1 (frame 1222/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 1223/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 1224/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.4ms\n",
      "video 1/1 (frame 1225/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 1226/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 1227/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 1228/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.3ms\n",
      "video 1/1 (frame 1229/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 1230/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 1231/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 1232/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 1233/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 1234/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 1235/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 1236/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 1237/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 1238/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 1239/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 1240/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 1241/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.3ms\n",
      "video 1/1 (frame 1242/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.3ms\n",
      "video 1/1 (frame 1243/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 10.1ms\n",
      "video 1/1 (frame 1244/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 9.0ms\n",
      "video 1/1 (frame 1245/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 1246/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 1247/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 1248/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.3ms\n",
      "video 1/1 (frame 1249/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.2ms\n",
      "video 1/1 (frame 1250/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.4ms\n",
      "video 1/1 (frame 1251/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.6ms\n",
      "video 1/1 (frame 1252/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.5ms\n",
      "video 1/1 (frame 1253/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.5ms\n",
      "video 1/1 (frame 1254/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.9ms\n",
      "video 1/1 (frame 1255/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.8ms\n",
      "video 1/1 (frame 1256/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.9ms\n",
      "video 1/1 (frame 1257/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.2ms\n",
      "video 1/1 (frame 1258/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.8ms\n",
      "video 1/1 (frame 1259/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.4ms\n",
      "video 1/1 (frame 1260/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.4ms\n",
      "video 1/1 (frame 1261/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.7ms\n",
      "video 1/1 (frame 1262/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.0ms\n",
      "video 1/1 (frame 1263/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.7ms\n",
      "video 1/1 (frame 1264/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.8ms\n",
      "video 1/1 (frame 1265/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.7ms\n",
      "video 1/1 (frame 1266/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.4ms\n",
      "video 1/1 (frame 1267/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.0ms\n",
      "video 1/1 (frame 1268/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 7.0ms\n",
      "video 1/1 (frame 1269/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.8ms\n",
      "video 1/1 (frame 1270/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1271/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1272/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1273/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1274/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1275/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1276/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 1277/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1278/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1279/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1280/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1281/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1282/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1283/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 1284/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1285/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 1286/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1287/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 1288/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 1289/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1290/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1291/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1292/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1293/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1294/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1295/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.8ms\n",
      "video 1/1 (frame 1296/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1297/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 1298/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 1299/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1300/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1301/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1302/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1303/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.6ms\n",
      "video 1/1 (frame 1304/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1305/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.3ms\n",
      "video 1/1 (frame 1306/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1307/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1308/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1309/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 1310/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1311/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1312/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1313/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1314/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1315/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 1316/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1317/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 1318/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1319/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.3ms\n",
      "video 1/1 (frame 1320/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 1321/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1322/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1323/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1324/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1325/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1326/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1327/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1328/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1329/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1330/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1331/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1332/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 1333/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1334/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1335/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1336/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1337/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1338/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1339/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1340/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1341/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1342/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1343/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1344/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.4ms\n",
      "video 1/1 (frame 1345/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 12.3ms\n",
      "video 1/1 (frame 1346/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 1347/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1348/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1349/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 1350/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 1351/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 1352/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1353/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 1354/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1355/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1356/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1357/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1358/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1359/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1360/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1361/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.3ms\n",
      "video 1/1 (frame 1362/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1363/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 1364/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1365/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 9.6ms\n",
      "video 1/1 (frame 1366/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.1ms\n",
      "video 1/1 (frame 1367/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.4ms\n",
      "video 1/1 (frame 1368/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 1369/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 1370/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1371/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1372/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1373/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 1374/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 1375/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1376/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 1377/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1378/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.9ms\n",
      "video 1/1 (frame 1379/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.4ms\n",
      "video 1/1 (frame 1380/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.2ms\n",
      "video 1/1 (frame 1381/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 8.1ms\n",
      "video 1/1 (frame 1382/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1383/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1384/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1385/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1386/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 1387/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1388/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 1389/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.2ms\n",
      "video 1/1 (frame 1390/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 1391/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1392/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1393/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1394/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1395/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 1396/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.7ms\n",
      "video 1/1 (frame 1397/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1398/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1399/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 1400/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1401/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1402/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1403/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 9.2ms\n",
      "video 1/1 (frame 1404/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1405/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1406/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1407/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.6ms\n",
      "video 1/1 (frame 1408/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.1ms\n",
      "video 1/1 (frame 1409/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 1410/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1411/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1412/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1413/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1414/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1415/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1416/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 4.9ms\n",
      "video 1/1 (frame 1417/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1418/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 4.9ms\n",
      "video 1/1 (frame 1419/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 4.9ms\n",
      "video 1/1 (frame 1420/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 11.1ms\n",
      "video 1/1 (frame 1421/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1422/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.2ms\n",
      "video 1/1 (frame 1423/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 295.8ms\n",
      "video 1/1 (frame 1424/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 10.6ms\n",
      "video 1/1 (frame 1425/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.2ms\n",
      "video 1/1 (frame 1426/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 1427/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.1ms\n",
      "video 1/1 (frame 1428/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1429/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 1430/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.0ms\n",
      "video 1/1 (frame 1431/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 1432/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 1433/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 1434/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.3ms\n",
      "video 1/1 (frame 1435/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 1436/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 4.9ms\n",
      "video 1/1 (frame 1437/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 1438/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.2ms\n",
      "video 1/1 (frame 1439/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 1440/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 1441/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 1442/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 1443/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.5ms\n",
      "video 1/1 (frame 1444/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1445/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.2ms\n",
      "video 1/1 (frame 1446/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1447/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 1448/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 1449/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 1450/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 1451/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 1452/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.2ms\n",
      "video 1/1 (frame 1453/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1454/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1455/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1456/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.7ms\n",
      "video 1/1 (frame 1457/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1458/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1459/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 1460/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 1461/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 1462/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 1463/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.2ms\n",
      "video 1/1 (frame 1464/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 1465/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.2ms\n",
      "video 1/1 (frame 1466/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 1467/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 1468/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 1469/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 1470/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 1471/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 1472/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 1473/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 1474/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 7.3ms\n",
      "video 1/1 (frame 1475/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.3ms\n",
      "video 1/1 (frame 1476/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 1477/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 1478/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 1479/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1480/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 11.7ms\n",
      "video 1/1 (frame 1481/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 1482/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1483/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1484/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.7ms\n",
      "video 1/1 (frame 1485/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 1486/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.5ms\n",
      "video 1/1 (frame 1487/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.4ms\n",
      "video 1/1 (frame 1488/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 1489/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 1490/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.5ms\n",
      "video 1/1 (frame 1491/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 1492/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.3ms\n",
      "video 1/1 (frame 1493/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 11.2ms\n",
      "video 1/1 (frame 1494/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 1495/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.6ms\n",
      "video 1/1 (frame 1496/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 1497/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 1498/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 1499/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.5ms\n",
      "video 1/1 (frame 1500/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.3ms\n",
      "video 1/1 (frame 1501/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 1502/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1503/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 1504/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 1505/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 1506/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 1507/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1508/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 1509/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 1510/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 1511/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 1512/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 1513/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 1514/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 1515/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 1516/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 1517/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.4ms\n",
      "video 1/1 (frame 1518/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 1519/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 1520/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 1521/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 1522/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.4ms\n",
      "video 1/1 (frame 1523/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 1524/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 1525/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 1526/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 1527/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 1528/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 1529/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 1530/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.6ms\n",
      "video 1/1 (frame 1531/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 1532/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 1533/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 1534/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 1535/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 1536/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1537/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 1538/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 1539/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1540/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.4ms\n",
      "video 1/1 (frame 1541/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.9ms\n",
      "video 1/1 (frame 1542/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.2ms\n",
      "video 1/1 (frame 1543/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 1544/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 1545/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.3ms\n",
      "video 1/1 (frame 1546/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1547/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 1548/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 1549/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 1550/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1551/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 1552/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1553/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 1554/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 1555/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1556/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.3ms\n",
      "video 1/1 (frame 1557/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1558/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1559/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1560/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.3ms\n",
      "video 1/1 (frame 1561/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 1562/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1563/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 1564/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.8ms\n",
      "video 1/1 (frame 1565/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.2ms\n",
      "video 1/1 (frame 1566/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1567/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1568/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1569/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1570/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1571/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1572/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1573/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1574/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.9ms\n",
      "video 1/1 (frame 1575/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.3ms\n",
      "video 1/1 (frame 1576/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 1577/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.0ms\n",
      "video 1/1 (frame 1578/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.3ms\n",
      "video 1/1 (frame 1579/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.3ms\n",
      "video 1/1 (frame 1580/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1581/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1582/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1583/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1584/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1585/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 1586/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.8ms\n",
      "video 1/1 (frame 1587/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 1588/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1589/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1590/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1591/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1592/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1593/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 1594/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 1595/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 1596/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1597/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1598/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1599/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1600/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1601/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1602/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 1603/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1604/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1605/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1606/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 1607/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.7ms\n",
      "video 1/1 (frame 1608/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.3ms\n",
      "video 1/1 (frame 1609/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.4ms\n",
      "video 1/1 (frame 1610/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.5ms\n",
      "video 1/1 (frame 1611/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.9ms\n",
      "video 1/1 (frame 1612/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1613/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.6ms\n",
      "video 1/1 (frame 1614/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1615/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.4ms\n",
      "video 1/1 (frame 1616/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 1617/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.3ms\n",
      "video 1/1 (frame 1618/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1619/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1620/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1621/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1622/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 1623/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1624/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1625/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1626/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1627/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1628/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1629/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1630/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1631/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1632/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1633/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1634/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1635/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1636/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 1637/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1638/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1639/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.6ms\n",
      "video 1/1 (frame 1640/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1641/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1642/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1643/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1644/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1645/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1646/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1647/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1648/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1649/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1650/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1651/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1652/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.1ms\n",
      "video 1/1 (frame 1653/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.7ms\n",
      "video 1/1 (frame 1654/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.4ms\n",
      "video 1/1 (frame 1655/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 9.5ms\n",
      "video 1/1 (frame 1656/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.7ms\n",
      "video 1/1 (frame 1657/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.9ms\n",
      "video 1/1 (frame 1658/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1659/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.2ms\n",
      "video 1/1 (frame 1660/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.9ms\n",
      "video 1/1 (frame 1661/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 1662/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 1663/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.3ms\n",
      "video 1/1 (frame 1664/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.0ms\n",
      "video 1/1 (frame 1665/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.5ms\n",
      "video 1/1 (frame 1666/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 1667/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 1668/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 1669/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.1ms\n",
      "video 1/1 (frame 1670/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.2ms\n",
      "video 1/1 (frame 1671/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 1672/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 1673/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1674/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.6ms\n",
      "video 1/1 (frame 1675/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 1676/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1677/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 1678/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 1679/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 1680/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 1681/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.7ms\n",
      "video 1/1 (frame 1682/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 1683/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1684/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 1685/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 1686/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 1687/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 1688/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 1689/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.4ms\n",
      "video 1/1 (frame 1690/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 1691/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.8ms\n",
      "video 1/1 (frame 1692/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1693/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 1694/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 1695/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.1ms\n",
      "video 1/1 (frame 1696/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.5ms\n",
      "video 1/1 (frame 1697/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 1698/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 1699/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1700/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 1701/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.1ms\n",
      "video 1/1 (frame 1702/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 9.1ms\n",
      "video 1/1 (frame 1703/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 1704/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1705/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1706/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.2ms\n",
      "video 1/1 (frame 1707/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.4ms\n",
      "video 1/1 (frame 1708/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.7ms\n",
      "video 1/1 (frame 1709/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 1710/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1711/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 8.2ms\n",
      "video 1/1 (frame 1712/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.1ms\n",
      "video 1/1 (frame 1713/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.0ms\n",
      "video 1/1 (frame 1714/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.5ms\n",
      "video 1/1 (frame 1715/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.4ms\n",
      "video 1/1 (frame 1716/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 1717/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 1718/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 1719/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1720/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 1721/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.4ms\n",
      "video 1/1 (frame 1722/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 1723/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.5ms\n",
      "video 1/1 (frame 1724/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.8ms\n",
      "video 1/1 (frame 1725/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 1726/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 1727/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1728/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1729/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1730/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1731/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1732/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 1733/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 1734/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 1735/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 1736/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.9ms\n",
      "video 1/1 (frame 1737/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 1738/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 9.3ms\n",
      "video 1/1 (frame 1739/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1740/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.3ms\n",
      "video 1/1 (frame 1741/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1742/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1743/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1744/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 9.4ms\n",
      "video 1/1 (frame 1745/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.2ms\n",
      "video 1/1 (frame 1746/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.3ms\n",
      "video 1/1 (frame 1747/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1748/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1749/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1750/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1751/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1752/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.9ms\n",
      "video 1/1 (frame 1753/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1754/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1755/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.3ms\n",
      "video 1/1 (frame 1756/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1757/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 1758/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 1759/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1760/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 1761/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 1762/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1763/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1764/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 1765/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 1766/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1767/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1768/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 1769/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 1770/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.5ms\n",
      "video 1/1 (frame 1771/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1772/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 1773/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 1774/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1775/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1776/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1777/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 1778/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1779/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1780/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1781/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1782/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1783/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1784/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 1785/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1786/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.8ms\n",
      "video 1/1 (frame 1787/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1788/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.1ms\n",
      "video 1/1 (frame 1789/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.9ms\n",
      "video 1/1 (frame 1790/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1791/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1792/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 1793/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1794/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 1795/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1796/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1797/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1798/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1799/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1800/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1801/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1802/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1803/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1804/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1805/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 1806/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 1807/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 9.7ms\n",
      "video 1/1 (frame 1808/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.2ms\n",
      "video 1/1 (frame 1809/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 1810/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1811/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1812/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1813/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 1814/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 9.1ms\n",
      "video 1/1 (frame 1815/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1816/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1817/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1818/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1819/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1820/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.6ms\n",
      "video 1/1 (frame 1821/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1822/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1823/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1824/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.3ms\n",
      "video 1/1 (frame 1825/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1826/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.5ms\n",
      "video 1/1 (frame 1827/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1828/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1829/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1830/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 1831/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 1832/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1833/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1834/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1835/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 1836/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 1837/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 1838/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 1839/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1840/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1841/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1842/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.3ms\n",
      "video 1/1 (frame 1843/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.5ms\n",
      "video 1/1 (frame 1844/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 1845/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1846/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1847/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 1848/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1849/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 1850/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1851/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1852/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 1853/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 1854/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1855/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1856/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1857/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 1858/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 1859/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1860/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 1861/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 1862/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.2ms\n",
      "video 1/1 (frame 1863/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 1864/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 1865/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 1866/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 1867/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 1868/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.2ms\n",
      "video 1/1 (frame 1869/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 1870/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 1871/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 1872/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1873/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 1874/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.1ms\n",
      "video 1/1 (frame 1875/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 1876/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 1877/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 1878/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1879/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 1880/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 1881/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 1882/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.3ms\n",
      "video 1/1 (frame 1883/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 1884/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 1885/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 1886/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 1887/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.7ms\n",
      "video 1/1 (frame 1888/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 1889/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.0ms\n",
      "video 1/1 (frame 1890/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 1891/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 1892/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 1893/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 1894/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 1895/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 1896/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 1897/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 1898/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 1899/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 1900/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 1901/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 1902/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1903/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 1904/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1905/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1906/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1907/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1908/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1909/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1910/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1911/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1912/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.9ms\n",
      "video 1/1 (frame 1913/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1914/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.1ms\n",
      "video 1/1 (frame 1915/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 1916/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 1917/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1918/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1919/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1920/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.5ms\n",
      "video 1/1 (frame 1921/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 1922/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.5ms\n",
      "video 1/1 (frame 1923/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1924/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1925/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 1926/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1927/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 1928/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1929/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1930/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1931/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1932/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 1933/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1934/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.7ms\n",
      "video 1/1 (frame 1935/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1936/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 1937/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 1938/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1939/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 1940/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1941/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.2ms\n",
      "video 1/1 (frame 1942/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 1943/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 1944/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 1945/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 1946/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 1947/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 1948/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 1949/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1950/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 1951/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 1952/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1953/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 1954/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 1955/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 1956/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 1957/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 1958/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 1959/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 1960/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1961/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 1962/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 1963/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 1964/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1965/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 1966/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 1967/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 1968/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.2ms\n",
      "video 1/1 (frame 1969/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1970/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.7ms\n",
      "video 1/1 (frame 1971/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 1972/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 1973/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 1974/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 1975/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 1976/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 1977/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 9.1ms\n",
      "video 1/1 (frame 1978/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.5ms\n",
      "video 1/1 (frame 1979/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 1980/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 1981/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 1982/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 1983/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.7ms\n",
      "video 1/1 (frame 1984/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 1985/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 1986/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 1987/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 1988/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 1989/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.2ms\n",
      "video 1/1 (frame 1990/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 1991/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1992/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 1993/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 1994/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 1995/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 1996/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 1997/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 1998/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 1999/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2000/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2001/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2002/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2003/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2004/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 2005/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2006/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2007/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2008/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 2009/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 2010/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2011/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2012/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2013/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2014/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2015/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2016/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2017/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2018/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.7ms\n",
      "video 1/1 (frame 2019/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2020/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 2021/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 2022/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2023/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2024/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2025/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2026/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2027/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 2028/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 2029/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2030/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2031/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 2032/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 2033/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 2034/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.0ms\n",
      "video 1/1 (frame 2035/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 2036/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2037/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 2038/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 2039/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 2040/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 2041/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 2042/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2043/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2044/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 2045/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2046/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2047/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2048/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2049/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.0ms\n",
      "video 1/1 (frame 2050/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2051/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 2052/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2053/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 2054/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 2055/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2056/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2057/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.5ms\n",
      "video 1/1 (frame 2058/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.3ms\n",
      "video 1/1 (frame 2059/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2060/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.1ms\n",
      "video 1/1 (frame 2061/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.4ms\n",
      "video 1/1 (frame 2062/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 2063/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 2064/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.1ms\n",
      "video 1/1 (frame 2065/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 2066/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.0ms\n",
      "video 1/1 (frame 2067/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 2068/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 2069/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 2070/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 2071/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2072/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2073/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 2074/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 2075/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 2076/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 2077/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2078/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2079/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2080/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2081/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 2082/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2083/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2084/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 2085/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 2086/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2087/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 2088/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 2089/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 2090/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2091/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2092/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 2093/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 2094/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.4ms\n",
      "video 1/1 (frame 2095/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.1ms\n",
      "video 1/1 (frame 2096/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2097/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 2098/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2099/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2100/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2101/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2102/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2103/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2104/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 2105/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2106/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 2107/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 2108/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 2109/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.2ms\n",
      "video 1/1 (frame 2110/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 2111/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 2112/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 2113/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2114/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2115/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 2116/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2117/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 2118/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 2119/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2120/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2121/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2122/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.2ms\n",
      "video 1/1 (frame 2123/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.0ms\n",
      "video 1/1 (frame 2124/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2125/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2126/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 2127/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 2128/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.6ms\n",
      "video 1/1 (frame 2129/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2130/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2131/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2132/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 2133/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.5ms\n",
      "video 1/1 (frame 2134/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 2135/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.4ms\n",
      "video 1/1 (frame 2136/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 2137/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 2138/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 2139/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 2140/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2141/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2142/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2143/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2144/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.1ms\n",
      "video 1/1 (frame 2145/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 2146/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 2147/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 2148/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 2149/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 2150/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2151/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 2152/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 2153/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 2154/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.1ms\n",
      "video 1/1 (frame 2155/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.9ms\n",
      "video 1/1 (frame 2156/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2157/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 2158/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2159/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2160/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 4.9ms\n",
      "video 1/1 (frame 2161/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2162/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2163/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2164/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2165/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2166/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2167/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2168/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.7ms\n",
      "video 1/1 (frame 2169/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2170/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2171/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2172/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2173/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2174/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2175/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2176/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2177/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2178/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2179/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 2180/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2181/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2182/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2183/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2184/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.2ms\n",
      "video 1/1 (frame 2185/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2186/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2187/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2188/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2189/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2190/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2191/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2192/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2193/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 2194/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 2195/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 2196/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2197/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2198/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2199/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2200/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2201/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.3ms\n",
      "video 1/1 (frame 2202/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2203/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2204/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2205/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2206/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.0ms\n",
      "video 1/1 (frame 2207/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2208/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2209/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 2210/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.6ms\n",
      "video 1/1 (frame 2211/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 9.6ms\n",
      "video 1/1 (frame 2212/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2213/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 2214/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 2215/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2216/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2217/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 2218/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.4ms\n",
      "video 1/1 (frame 2219/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 2220/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 2221/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2222/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2223/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 2224/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2225/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2226/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 2227/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 2228/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 2229/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 2230/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 2231/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 2232/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 2233/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.4ms\n",
      "video 1/1 (frame 2234/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 2235/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 2236/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 2237/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 2238/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2239/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2240/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2241/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.8ms\n",
      "video 1/1 (frame 2242/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 2243/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2244/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2245/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2246/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2247/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2248/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 2249/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2250/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2251/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2252/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2253/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.7ms\n",
      "video 1/1 (frame 2254/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2255/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2256/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2257/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 2258/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 2259/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.3ms\n",
      "video 1/1 (frame 2260/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2261/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2262/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2263/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2264/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2265/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2266/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2267/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.3ms\n",
      "video 1/1 (frame 2268/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2269/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 2270/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2271/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2272/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2273/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.2ms\n",
      "video 1/1 (frame 2274/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2275/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.6ms\n",
      "video 1/1 (frame 2276/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 2277/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 2278/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2279/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 2280/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 8.0ms\n",
      "video 1/1 (frame 2281/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.3ms\n",
      "video 1/1 (frame 2282/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 2283/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 2284/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.9ms\n",
      "video 1/1 (frame 2285/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 2286/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 2287/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 2288/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 2289/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 2290/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2291/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2292/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2293/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2294/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 2295/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 2296/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2297/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.1ms\n",
      "video 1/1 (frame 2298/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 2299/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2300/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 2301/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2302/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2303/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2304/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2305/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 2306/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 2307/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 11.8ms\n",
      "video 1/1 (frame 2308/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2309/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 2310/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 2311/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 2312/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 2313/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2314/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2315/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 2316/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 2317/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2318/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 2319/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2320/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2321/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.7ms\n",
      "video 1/1 (frame 2322/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 2323/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 2324/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 2325/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 2326/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.6ms\n",
      "video 1/1 (frame 2327/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.6ms\n",
      "video 1/1 (frame 2328/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.4ms\n",
      "video 1/1 (frame 2329/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 8.0ms\n",
      "video 1/1 (frame 2330/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.4ms\n",
      "video 1/1 (frame 2331/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 2332/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 2333/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.4ms\n",
      "video 1/1 (frame 2334/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.9ms\n",
      "video 1/1 (frame 2335/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.2ms\n",
      "video 1/1 (frame 2336/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 2337/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 2338/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 2339/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 2340/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 2341/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2342/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.8ms\n",
      "video 1/1 (frame 2343/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.3ms\n",
      "video 1/1 (frame 2344/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.2ms\n",
      "video 1/1 (frame 2345/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.8ms\n",
      "video 1/1 (frame 2346/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 2347/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 2348/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 2349/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2350/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2351/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 2352/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 2353/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2354/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 2355/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 2356/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2357/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2358/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2359/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.7ms\n",
      "video 1/1 (frame 2360/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2361/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 2362/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2363/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.2ms\n",
      "video 1/1 (frame 2364/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2365/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 2366/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 2367/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2368/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 2369/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2370/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 2371/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2372/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.8ms\n",
      "video 1/1 (frame 2373/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 2374/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 2375/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.9ms\n",
      "video 1/1 (frame 2376/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 2377/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 2378/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 2379/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 2380/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 2381/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 2382/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 2383/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2384/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.7ms\n",
      "video 1/1 (frame 2385/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 2386/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 2387/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 2388/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2389/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2390/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2391/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 2392/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.8ms\n",
      "video 1/1 (frame 2393/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 9.0ms\n",
      "video 1/1 (frame 2394/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 12.7ms\n",
      "video 1/1 (frame 2395/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 2396/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2397/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2398/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2399/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2400/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2401/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2402/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2403/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2404/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 2405/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2406/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2407/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2408/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 2409/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2410/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 2411/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2412/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 2413/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2414/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2415/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2416/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2417/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 2418/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 2419/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2420/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2421/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2422/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 2423/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.8ms\n",
      "video 1/1 (frame 2424/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2425/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2426/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2427/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 11.4ms\n",
      "video 1/1 (frame 2428/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.6ms\n",
      "video 1/1 (frame 2429/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.0ms\n",
      "video 1/1 (frame 2430/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 2431/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 2432/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2433/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 2434/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 2435/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 2436/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2437/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.1ms\n",
      "video 1/1 (frame 2438/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 2439/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 2440/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2441/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 2442/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 2443/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2444/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2445/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2446/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2447/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 2448/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2449/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2450/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.6ms\n",
      "video 1/1 (frame 2451/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.3ms\n",
      "video 1/1 (frame 2452/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.1ms\n",
      "video 1/1 (frame 2453/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 9.0ms\n",
      "video 1/1 (frame 2454/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.9ms\n",
      "video 1/1 (frame 2455/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.0ms\n",
      "video 1/1 (frame 2456/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.0ms\n",
      "video 1/1 (frame 2457/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.5ms\n",
      "video 1/1 (frame 2458/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2459/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.6ms\n",
      "video 1/1 (frame 2460/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2461/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2462/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2463/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2464/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2465/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.8ms\n",
      "video 1/1 (frame 2466/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.3ms\n",
      "video 1/1 (frame 2467/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.4ms\n",
      "video 1/1 (frame 2468/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.4ms\n",
      "video 1/1 (frame 2469/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.7ms\n",
      "video 1/1 (frame 2470/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.4ms\n",
      "video 1/1 (frame 2471/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 9.8ms\n",
      "video 1/1 (frame 2472/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.8ms\n",
      "video 1/1 (frame 2473/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.5ms\n",
      "video 1/1 (frame 2474/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 8.3ms\n",
      "video 1/1 (frame 2475/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 2476/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2477/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2478/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2479/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2480/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2481/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2482/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2483/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2484/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2485/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2486/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2487/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 2488/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 2489/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.1ms\n",
      "video 1/1 (frame 2490/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2491/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2492/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.4ms\n",
      "video 1/1 (frame 2493/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2494/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2495/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2496/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2497/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2498/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 2499/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2500/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2501/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2502/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2503/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 2504/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2505/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 2506/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 2507/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 2508/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2509/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2510/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2511/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2512/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2513/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2514/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.1ms\n",
      "video 1/1 (frame 2515/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.7ms\n",
      "video 1/1 (frame 2516/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2517/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 2518/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 2519/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2520/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.3ms\n",
      "video 1/1 (frame 2521/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 2522/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 2523/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2524/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 2525/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 2526/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2527/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2528/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2529/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2530/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 2531/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 2532/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 2533/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 2534/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 2535/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 2536/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 2537/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 2538/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 2539/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 2540/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2541/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 13.0ms\n",
      "video 1/1 (frame 2542/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 9.8ms\n",
      "video 1/1 (frame 2543/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.8ms\n",
      "video 1/1 (frame 2544/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 2545/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 2546/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2547/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2548/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2549/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2550/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2551/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2552/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.5ms\n",
      "video 1/1 (frame 2553/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.4ms\n",
      "video 1/1 (frame 2554/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.1ms\n",
      "video 1/1 (frame 2555/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.0ms\n",
      "video 1/1 (frame 2556/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 2557/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 2558/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.7ms\n",
      "video 1/1 (frame 2559/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.7ms\n",
      "video 1/1 (frame 2560/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 2561/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2562/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 2563/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2564/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 2565/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.4ms\n",
      "video 1/1 (frame 2566/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 2567/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.2ms\n",
      "video 1/1 (frame 2568/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2569/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 2570/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2571/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.7ms\n",
      "video 1/1 (frame 2572/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 2573/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.7ms\n",
      "video 1/1 (frame 2574/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 2575/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.7ms\n",
      "video 1/1 (frame 2576/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 2577/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 2578/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 2579/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 2580/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.9ms\n",
      "video 1/1 (frame 2581/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 2582/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 2583/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 2584/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 2585/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 2586/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 2587/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 2588/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.7ms\n",
      "video 1/1 (frame 2589/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 2590/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 2591/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 2592/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 2593/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 2594/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.9ms\n",
      "video 1/1 (frame 2595/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 2596/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 2597/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 2598/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 2599/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.6ms\n",
      "video 1/1 (frame 2600/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 2601/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 2602/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 2603/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 2604/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.1ms\n",
      "video 1/1 (frame 2605/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.0ms\n",
      "video 1/1 (frame 2606/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.9ms\n",
      "video 1/1 (frame 2607/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 2608/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 2609/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 2610/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 2611/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 2612/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 2613/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 2614/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 2615/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 2616/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.0ms\n",
      "video 1/1 (frame 2617/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.0ms\n",
      "video 1/1 (frame 2618/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.3ms\n",
      "video 1/1 (frame 2619/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 2620/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 2621/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 2622/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2623/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2624/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.2ms\n",
      "video 1/1 (frame 2625/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.0ms\n",
      "video 1/1 (frame 2626/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 2627/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2628/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 2629/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 2630/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 2631/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 2632/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.6ms\n",
      "video 1/1 (frame 2633/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 2634/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.8ms\n",
      "video 1/1 (frame 2635/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 2636/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 2637/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.0ms\n",
      "video 1/1 (frame 2638/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.9ms\n",
      "video 1/1 (frame 2639/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.7ms\n",
      "video 1/1 (frame 2640/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.2ms\n",
      "video 1/1 (frame 2641/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.8ms\n",
      "video 1/1 (frame 2642/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.4ms\n",
      "video 1/1 (frame 2643/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.5ms\n",
      "video 1/1 (frame 2644/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 2645/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 2646/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 2647/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 2648/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 2649/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 2650/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.7ms\n",
      "video 1/1 (frame 2651/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 2652/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 2653/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.4ms\n",
      "video 1/1 (frame 2654/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 2655/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.0ms\n",
      "video 1/1 (frame 2656/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 2657/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 2658/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.8ms\n",
      "video 1/1 (frame 2659/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2660/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 2661/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2662/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 2663/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2664/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 2665/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2666/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.8ms\n",
      "video 1/1 (frame 2667/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.3ms\n",
      "video 1/1 (frame 2668/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.7ms\n",
      "video 1/1 (frame 2669/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.1ms\n",
      "video 1/1 (frame 2670/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.5ms\n",
      "video 1/1 (frame 2671/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.8ms\n",
      "video 1/1 (frame 2672/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 2673/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2674/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.6ms\n",
      "video 1/1 (frame 2675/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2676/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 2677/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.8ms\n",
      "video 1/1 (frame 2678/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.1ms\n",
      "video 1/1 (frame 2679/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 2680/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 2681/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 2682/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 2683/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 2684/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 2685/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 2686/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.8ms\n",
      "video 1/1 (frame 2687/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.3ms\n",
      "video 1/1 (frame 2688/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 2689/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 2690/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.3ms\n",
      "video 1/1 (frame 2691/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.9ms\n",
      "video 1/1 (frame 2692/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 2693/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 2694/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 2695/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 2696/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.9ms\n",
      "video 1/1 (frame 2697/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 9.6ms\n",
      "video 1/1 (frame 2698/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.7ms\n",
      "video 1/1 (frame 2699/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.2ms\n",
      "video 1/1 (frame 2700/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 2701/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2702/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2703/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 2704/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 2705/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2706/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 2707/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 2708/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 2709/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.1ms\n",
      "video 1/1 (frame 2710/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 2711/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2712/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.1ms\n",
      "video 1/1 (frame 2713/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.5ms\n",
      "video 1/1 (frame 2714/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.1ms\n",
      "video 1/1 (frame 2715/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.3ms\n",
      "video 1/1 (frame 2716/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 2717/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 2718/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 2719/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 9.9ms\n",
      "video 1/1 (frame 2720/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.7ms\n",
      "video 1/1 (frame 2721/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 2722/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 2723/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.7ms\n",
      "video 1/1 (frame 2724/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.7ms\n",
      "video 1/1 (frame 2725/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.0ms\n",
      "video 1/1 (frame 2726/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.1ms\n",
      "video 1/1 (frame 2727/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 2728/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.9ms\n",
      "video 1/1 (frame 2729/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 2730/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.3ms\n",
      "video 1/1 (frame 2731/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 2732/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.8ms\n",
      "video 1/1 (frame 2733/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 2734/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.8ms\n",
      "video 1/1 (frame 2735/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2736/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 2737/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 2738/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2739/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 2740/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.2ms\n",
      "video 1/1 (frame 2741/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.5ms\n",
      "video 1/1 (frame 2742/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.4ms\n",
      "video 1/1 (frame 2743/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.3ms\n",
      "video 1/1 (frame 2744/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.1ms\n",
      "video 1/1 (frame 2745/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.8ms\n",
      "video 1/1 (frame 2746/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 9.6ms\n",
      "video 1/1 (frame 2747/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.9ms\n",
      "video 1/1 (frame 2748/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.0ms\n",
      "video 1/1 (frame 2749/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.7ms\n",
      "video 1/1 (frame 2750/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.4ms\n",
      "video 1/1 (frame 2751/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.4ms\n",
      "video 1/1 (frame 2752/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.2ms\n",
      "video 1/1 (frame 2753/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.3ms\n",
      "video 1/1 (frame 2754/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.4ms\n",
      "video 1/1 (frame 2755/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.5ms\n",
      "video 1/1 (frame 2756/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.3ms\n",
      "video 1/1 (frame 2757/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.3ms\n",
      "video 1/1 (frame 2758/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.2ms\n",
      "video 1/1 (frame 2759/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 6.3ms\n",
      "video 1/1 (frame 2760/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.6ms\n",
      "video 1/1 (frame 2761/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.3ms\n",
      "video 1/1 (frame 2762/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 5.5ms\n",
      "video 1/1 (frame 2763/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.5ms\n",
      "video 1/1 (frame 2764/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.3ms\n",
      "video 1/1 (frame 2765/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 15 persons, 5.9ms\n",
      "video 1/1 (frame 2766/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.5ms\n",
      "video 1/1 (frame 2767/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.4ms\n",
      "video 1/1 (frame 2768/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.8ms\n",
      "video 1/1 (frame 2769/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.0ms\n",
      "video 1/1 (frame 2770/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.2ms\n",
      "video 1/1 (frame 2771/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.1ms\n",
      "video 1/1 (frame 2772/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.2ms\n",
      "video 1/1 (frame 2773/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.3ms\n",
      "video 1/1 (frame 2774/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.9ms\n",
      "video 1/1 (frame 2775/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 2776/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.2ms\n",
      "video 1/1 (frame 2777/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.2ms\n",
      "video 1/1 (frame 2778/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.2ms\n",
      "video 1/1 (frame 2779/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 2780/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.5ms\n",
      "video 1/1 (frame 2781/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 12.6ms\n",
      "video 1/1 (frame 2782/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.1ms\n",
      "video 1/1 (frame 2783/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.4ms\n",
      "video 1/1 (frame 2784/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 7.1ms\n",
      "video 1/1 (frame 2785/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.3ms\n",
      "video 1/1 (frame 2786/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 2787/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.6ms\n",
      "video 1/1 (frame 2788/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.7ms\n",
      "video 1/1 (frame 2789/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.3ms\n",
      "video 1/1 (frame 2790/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 2791/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 2792/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 2793/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 2794/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 2795/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 2796/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 2797/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 2798/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 2799/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.0ms\n",
      "video 1/1 (frame 2800/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.9ms\n",
      "video 1/1 (frame 2801/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.7ms\n",
      "video 1/1 (frame 2802/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.5ms\n",
      "video 1/1 (frame 2803/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 2804/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 2805/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.3ms\n",
      "video 1/1 (frame 2806/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 2807/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 2808/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2809/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 2810/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 2811/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 2812/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.4ms\n",
      "video 1/1 (frame 2813/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 2814/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2815/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2816/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 2817/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 2818/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2819/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 2820/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2821/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 2822/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.7ms\n",
      "video 1/1 (frame 2823/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.6ms\n",
      "video 1/1 (frame 2824/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.8ms\n",
      "video 1/1 (frame 2825/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 18.2ms\n",
      "video 1/1 (frame 2826/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 2827/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.1ms\n",
      "video 1/1 (frame 2828/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2829/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 2830/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.7ms\n",
      "video 1/1 (frame 2831/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2832/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 2833/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 2834/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 2835/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2836/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.0ms\n",
      "video 1/1 (frame 2837/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.3ms\n",
      "video 1/1 (frame 2838/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.0ms\n",
      "video 1/1 (frame 2839/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.7ms\n",
      "video 1/1 (frame 2840/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.8ms\n",
      "video 1/1 (frame 2841/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.1ms\n",
      "video 1/1 (frame 2842/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.7ms\n",
      "video 1/1 (frame 2843/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.2ms\n",
      "video 1/1 (frame 2844/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.8ms\n",
      "video 1/1 (frame 2845/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.6ms\n",
      "video 1/1 (frame 2846/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.5ms\n",
      "video 1/1 (frame 2847/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.0ms\n",
      "video 1/1 (frame 2848/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.1ms\n",
      "video 1/1 (frame 2849/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 2850/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 2851/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 7.2ms\n",
      "video 1/1 (frame 2852/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.6ms\n",
      "video 1/1 (frame 2853/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.2ms\n",
      "video 1/1 (frame 2854/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 7.5ms\n",
      "video 1/1 (frame 2855/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 7.4ms\n",
      "video 1/1 (frame 2856/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.9ms\n",
      "video 1/1 (frame 2857/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.2ms\n",
      "video 1/1 (frame 2858/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.1ms\n",
      "video 1/1 (frame 2859/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.3ms\n",
      "video 1/1 (frame 2860/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.5ms\n",
      "video 1/1 (frame 2861/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.3ms\n",
      "video 1/1 (frame 2862/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.5ms\n",
      "video 1/1 (frame 2863/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 2864/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 2865/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.4ms\n",
      "video 1/1 (frame 2866/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 2867/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 2868/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.4ms\n",
      "video 1/1 (frame 2869/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.4ms\n",
      "video 1/1 (frame 2870/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.5ms\n",
      "video 1/1 (frame 2871/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2872/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 10.8ms\n",
      "video 1/1 (frame 2873/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.9ms\n",
      "video 1/1 (frame 2874/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2875/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 2876/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2877/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2878/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.2ms\n",
      "video 1/1 (frame 2879/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2880/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2881/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 2882/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2883/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.9ms\n",
      "video 1/1 (frame 2884/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.2ms\n",
      "video 1/1 (frame 2885/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.6ms\n",
      "video 1/1 (frame 2886/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 2887/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 10.7ms\n",
      "video 1/1 (frame 2888/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.3ms\n",
      "video 1/1 (frame 2889/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 9.0ms\n",
      "video 1/1 (frame 2890/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.6ms\n",
      "video 1/1 (frame 2891/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 2892/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 2893/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 2894/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2895/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 2896/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2897/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 2898/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.0ms\n",
      "video 1/1 (frame 2899/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 2900/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 2901/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 2902/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 2903/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.8ms\n",
      "video 1/1 (frame 2904/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 2905/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 2906/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 2907/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2908/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.6ms\n",
      "video 1/1 (frame 2909/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 2910/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 2911/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 2912/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 2913/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.9ms\n",
      "video 1/1 (frame 2914/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 2915/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 2916/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 2917/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 2918/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 2919/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 2920/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 2921/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2922/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.6ms\n",
      "video 1/1 (frame 2923/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.3ms\n",
      "video 1/1 (frame 2924/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.4ms\n",
      "video 1/1 (frame 2925/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.6ms\n",
      "video 1/1 (frame 2926/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 2927/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 2928/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2929/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2930/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2931/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2932/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 2933/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2934/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 2935/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2936/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 2937/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 2938/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2939/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 2940/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 2941/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 2942/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 2943/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 2944/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2945/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 2946/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 2947/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2948/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 2949/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.1ms\n",
      "video 1/1 (frame 2950/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 2951/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 2952/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2953/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 2954/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 2955/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.2ms\n",
      "video 1/1 (frame 2956/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2957/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2958/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 2959/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.9ms\n",
      "video 1/1 (frame 2960/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 2961/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.0ms\n",
      "video 1/1 (frame 2962/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.0ms\n",
      "video 1/1 (frame 2963/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 2964/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 2965/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 2966/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 2967/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 2968/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2969/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 2970/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 2971/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.2ms\n",
      "video 1/1 (frame 2972/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.7ms\n",
      "video 1/1 (frame 2973/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 8.7ms\n",
      "video 1/1 (frame 2974/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.4ms\n",
      "video 1/1 (frame 2975/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.5ms\n",
      "video 1/1 (frame 2976/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 2977/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 2978/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 2979/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 2980/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.3ms\n",
      "video 1/1 (frame 2981/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 2982/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 2983/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 2984/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 2985/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 2986/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.6ms\n",
      "video 1/1 (frame 2987/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.8ms\n",
      "video 1/1 (frame 2988/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.4ms\n",
      "video 1/1 (frame 2989/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.3ms\n",
      "video 1/1 (frame 2990/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.7ms\n",
      "video 1/1 (frame 2991/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.4ms\n",
      "video 1/1 (frame 2992/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 2993/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.1ms\n",
      "video 1/1 (frame 2994/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.0ms\n",
      "video 1/1 (frame 2995/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.3ms\n",
      "video 1/1 (frame 2996/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.6ms\n",
      "video 1/1 (frame 2997/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 16 persons, 6.7ms\n",
      "video 1/1 (frame 2998/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 18 persons, 5.5ms\n",
      "video 1/1 (frame 2999/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 3000/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.9ms\n",
      "video 1/1 (frame 3001/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.0ms\n",
      "video 1/1 (frame 3002/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 8.2ms\n",
      "video 1/1 (frame 3003/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 7.5ms\n",
      "video 1/1 (frame 3004/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 6.0ms\n",
      "video 1/1 (frame 3005/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.9ms\n",
      "video 1/1 (frame 3006/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 3007/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.4ms\n",
      "video 1/1 (frame 3008/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.9ms\n",
      "video 1/1 (frame 3009/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 3010/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.2ms\n",
      "video 1/1 (frame 3011/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.8ms\n",
      "video 1/1 (frame 3012/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 3013/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 3014/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 3015/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3016/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3017/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 3018/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3019/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3020/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3021/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3022/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3023/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3024/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 3025/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 3026/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3027/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3028/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3029/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 3030/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3031/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3032/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3033/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3034/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 3035/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3036/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3037/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3038/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3039/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3040/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 3041/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 3042/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.8ms\n",
      "video 1/1 (frame 3043/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.5ms\n",
      "video 1/1 (frame 3044/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.2ms\n",
      "video 1/1 (frame 3045/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3046/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 3047/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3048/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 3049/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3050/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3051/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3052/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.9ms\n",
      "video 1/1 (frame 3053/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 9.5ms\n",
      "video 1/1 (frame 3054/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3055/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3056/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.5ms\n",
      "video 1/1 (frame 3057/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 3058/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 3059/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3060/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3061/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3062/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 3063/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3064/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 3065/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 3066/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 3067/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 3068/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3069/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3070/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 3071/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3072/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3073/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 9.4ms\n",
      "video 1/1 (frame 3074/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.9ms\n",
      "video 1/1 (frame 3075/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.9ms\n",
      "video 1/1 (frame 3076/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.9ms\n",
      "video 1/1 (frame 3077/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.1ms\n",
      "video 1/1 (frame 3078/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3079/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3080/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3081/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3082/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3083/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3084/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 3085/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3086/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 3087/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 3088/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 3089/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.8ms\n",
      "video 1/1 (frame 3090/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.0ms\n",
      "video 1/1 (frame 3091/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3092/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3093/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.1ms\n",
      "video 1/1 (frame 3094/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 3095/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 9.7ms\n",
      "video 1/1 (frame 3096/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.6ms\n",
      "video 1/1 (frame 3097/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3098/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.5ms\n",
      "video 1/1 (frame 3099/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.3ms\n",
      "video 1/1 (frame 3100/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.7ms\n",
      "video 1/1 (frame 3101/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3102/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3103/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 3104/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.7ms\n",
      "video 1/1 (frame 3105/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.8ms\n",
      "video 1/1 (frame 3106/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 3107/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 3108/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.7ms\n",
      "video 1/1 (frame 3109/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.3ms\n",
      "video 1/1 (frame 3110/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.5ms\n",
      "video 1/1 (frame 3111/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3112/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3113/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.7ms\n",
      "video 1/1 (frame 3114/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.5ms\n",
      "video 1/1 (frame 3115/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 8.6ms\n",
      "video 1/1 (frame 3116/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3117/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.1ms\n",
      "video 1/1 (frame 3118/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.9ms\n",
      "video 1/1 (frame 3119/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.5ms\n",
      "video 1/1 (frame 3120/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.1ms\n",
      "video 1/1 (frame 3121/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3122/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.3ms\n",
      "video 1/1 (frame 3123/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.8ms\n",
      "video 1/1 (frame 3124/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.8ms\n",
      "video 1/1 (frame 3125/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.9ms\n",
      "video 1/1 (frame 3126/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.7ms\n",
      "video 1/1 (frame 3127/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3128/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 3129/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.4ms\n",
      "video 1/1 (frame 3130/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.2ms\n",
      "video 1/1 (frame 3131/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 3132/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.4ms\n",
      "video 1/1 (frame 3133/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 3134/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.8ms\n",
      "video 1/1 (frame 3135/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.1ms\n",
      "video 1/1 (frame 3136/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 3137/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 3138/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.6ms\n",
      "video 1/1 (frame 3139/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.3ms\n",
      "video 1/1 (frame 3140/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.3ms\n",
      "video 1/1 (frame 3141/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 822.3ms\n",
      "video 1/1 (frame 3142/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 3143/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.7ms\n",
      "video 1/1 (frame 3144/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3145/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3146/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3147/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3148/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 3149/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3150/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.2ms\n",
      "video 1/1 (frame 3151/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.2ms\n",
      "video 1/1 (frame 3152/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3153/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3154/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3155/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3156/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3157/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3158/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 3159/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 3160/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 3161/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 3162/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 3163/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 3164/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 3165/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 3166/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 3167/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 3168/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.3ms\n",
      "video 1/1 (frame 3169/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 3170/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 3171/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 3172/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 3173/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 3174/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 3175/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 3176/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.0ms\n",
      "video 1/1 (frame 3177/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.9ms\n",
      "video 1/1 (frame 3178/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.5ms\n",
      "video 1/1 (frame 3179/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 3180/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 3181/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3182/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 3183/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.2ms\n",
      "video 1/1 (frame 3184/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.8ms\n",
      "video 1/1 (frame 3185/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.5ms\n",
      "video 1/1 (frame 3186/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.3ms\n",
      "video 1/1 (frame 3187/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.6ms\n",
      "video 1/1 (frame 3188/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 3189/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 3190/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.6ms\n",
      "video 1/1 (frame 3191/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 3192/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.9ms\n",
      "video 1/1 (frame 3193/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 3194/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.5ms\n",
      "video 1/1 (frame 3195/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 7.1ms\n",
      "video 1/1 (frame 3196/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.3ms\n",
      "video 1/1 (frame 3197/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 3198/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 3199/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.6ms\n",
      "video 1/1 (frame 3200/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 3201/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 3202/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.6ms\n",
      "video 1/1 (frame 3203/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.9ms\n",
      "video 1/1 (frame 3204/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3205/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3206/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 3207/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3208/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 3209/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 3210/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 3211/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 3212/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3213/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3214/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3215/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 3216/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3217/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3218/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 3219/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 3220/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3221/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 3222/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3223/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 3224/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 3225/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.7ms\n",
      "video 1/1 (frame 3226/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.0ms\n",
      "video 1/1 (frame 3227/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 3228/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 3229/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 3230/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 3231/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.4ms\n",
      "video 1/1 (frame 3232/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 3233/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 3234/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3235/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 3236/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3237/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3238/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3239/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3240/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 3241/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3242/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.4ms\n",
      "video 1/1 (frame 3243/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3244/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3245/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3246/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3247/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 3248/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 3249/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.8ms\n",
      "video 1/1 (frame 3250/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3251/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3252/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 3253/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 3254/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3255/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 3256/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 3257/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3258/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 3259/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.7ms\n",
      "video 1/1 (frame 3260/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.9ms\n",
      "video 1/1 (frame 3261/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 3262/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 3263/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 3264/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3265/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3266/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 3267/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.2ms\n",
      "video 1/1 (frame 3268/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3269/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.7ms\n",
      "video 1/1 (frame 3270/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3271/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 3272/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 3273/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.1ms\n",
      "video 1/1 (frame 3274/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 3275/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3276/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3277/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3278/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3279/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.8ms\n",
      "video 1/1 (frame 3280/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.3ms\n",
      "video 1/1 (frame 3281/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 3282/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.6ms\n",
      "video 1/1 (frame 3283/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.9ms\n",
      "video 1/1 (frame 3284/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 14 persons, 5.6ms\n",
      "video 1/1 (frame 3285/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 7.3ms\n",
      "video 1/1 (frame 3286/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.6ms\n",
      "video 1/1 (frame 3287/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.6ms\n",
      "video 1/1 (frame 3288/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 6.1ms\n",
      "video 1/1 (frame 3289/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.4ms\n",
      "video 1/1 (frame 3290/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.1ms\n",
      "video 1/1 (frame 3291/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.6ms\n",
      "video 1/1 (frame 3292/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 3293/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.1ms\n",
      "video 1/1 (frame 3294/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.4ms\n",
      "video 1/1 (frame 3295/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.4ms\n",
      "video 1/1 (frame 3296/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.8ms\n",
      "video 1/1 (frame 3297/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.9ms\n",
      "video 1/1 (frame 3298/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.4ms\n",
      "video 1/1 (frame 3299/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 5.9ms\n",
      "video 1/1 (frame 3300/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.8ms\n",
      "video 1/1 (frame 3301/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.6ms\n",
      "video 1/1 (frame 3302/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.9ms\n",
      "video 1/1 (frame 3303/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.7ms\n",
      "video 1/1 (frame 3304/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.0ms\n",
      "video 1/1 (frame 3305/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 3306/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.9ms\n",
      "video 1/1 (frame 3307/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 3308/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.1ms\n",
      "video 1/1 (frame 3309/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.0ms\n",
      "video 1/1 (frame 3310/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.1ms\n",
      "video 1/1 (frame 3311/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.6ms\n",
      "video 1/1 (frame 3312/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 5.5ms\n",
      "video 1/1 (frame 3313/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 5.8ms\n",
      "video 1/1 (frame 3314/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.4ms\n",
      "video 1/1 (frame 3315/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 11 persons, 5.6ms\n",
      "video 1/1 (frame 3316/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.3ms\n",
      "video 1/1 (frame 3317/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 3318/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 3319/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 3320/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 8.5ms\n",
      "video 1/1 (frame 3321/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.3ms\n",
      "video 1/1 (frame 3322/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 3323/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 3324/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 3325/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 3326/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3327/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 3328/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 3329/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 3330/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3331/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3332/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3333/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 3334/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3335/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 3336/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3337/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.6ms\n",
      "video 1/1 (frame 3338/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 3339/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 3340/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 9.2ms\n",
      "video 1/1 (frame 3341/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 9.1ms\n",
      "video 1/1 (frame 3342/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3343/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.5ms\n",
      "video 1/1 (frame 3344/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 6.6ms\n",
      "video 1/1 (frame 3345/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 5.9ms\n",
      "video 1/1 (frame 3346/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.2ms\n",
      "video 1/1 (frame 3347/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 3348/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.5ms\n",
      "video 1/1 (frame 3349/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 3350/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 3351/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 3352/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3353/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 3354/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 3355/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 3356/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3357/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3358/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3359/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.3ms\n",
      "video 1/1 (frame 3360/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3361/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 9.0ms\n",
      "video 1/1 (frame 3362/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.9ms\n",
      "video 1/1 (frame 3363/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 3364/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 3365/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.8ms\n",
      "video 1/1 (frame 3366/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 3367/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 3368/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 3369/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3370/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3371/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.7ms\n",
      "video 1/1 (frame 3372/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3373/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 3374/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.1ms\n",
      "video 1/1 (frame 3375/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3376/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3377/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3378/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.5ms\n",
      "video 1/1 (frame 3379/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3380/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 3381/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.4ms\n",
      "video 1/1 (frame 3382/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.6ms\n",
      "video 1/1 (frame 3383/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 3384/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 3385/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3386/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3387/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3388/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 3389/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 3390/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 3391/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.4ms\n",
      "video 1/1 (frame 3392/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 3393/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.3ms\n",
      "video 1/1 (frame 3394/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3395/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3396/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3397/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3398/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3399/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3400/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3401/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.3ms\n",
      "video 1/1 (frame 3402/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3403/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 3404/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.8ms\n",
      "video 1/1 (frame 3405/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 3406/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.9ms\n",
      "video 1/1 (frame 3407/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3408/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.7ms\n",
      "video 1/1 (frame 3409/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 3410/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 3411/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.1ms\n",
      "video 1/1 (frame 3412/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 3413/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 7.3ms\n",
      "video 1/1 (frame 3414/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.0ms\n",
      "video 1/1 (frame 3415/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 3416/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 3417/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3418/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.6ms\n",
      "video 1/1 (frame 3419/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 3420/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3421/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 8.5ms\n",
      "video 1/1 (frame 3422/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3423/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3424/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 3425/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 3426/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3427/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.1ms\n",
      "video 1/1 (frame 3428/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.4ms\n",
      "video 1/1 (frame 3429/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 3430/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 3431/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 3432/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3433/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3434/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3435/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 3436/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.0ms\n",
      "video 1/1 (frame 3437/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.3ms\n",
      "video 1/1 (frame 3438/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 3439/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.5ms\n",
      "video 1/1 (frame 3440/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 3441/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.6ms\n",
      "video 1/1 (frame 3442/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.2ms\n",
      "video 1/1 (frame 3443/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 3444/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 3445/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.4ms\n",
      "video 1/1 (frame 3446/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 11.6ms\n",
      "video 1/1 (frame 3447/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 3448/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 8.2ms\n",
      "video 1/1 (frame 3449/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 9.3ms\n",
      "video 1/1 (frame 3450/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 3451/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 3452/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 3453/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 3454/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 3455/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.8ms\n",
      "video 1/1 (frame 3456/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 3457/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 6.2ms\n",
      "video 1/1 (frame 3458/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.6ms\n",
      "video 1/1 (frame 3459/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 12 persons, 5.6ms\n",
      "video 1/1 (frame 3460/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 6.0ms\n",
      "video 1/1 (frame 3461/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 6.6ms\n",
      "video 1/1 (frame 3462/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 13 persons, 10.2ms\n",
      "video 1/1 (frame 3463/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 6.1ms\n",
      "video 1/1 (frame 3464/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 7.1ms\n",
      "video 1/1 (frame 3465/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.7ms\n",
      "video 1/1 (frame 3466/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.5ms\n",
      "video 1/1 (frame 3467/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 3468/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 9 persons, 8.7ms\n",
      "video 1/1 (frame 3469/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 3470/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 3471/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 3472/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.1ms\n",
      "video 1/1 (frame 3473/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.2ms\n",
      "video 1/1 (frame 3474/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.4ms\n",
      "video 1/1 (frame 3475/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.8ms\n",
      "video 1/1 (frame 3476/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 7.4ms\n",
      "video 1/1 (frame 3477/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 8.0ms\n",
      "video 1/1 (frame 3478/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.2ms\n",
      "video 1/1 (frame 3479/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.0ms\n",
      "video 1/1 (frame 3480/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 (no detections), 7.9ms\n",
      "video 1/1 (frame 3481/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 6.7ms\n",
      "video 1/1 (frame 3482/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 1 person, 5.7ms\n",
      "video 1/1 (frame 3483/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.8ms\n",
      "video 1/1 (frame 3484/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 3485/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 3486/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 11.1ms\n",
      "video 1/1 (frame 3487/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3488/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 3489/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3490/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3491/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3492/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3493/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 3494/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 3495/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3496/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 3497/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 3498/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 3499/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.2ms\n",
      "video 1/1 (frame 3500/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 3501/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 3502/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 3503/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.5ms\n",
      "video 1/1 (frame 3504/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.5ms\n",
      "video 1/1 (frame 3505/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 3506/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 3507/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.7ms\n",
      "video 1/1 (frame 3508/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3509/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 3510/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.1ms\n",
      "video 1/1 (frame 3511/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.0ms\n",
      "video 1/1 (frame 3512/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.2ms\n",
      "video 1/1 (frame 3513/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.0ms\n",
      "video 1/1 (frame 3514/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.5ms\n",
      "video 1/1 (frame 3515/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 3516/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 3517/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.1ms\n",
      "video 1/1 (frame 3518/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3519/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.8ms\n",
      "video 1/1 (frame 3520/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.9ms\n",
      "video 1/1 (frame 3521/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.8ms\n",
      "video 1/1 (frame 3522/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 10 persons, 10.3ms\n",
      "video 1/1 (frame 3523/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 6.1ms\n",
      "video 1/1 (frame 3524/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.0ms\n",
      "video 1/1 (frame 3525/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 8 persons, 5.7ms\n",
      "video 1/1 (frame 3526/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 3527/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 3528/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 3529/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 3530/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.9ms\n",
      "video 1/1 (frame 3531/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.5ms\n",
      "video 1/1 (frame 3532/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 3533/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 3534/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 3535/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 3536/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 3537/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.9ms\n",
      "video 1/1 (frame 3538/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.9ms\n",
      "video 1/1 (frame 3539/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 3540/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 3541/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3542/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.9ms\n",
      "video 1/1 (frame 3543/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.3ms\n",
      "video 1/1 (frame 3544/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3545/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 3546/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3547/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.3ms\n",
      "video 1/1 (frame 3548/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.9ms\n",
      "video 1/1 (frame 3549/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 3550/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3551/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.5ms\n",
      "video 1/1 (frame 3552/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.6ms\n",
      "video 1/1 (frame 3553/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 3554/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.7ms\n",
      "video 1/1 (frame 3555/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3556/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.2ms\n",
      "video 1/1 (frame 3557/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.2ms\n",
      "video 1/1 (frame 3558/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 5.7ms\n",
      "video 1/1 (frame 3559/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 2 persons, 6.5ms\n",
      "video 1/1 (frame 3560/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.1ms\n",
      "video 1/1 (frame 3561/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 8.8ms\n",
      "video 1/1 (frame 3562/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 3563/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.8ms\n",
      "video 1/1 (frame 3564/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3565/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 3566/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 3567/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.8ms\n",
      "video 1/1 (frame 3568/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 3569/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.8ms\n",
      "video 1/1 (frame 3570/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 3571/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.2ms\n",
      "video 1/1 (frame 3572/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.7ms\n",
      "video 1/1 (frame 3573/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3574/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3575/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3576/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3577/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3578/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.4ms\n",
      "video 1/1 (frame 3579/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.2ms\n",
      "video 1/1 (frame 3580/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 9.3ms\n",
      "video 1/1 (frame 3581/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.9ms\n",
      "video 1/1 (frame 3582/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3583/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.2ms\n",
      "video 1/1 (frame 3584/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 3585/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3586/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3587/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3588/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.7ms\n",
      "video 1/1 (frame 3589/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.5ms\n",
      "video 1/1 (frame 3590/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3591/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.1ms\n",
      "video 1/1 (frame 3592/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.2ms\n",
      "video 1/1 (frame 3593/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.0ms\n",
      "video 1/1 (frame 3594/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 3595/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.3ms\n",
      "video 1/1 (frame 3596/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3597/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3598/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3599/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.0ms\n",
      "video 1/1 (frame 3600/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 3601/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.0ms\n",
      "video 1/1 (frame 3602/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 5.6ms\n",
      "video 1/1 (frame 3603/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 6.3ms\n",
      "video 1/1 (frame 3604/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 3 persons, 7.0ms\n",
      "video 1/1 (frame 3605/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 3606/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.1ms\n",
      "video 1/1 (frame 3607/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.6ms\n",
      "video 1/1 (frame 3608/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3609/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3610/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 9.4ms\n",
      "video 1/1 (frame 3611/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 3612/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 3613/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 7.6ms\n",
      "video 1/1 (frame 3614/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.7ms\n",
      "video 1/1 (frame 3615/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 3616/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 3617/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3618/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.3ms\n",
      "video 1/1 (frame 3619/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 3620/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.8ms\n",
      "video 1/1 (frame 3621/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3622/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 3623/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3624/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.7ms\n",
      "video 1/1 (frame 3625/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.3ms\n",
      "video 1/1 (frame 3626/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 3627/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 3628/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 3629/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 3630/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.5ms\n",
      "video 1/1 (frame 3631/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.1ms\n",
      "video 1/1 (frame 3632/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 5.8ms\n",
      "video 1/1 (frame 3633/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.3ms\n",
      "video 1/1 (frame 3634/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.6ms\n",
      "video 1/1 (frame 3635/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.4ms\n",
      "video 1/1 (frame 3636/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3637/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.4ms\n",
      "video 1/1 (frame 3638/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3639/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.7ms\n",
      "video 1/1 (frame 3640/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.1ms\n",
      "video 1/1 (frame 3641/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.6ms\n",
      "video 1/1 (frame 3642/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.4ms\n",
      "video 1/1 (frame 3643/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.5ms\n",
      "video 1/1 (frame 3644/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3645/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3646/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 3647/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.4ms\n",
      "video 1/1 (frame 3648/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3649/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.5ms\n",
      "video 1/1 (frame 3650/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3651/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3652/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 3653/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 3654/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 3655/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.8ms\n",
      "video 1/1 (frame 3656/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 3657/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.3ms\n",
      "video 1/1 (frame 3658/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 9.1ms\n",
      "video 1/1 (frame 3659/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.6ms\n",
      "video 1/1 (frame 3660/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.4ms\n",
      "video 1/1 (frame 3661/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 3662/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.8ms\n",
      "video 1/1 (frame 3663/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.4ms\n",
      "video 1/1 (frame 3664/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3665/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 7.1ms\n",
      "video 1/1 (frame 3666/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 7.1ms\n",
      "video 1/1 (frame 3667/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.2ms\n",
      "video 1/1 (frame 3668/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3669/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.2ms\n",
      "video 1/1 (frame 3670/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 3671/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.7ms\n",
      "video 1/1 (frame 3672/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 4 persons, 6.4ms\n",
      "video 1/1 (frame 3673/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3674/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 7.6ms\n",
      "video 1/1 (frame 3675/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 5.9ms\n",
      "video 1/1 (frame 3676/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 6.8ms\n",
      "video 1/1 (frame 3677/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 7 persons, 5.7ms\n",
      "video 1/1 (frame 3678/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 6 persons, 6.1ms\n",
      "video 1/1 (frame 3679/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3680/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.3ms\n",
      "video 1/1 (frame 3681/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3682/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.4ms\n",
      "video 1/1 (frame 3683/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.8ms\n",
      "video 1/1 (frame 3684/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.6ms\n",
      "video 1/1 (frame 3685/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.2ms\n",
      "video 1/1 (frame 3686/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3687/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3688/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3689/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.9ms\n",
      "video 1/1 (frame 3690/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.5ms\n",
      "video 1/1 (frame 3691/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3692/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.9ms\n",
      "video 1/1 (frame 3693/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 6.0ms\n",
      "video 1/1 (frame 3694/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 5.7ms\n",
      "video 1/1 (frame 3695/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 8.2ms\n",
      "video 1/1 (frame 3696/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 9.1ms\n",
      "video 1/1 (frame 3697/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 14.6ms\n",
      "video 1/1 (frame 3698/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 24.0ms\n",
      "video 1/1 (frame 3699/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 90.3ms\n",
      "video 1/1 (frame 3700/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 77.3ms\n",
      "video 1/1 (frame 3701/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 66.3ms\n",
      "video 1/1 (frame 3702/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 92.7ms\n",
      "video 1/1 (frame 3703/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 105.5ms\n",
      "video 1/1 (frame 3704/4075) /home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4: 384x640 5 persons, 100.9ms\n"
     ]
    }
   ],
   "source": [
    "source_path_01 = \"/home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4\"\n",
    "\n",
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "model(source = source_path_01, show = True,  imgsz = 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798437ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdc3b072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@1277.286] global cap_v4l.cpp:914 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
      "[ERROR:0@1277.287] global obsensor_uvc_stream_channel.cpp:163 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    results = model(frame, stream = True)\n",
    "\n",
    "    for r in results:\n",
    "        keypoints = r.keypoints\n",
    "\n",
    "        for ks in keypoints:\n",
    "            k = np.array(ks.xy[0], dtype = int)\n",
    "\n",
    "            for org in k:\n",
    "                cv2.line(frame, org, org, (0, 0, 255), 10)\n",
    "\n",
    "    \n",
    "    cv2.imshow(\"Result Preview\", frame)\n",
    "\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac50b727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 8.0ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8111/1964933505.py:20: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  k = np.array(ks.xy[0], dtype = int)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m keypoints = r.keypoints\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ks \u001b[38;5;129;01min\u001b[39;00m keypoints:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     k = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     25\u001b[39m         org = k[\u001b[32m3\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/yolo_tutorial/lib/python3.12/site-packages/torch/_tensor.py:1213\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy()\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8n-pose.pt\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    results = model(frame, stream = True)\n",
    "\n",
    "    for r in results:\n",
    "        keypoints = r.keypoints\n",
    "\n",
    "        for ks in keypoints:\n",
    "            k = np.array(ks.xy[0], dtype = int)\n",
    "\n",
    "\n",
    "\n",
    "            try:\n",
    "                org = k[3]\n",
    "                cv2.line(frame, org, org, (0, 0, 255), 10)\n",
    "\n",
    "                org = k[4]\n",
    "                cv2.line(frame, org, org, (255, 0, 0), 10)\n",
    "\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    cv2.imshow(\"Result Preview\", frame)\n",
    "\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb100e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b7492dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 16 persons, 7.5ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14217/1708592255.py:37: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  k = np.array(kp.xy[0], dtype = np.int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# keypoints   \u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#   ultralytics.engine.results.Keypoints object with attributes:\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#   \u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# .   (Shape): (N, K, 2)\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# N : , K :  2: X Y\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m kp \u001b[38;5;129;01min\u001b[39;00m keypoints:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     k = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# k =     numpy  \u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venv/yolo_tutorial/lib/python3.12/site-packages/torch/_tensor.py:1213\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy()\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.astype(dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8n-pose.pt')\n",
    "\n",
    "source_path = \"/home/roni/dev_ws/yolo_tut/datasets/le-sserafim-reuserapim-crazy-official-mv_video_720p_original.mp4\"\n",
    "cap = cv2.VideoCapture(source_path)\n",
    "\n",
    "\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH,\n",
    "                                        cv2.CAP_PROP_FRAME_HEIGHT,\n",
    "                                        cv2.CAP_PROP_FPS))\n",
    "\n",
    "img2 = cv2.imread('/home/roni/dev_ws/yolo_tut/datasets/Santa-Claus-Hat-transparent-png.png')\n",
    "img2_resized = cv2.resize(img2, (80, 80))\n",
    "img2_h, img2_w, img2_c = img2_resized.shape\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"video frame is empty or video processing has been successfully completed\")\n",
    "        break\n",
    "\n",
    "    \n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    results = model(frame, stream = True)\n",
    "\n",
    "\n",
    "    for r in results:\n",
    "        keypoints = r.keypoints\n",
    "        # keypoints   \n",
    "        #   ultralytics.engine.results.Keypoints object with attributes:\n",
    "        #   \n",
    "        # .   (Shape): (N, K, 2)\n",
    "        # N : , K :  2: X Y\n",
    "        for kp in keypoints:\n",
    "\n",
    "            k = np.array(kp.xy[0], dtype = np.int32)\n",
    "            # k =     numpy  \n",
    "            \n",
    "\n",
    "            try:\n",
    "                org = k[0]\n",
    "\n",
    "                x_offset = org[0] - int(img2_w // 2)\n",
    "                #  x  -   \n",
    "                y_offset = org[1] - int(img2_h // 2)\n",
    "                #  y  -   \n",
    "\n",
    "\n",
    "                roi = frame[y_offset : y_offset +img2_h,\n",
    "                            x_offset : x_offset + img2_w]\n",
    "                \n",
    "                # Region of Interest,  \n",
    "\n",
    "                # y_offset : y_offset + img2_h\n",
    "                # y_offset      \n",
    "                # x_offset : x_offset + img2_w\n",
    "                # x_offset      \n",
    "\n",
    "\n",
    "                img2_gray = cv2.cvtColor(img2_resized, cv2.COLOR_BGR2GRAY)\n",
    "                #     BGR  grayscale \n",
    "\n",
    "                mask_inv = cv2.bitwise_not(img2_gray)\n",
    "                #   white  \n",
    "\n",
    "\n",
    "\n",
    "                fg = cv2.bitwise_or(img2, img2,  mask = mask_inv)\n",
    "                # foreGround() \n",
    "                # mask_inv         \n",
    "\n",
    "                final_roi = cv2.bitwise_or(roi, fg)\n",
    "\n",
    "                frame[y_offset : y_offset +img2_h,\n",
    "                    x_offset : x_offset + img2_w] = final_roi\n",
    "\n",
    "            except:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "\n",
    "    cv2.imshow(\"result_preview\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c54ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_tutorial (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
